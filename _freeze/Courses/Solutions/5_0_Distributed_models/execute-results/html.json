{
  "hash": "ee5ea1cd5bf5e5c72a04a101c77cc73b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Distributed models with dask\"\nexecute: \n  eval: false\n---\n\n\n\n\n# Initialization \n\n::: {#35b93ea3 .cell execution_count=1}\n``` {.python .cell-code}\nfrom ipyparallel import Client\n\nrc = Client()\n```\n:::\n\n\n`rc` is an interable of accessibles computing nodes.\n\n::: {#b6f903f9 .cell execution_count=2}\n``` {.python .cell-code}\nviews = rc[:]\n```\n:::\n\n\n::: {#4c929b84 .cell execution_count=3}\n``` {.python .cell-code}\nviews\n```\n:::\n\n\n## Check cluster engines\n\n::: {#185e83ed .cell execution_count=4}\n``` {.python .cell-code}\nimport platform\nplatform.node()\n```\n:::\n\n\n::: {#79688641 .cell execution_count=5}\n``` {.python .cell-code}\nviews.apply_sync(platform.node)\n```\n:::\n\n\n## Distributed prime numbers\n\nLet's revive our functions\n\n::: {#2aaac32c .cell execution_count=6}\n``` {.python .cell-code}\nimport math\n\ndef check_prime(n):\n    if n % 2 == 0:\n        return False\n    for i in range(3, int(math.sqrt(n)) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n```\n:::\n\n\n::: {#2e1cb724 .cell execution_count=7}\n``` {.python .cell-code}\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\n```\n:::\n\n\n::: {#29e17b4c .cell execution_count=8}\n``` {.python .cell-code}\ndef find_primes(r):\n    return list(filter(check_prime,r))\n```\n:::\n\n\n## Peculiarities\n\nYou'll have to\n- [`push`](https://ipyparallel.readthedocs.io/en/latest/api/ipyparallel.html#ipyparallel.DirectView.push) your dependant functions to the engines (`ipyparallel` does push your main \"mapped\" function, but not its dependancies) \n- explicitly import any required python library to the engines \n\n::: {#416d5b42 .cell execution_count=9}\n``` {.python .cell-code}\nviews.push({'check_prime': check_prime})\n```\n:::\n\n\n::: {#5d08c31c .cell execution_count=10}\n``` {.python .cell-code}\nwith views.sync_imports():\n    import math\n```\n:::\n\n\n### First steps\n\n1. Complete with the correct [`views.map`](https://ipyparallel.readthedocs.io/en/latest/api/ipyparallel.html#ipyparallel.DirectView.map) call\n\n```python\ndef calculate_primes(N,chunksize):\n    return ...\n```\n\n2. Benchmark it for \n\n```python\nN = 5000000\nchunksize = int(N/64)\n```\n\n::: {#d1f4ef21 .cell tags='[\"solution\"]' execution_count=11}\n``` {.python .cell-code}\ndef calculate_primes(N,chunksize):\n    return views.map_sync(find_primes,chunks(range(1,N),chunksize))\n```\n:::\n\n\n::: {#8be731a2 .cell execution_count=12}\n``` {.python .cell-code}\nN = 5000000\n```\n:::\n\n\n::: {#64f50a5e .cell execution_count=13}\n``` {.python .cell-code}\n%timeit -r 1 -n 1 calculate_primes(N,int(N/64))\n```\n:::\n\n\n# (Aside) a network optimization : [broadcast_view](https://ipyparallel.readthedocs.io/en/latest/examples/broadcast/Broadcast%20view.html) (network optimization)\n\n<center>\n<img src=\"attachment:image.png\" width=\"500\"/>\n</center>\n\n::: {#e6dcc709 .cell execution_count=14}\n``` {.python .cell-code}\ndirect_view = rc.direct_view()\nbcast_view = rc.broadcast_view()\n```\n:::\n\n\n::: {#bd3bec6a .cell execution_count=15}\n``` {.python .cell-code}\n%timeit direct_view.apply_sync(lambda: None)\n```\n:::\n\n\n::: {#5c96710a .cell execution_count=16}\n``` {.python .cell-code}\n%timeit bcast_view.apply_sync(lambda: None)\n```\n:::\n\n\n# An embarrasingly parallel example : distributed Monte-Carlo computing of $\\pi$\n\nIf we sample randomly a bunch of $N$ points in the unity square, and counts all points $N_I$ verifying the condition\n\n$x^2 + y^2 \\le 1$ whichs means they are in the upper right quarter of a disk.\n\nWe have this convergence\n\n$\\lim_{N\\to\\infty} 4\\frac{N_I}{N} = \\pi$\n\n<center>\n    <img src=\"attachment:hpp2_0901.png\" width=\"40%\" />\n</center>\n\n### 2. Write the function which :\n- takes a number of estimates `nbr_estimates` as argument\n- samples them in the [(0,0),(1,1)] unity square\n- returns the number of points inside the disk quarter\n\n```python\ndef estimate_nbr_points_in_quarter_circle(nbr_estimates):\n    ...\n    return nbr_trials_in_quarter_unit_circle\n```\n\n::: {#7b4785c2 .cell tags='[\"solution\"]' execution_count=17}\n``` {.python .cell-code}\nwith views.sync_imports():\n    import random\n```\n:::\n\n\n::: {#7f196956 .cell tags='[\"solution\"]' execution_count=18}\n``` {.python .cell-code}\ndef estimate_nbr_points_in_quarter_circle(nbr_estimates):\n    nbr_trials_in_quarter_unit_circle = 0\n    for step in range(int(nbr_estimates)):\n        x = random.uniform(0, 1)\n        y = random.uniform(0, 1)\n        is_in_unit_circle = x * x + y * y <= 1.0\n        nbr_trials_in_quarter_unit_circle += is_in_unit_circle\n    return nbr_trials_in_quarter_unit_circle\n```\n:::\n\n\n::: {#5c5ba259 .cell tags='[\"solution\"]' execution_count=19}\n``` {.python .cell-code}\n4*estimate_nbr_points_in_quarter_circle(1e4)/1e4\n```\n:::\n\n\n### 3. Make it distributed\n\n- Wraps the previous function in\n    ```python\n    def calculate_pi_distributed(nnodes,nbr_samples_in_total)\n        ...\n        return estimated_pi\n    ```\n- `nnodes` will use only `rc[:nnodes]` and split the number of estimates for each worker nodes into `nnodes` blocks.\n- Try it on `1e8` samples and benchmark it on 1 to 8 nodes. (use [`time`](https://docs.python.org/3/library/time.html#time.time))\n- Plot the performance gain over one node and comment the plot.\n\n::: {#09ce449f .cell tags='[\"solution\"]' execution_count=20}\n``` {.python .cell-code}\ndef calculate_pi_distributed(nnodes,nbr_samples_in_total):\n    dview = rc[:nnodes]\n    nbr_samples_per_worker = nbr_samples_in_total / nnodes\n    nbr_in_quarter_unit_circles = dview.apply_sync(estimate_nbr_points_in_quarter_circle, \\\n                                                   nbr_samples_per_worker)\n    nbr_jobs = len(nbr_in_quarter_unit_circles)\n    return sum(nbr_in_quarter_unit_circles) * 4 / nbr_samples_in_total\n```\n:::\n\n\n::: {#755a0adc .cell tags='[\"solution\"]' execution_count=21}\n``` {.python .cell-code}\ncalculate_pi_distributed(8,1e7)\n```\n:::\n\n\n::: {#1a680f2e .cell tags='[\"solution\"]' execution_count=22}\n``` {.python .cell-code}\nimport time\n\nN = 1e8\ncluster_times = []\npis = []\nfor nbr_parallel_blocks in range(1,9):\n    print(f\"With {nbr_parallel_blocks} node(s): \")\n    t1 = time.time()\n    pi_estimate = calculate_pi_distributed(nbr_parallel_blocks,N)\n    total_time = time.time() - t1\n    print(f\"\\tPi estimate : {pi_estimate}\")\n    print(\"\\tTime : {:.2f}s\".format(total_time))\n    cluster_times.append(total_time)\n    pis.append(pi_estimate)\n```\n:::\n\n\n::: {#bb9a3a36 .cell tags='[\"solution\"]' execution_count=23}\n``` {.python .cell-code}\nimport plotly.express as px\n\nspeedups_cores = [cluster_times[0]/cluster_times[i] for i in range(8)]\npx.line(y=speedups_cores,x=range(1,9),\n        labels={\"x\":\"Number of cores\",\n                \"y\":\"Speedup over 1 core\"},\n       width=600)\n```\n:::\n\n\n$\\Longrightarrow$ We see a near perfect linear scalability.\n\n",
    "supporting": [
      "5_0_Distributed_models_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}