{
  "hash": "fe1e6bf965f2ca1071ff908269c1ed29",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Dask `delayed` App\"\nexecute: \n  eval: false\n---\n\n\n\n\n<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\"\n     align=\"right\"\n     width=\"30%\"\n     alt=\"Dask logo\\\">\n\n# dask.delayed - parallelize any code\n\nWhat if you don't have an array or dataframe? Instead of having blocks where the function is applied to each block, you can decorate functions with `@delayed` and _have the functions themselves be lazy_. \n\nThis is a simple way to use `dask` to parallelize existing codebases or build [complex systems](https://blog.dask.org/2018/02/09/credit-models-with-dask). \n\n**Related Documentation**\n\n* [Delayed documentation](https://docs.dask.org/en/latest/delayed.html)\n* [Delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU)\n* [Delayed API](https://docs.dask.org/en/latest/delayed-api.html)\n* [Delayed examples](https://examples.dask.org/delayed.html)\n* [Delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html)\n\nAs we'll see in the [distributed scheduler notebook](05_distributed.ipynb), Dask has several ways of executing code in parallel. We'll use the distributed scheduler by creating a `dask.distributed.Client`. For now, this will provide us with some nice diagnostics. We'll talk about schedulers in depth later.\n\n### Iniatilization of the ipyparallel/dask interface\n\n::: {#2e069890 .cell execution_count=1}\n``` {.python .cell-code}\nfrom ipyparallel import Client\n\n### Force the 8 nodes/1 core layout\nrc = Client()\nrc.stop_distributed()\nclient = rc.become_dask()\nclient\n```\n:::\n\n\n## A Typical Workflow\n\nTypically if a workflow contains a for-loop it can benefit from delayed. The following example outlines a read-transform-write:\n\n```python\nimport dask\n    \n@dask.delayed\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nresults = []\nfor filename in filenames:\n    results.append(process_file(filename))\n    \ndask.compute(results)\n```\n\n## Basics\n\nFirst let's make some toy functions, `inc` and `add`, that sleep for a while to simulate work. We'll then time running these functions normally.\n\nIn the next section we'll parallelize this code.\n\n::: {#39c3a84e .cell execution_count=2}\n``` {.python .cell-code}\nfrom time import sleep\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\ndef add(x, y):\n    sleep(1)\n    return x + y\n```\n:::\n\n\nWe time the execution of this normal code using the `%%time` magic, which is a special function of the Jupyter Notebook.\n\n::: {#b196a279 .cell execution_count=3}\n``` {.python .cell-code}\n%%time\n# This takes three seconds to run because we call each\n# function sequentially, one after the other\n\nx = inc(1)\ny = inc(2)\nz = add(x, y) \n```\n:::\n\n\n### Parallelize with the `dask.delayed` decorator\n\nThose two increment calls *could* be called in parallel, because they are totally independent of one-another.\n\nWe'll make the `inc` and `add` functions lazy using the `dask.delayed` decorator. When we call the delayed version by passing the arguments, exactly as before, the original function isn't actually called yet - which is why the cell execution finishes very quickly.\nInstead, a *delayed object* is made, which keeps track of the function to call and the arguments to pass to it.\n\n::: {#1499af86 .cell execution_count=4}\n``` {.python .cell-code}\nimport dask\n\n\n@dask.delayed\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\n@dask.delayed\ndef add(x, y):\n    sleep(1)\n    return x + y\n```\n:::\n\n\n::: {#c161c8e1 .cell execution_count=5}\n``` {.python .cell-code}\n%%time\n# This runs immediately, all it does is build a graph\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\n```\n:::\n\n\nThis ran immediately, since nothing has really happened yet.\n\nTo get the result, call `compute`. Notice that this runs faster than the original code.\n\n::: {#ddac481c .cell execution_count=6}\n``` {.python .cell-code}\n%%time\n# This actually runs our computation using a local thread pool\n\nz.compute()\n```\n:::\n\n\n## What just happened?\n\nThe `z` object is a lazy `Delayed` object.  This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another.  We can evaluate the result with `.compute()` as above or we can visualize the task graph for this value with `.visualize()`.\n\n::: {#cacb5292 .cell execution_count=7}\n``` {.python .cell-code}\nz\n```\n:::\n\n\n::: {#5d45e537 .cell execution_count=8}\n``` {.python .cell-code}\n# Look at the task graph for `z`\nz.visualize()\n```\n:::\n\n\nNotice that this includes the names of the functions from before, and the logical flow of the outputs of the `inc` functions to the inputs of `add`.\n\n### Some questions to consider:\n\n-  Why did we go from 3s to 2s?  Why weren't we able to parallelize down to 1s?\n-  What would have happened if the inc and add functions didn't include the `sleep(1)`?  Would Dask still be able to speed up this code?\n-  What if we have multiple outputs or also want to get access to x or y?\n\n## Exercise: Parallelize a for loop\n\n`for` loops are one of the most common things that we want to parallelize.  Use `dask.delayed` on `inc` and `sum` to parallelize the computation below:\n\n::: {#ae8d017d .cell execution_count=9}\n``` {.python .cell-code}\ndata = [1, 2, 3, 4, 5, 6, 7, 8] \n```\n:::\n\n\n::: {#c97f3d74 .cell execution_count=10}\n``` {.python .cell-code}\n%%time\n# Sequential code\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nresults = []\nfor x in data:\n    y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\n```\n:::\n\n\n::: {#2065e177 .cell execution_count=11}\n``` {.python .cell-code}\ntotal\n```\n:::\n\n\n::: {#c9ad505d .cell execution_count=12}\n``` {.python .cell-code}\n%%time\n# Your parallel code here...\n```\n:::\n\n\n::: {#78de5943 .cell tags='[\"solution\"]' execution_count=13}\n``` {.python .cell-code}\n%%time\n\n@dask.delayed\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nresults = []\nfor x in data:\n    y = inc(x)\n    results.append(y)\n\ntotal = dask.delayed(sum)(results)\nprint(\"Before computing:\", total)  # Let's see what type of thing total is\nresult = total.compute()\nprint(\"After computing :\", result)  # After it's computed\n```\n:::\n\n\nHow do the graph visualizations compare with the given solution, compared to a version with the `sum` function used directly rather than wrapped with `delayed`? Can you explain the latter version? You might find the result of the following expression illuminating\n```python\ninc(1) + inc(2)\n```\n\n::: {#0b9da96c .cell tags='[\"solution\"]' execution_count=14}\n``` {.python .cell-code}\n(inc(1) + inc(2)).visualize()\n```\n:::\n\n\n::: {#7bc0f2aa .cell tags='[\"solution\"]' execution_count=15}\n``` {.python .cell-code}\nsum([inc(1),inc(2),inc(3)]).visualize()\n```\n:::\n\n\n::: {#421674cf .cell tags='[\"solution\"]' execution_count=16}\n``` {.python .cell-code}\ntotal.visualize()\n```\n:::\n\n\n## Putting it all together with a the producer-consumer pattern\n\n## Introduction with a simple producer-consumer example\n\nWeâ€™ll try to reproduce the producer/consumer code from asynchronous application with `asyncio` or `threading` but with `dask.delayed` and compute\n\n```python\nfrom dask import delayed, compute\nfrom dask.distributed import Queue, print\nfrom time import sleep\n\nqueue = Queue()\n...\n```\n\n::: {#f6f2bbb6 .cell tags='[\"solution\"]' execution_count=17}\n``` {.python .cell-code}\nfrom dask import delayed, compute\nfrom dask.distributed import Queue, print\nfrom time import sleep\n\nqueue = Queue()\n\n@delayed\ndef produce(queue, n):\n    print(\"producing {} items\".format(n))\n    for x in range(1, n + 1):\n        # simulate i/o operation using sleep\n        sleep(1)\n        # produce an item\n        print(\"producing {}/{}\".format(x, n))\n        item = str(x)\n        # put the item in the queue\n        queue.put(item)\n\n    # indicate the producer is done\n    queue.put(None)\n    return n\n\n@delayed\ndef consume(queue):\n    consumed = 0\n    print(\"consuming items\")\n    while True:\n        # wait for an item from the producer\n        item = queue.get()\n        if item is None:\n            # the producer emits None to indicate that it is done\n            break\n\n        # process the item\n        print(\"consuming {}\".format(item))\n        consumed += 1\n    return consumed\n\ncompute(\n    produce(queue, 5), \n    consume(queue)\n)\n```\n:::\n\n\n## Exercise: Parallelize a for-loop code with control flow\n\nOften we want to delay only *some* functions, running a few of them immediately.  This is especially helpful when those functions are fast and help us to determine what other slower functions we should call.  This decision, to delay or not to delay, is usually where we need to be thoughtful when using `dask.delayed`.\n\nIn the example below we iterate through a list of inputs.  If that input is even then we want to call `inc`.  If the input is odd then we want to call `double`.  This `is_even` decision to call `inc` or `double` has to be made immediately (not lazily) in order for our graph-building Python code to proceed.\n\n::: {#6e711178 .cell execution_count=18}\n``` {.python .cell-code}\ndef double(x):\n    sleep(1)\n    return 2 * x\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\ndef is_even(x):\n    return not x % 2\n\n\ndata = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n```\n:::\n\n\n::: {#715e6798 .cell execution_count=19}\n``` {.python .cell-code}\n%%time\n# Sequential code\n\nresults = []\nfor x in data:\n    if is_even(x):\n        y = double(x)\n    else:\n        y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\nprint(total)\n```\n:::\n\n\n```python\n%%time\n# Your parallel code here...\n# TODO: parallelize the sequential code above using dask.delayed\n# You will need to delay some functions, but not all\n```\n\n::: {#a0ed07ca .cell tags='[\"solution\"]' execution_count=20}\n``` {.python .cell-code}\n@dask.delayed\ndef double(x):\n    sleep(1)\n    return 2 * x\n\n@dask.delayed\ndef inc(x):\n    sleep(1)\n    return x + 1\n\nresults = []\nfor x in data:\n    if is_even(x):  # even\n        y = double(x)\n    else:  # odd\n        y = inc(x)\n    results.append(y)\n\ntotal = sum(results)\n```\n:::\n\n\n::: {#0eaca02c .cell tags='[\"solution\"]' execution_count=21}\n``` {.python .cell-code}\n%time total.compute()\n```\n:::\n\n\n::: {#e05b0987 .cell tags='[\"solution\"]' execution_count=22}\n``` {.python .cell-code}\ntotal.visualize()\n```\n:::\n\n\n### Some questions to consider:\n\n-  What are other examples of control flow where we can't use delayed?\n-  What would have happened if we had delayed the evaluation of `is_even(x)` in the example above?\n-  What are your thoughts on delaying `sum`?  This function is both computational but also fast to run.\n\n- Example of control flow with weâ€¯canâ€™t use delayedÂ : conditional loops, recursive functions\n- Nothing, we have to compute it immediately for branching\n- Delaying sum isnâ€™tâ€¯worth the graph walkâ€¯overload\n\n## Exercise: Parallelize a Pandas Groupby Reduction\n\nIn this exercise we read several CSV files and perform a groupby operation in parallel.  We are given sequential code to do this and parallelize it with `dask.delayed`.\n\nThe computation we will parallelize is to compute the mean departure delay per airport from some historical flight data.  We will do this by using `dask.delayed` together with `pandas`.  In a future section we will do this same exercise with `dask.dataframe`.\n\n## Create data\n\nRun this code to prep some data.\n\nThis downloads and extracts some historical flight data for flights out of NYC between 1990 and 2000. The data is originally from [here](http://stat-computing.org/dataexpo/2009/the-data.html).\n\n::: {#845c3ba0 .cell execution_count=23}\n``` {.python .cell-code}\nimport os\nimport requests\n\nos.makedirs(\"data\", exist_ok=True)\ncode_url = \"https://raw.githubusercontent.com/dask/dask-tutorial/main/prep.py\"\nwith open(\"prep.py\", \"wb\") as f:\n    f.write(requests.get(code_url).content)\n%run prep.py -d flights\n```\n:::\n\n\n### Inspect data\n\n::: {#2a7444b3 .cell execution_count=24}\n``` {.python .cell-code}\nsorted(os.listdir(os.path.join(\"data\", \"nycflights\")))\n```\n:::\n\n\n### Read one file with `pandas.read_csv` and compute mean departure delay\n\n::: {#a028ceb3 .cell execution_count=25}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv(os.path.join(\"data\", \"nycflights\", \"1990.csv\"))\ndf.head()\n```\n:::\n\n\n::: {#825536f3 .cell execution_count=26}\n``` {.python .cell-code}\n# What is the schema?\ndf.dtypes\n```\n:::\n\n\n::: {#e35b4bd5 .cell execution_count=27}\n``` {.python .cell-code}\n# What originating airports are in the data?\ndf.Origin.unique()\n```\n:::\n\n\n::: {#ccc52b10 .cell execution_count=28}\n``` {.python .cell-code}\n# Mean departure delay per-airport for one year\ndf.groupby(\"Origin\").DepDelay.mean()\n```\n:::\n\n\n### Sequential code: Mean Departure Delay Per Airport\n\nThe above cell computes the mean departure delay per-airport for one year. Here we expand that to all years using a sequential for loop.\n\n::: {#dc2ce1ef .cell execution_count=29}\n``` {.python .cell-code}\nfrom glob import glob\n\nfilenames = sorted(glob(os.path.join(\"data\", \"nycflights\", \"*.csv\")))\n```\n:::\n\n\n::: {#88da3cb8 .cell execution_count=30}\n``` {.python .cell-code}\nfilenames\n```\n:::\n\n\n::: {#d44fba00 .cell execution_count=31}\n``` {.python .cell-code}\n%%time\n\nsums = []\ncounts = []\nfor fn in filenames:\n    # Read in file\n    df = pd.read_csv(fn)\n\n    # Groupby origin airport\n    by_origin = df.groupby(\"Origin\")\n\n    # Sum of all departure delays by origin\n    total = by_origin.DepDelay.sum()\n\n    # Number of flights by origin\n    count = by_origin.DepDelay.count()\n\n    # Save the intermediates\n    sums.append(total)\n    counts.append(count)\n\n# Combine intermediates to get total mean-delay-per-origin\ntotal_delays = sum(sums)\nn_flights = sum(counts)\nmean = total_delays / n_flights\n```\n:::\n\n\n::: {#0e76c999 .cell execution_count=32}\n``` {.python .cell-code}\nmean\n```\n:::\n\n\n### Parallelize the code above\n\nUse `dask.delayed` to parallelize the code above.  Some extra things you will need to know.\n\n1.  Methods and attribute access on delayed objects work automatically, so if you have a delayed object you can perform normal arithmetic, slicing, and method calls on it and it will produce the correct delayed calls.\n    \n2.  Calling the `.compute()` method works well when you have a single output.  When you have multiple outputs you might want to use the `dask.compute` function. This way Dask can share the intermediate values.\n    \nSo your goal is to parallelize the code above (which has been copied below) using `dask.delayed`.  You may also want to visualize a bit of the computation to see if you're doing it correctly.\n\n```python\n%%time\n# your code here\n```\n\n::: {#51fa937d .cell tags='[\"solution\"]' execution_count=33}\n``` {.python .cell-code}\n%%time\n\n# This is just one possible solution, there are\n# several ways to do this using `dask.delayed`\n\n\n@dask.delayed\ndef read_file(filename):\n    # Read in file\n    return pd.read_csv(filename)\n\n\nsums = []\ncounts = []\nfor fn in filenames:\n    # Delayed read in file\n    df = read_file(fn)\n\n    # Groupby origin airport\n    by_origin = df.groupby(\"Origin\")\n\n    # Sum of all departure delays by origin\n    total = by_origin.DepDelay.sum()\n\n    # Number of flights by origin\n    count = by_origin.DepDelay.count()\n\n    # Save the intermediates\n    sums.append(total)\n    counts.append(count)\n\n# Combine intermediates to get total mean-delay-per-origin\ntotal_delays = sum(sums)\nn_flights = sum(counts)\nmean, *_ = dask.compute(total_delays / n_flights)\n```\n:::\n\n\n::: {#20286a4b .cell execution_count=34}\n``` {.python .cell-code}\n# ensure the results still match\nmean\n```\n:::\n\n\n### Some questions to consider:\n\n- How much speedup did you get? Is this how much speedup you'd expect?\n- Experiment with where to call `compute`. What happens when you call it on `sums` and `counts`? What happens if you wait and call it on `mean`?\n- Experiment with delaying the call to `sum`. What does the graph look like if `sum` is delayed? What does the graph look like if it isn't?\n- Can you think of any reason why you'd want to do the reduction one way over the other?\n\n### Learn More\n\nVisit the [Delayed documentation](https://docs.dask.org/en/latest/delayed.html). In particular, this [delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU) will reinforce the concepts you learned here and the [delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html) document collects advice on using `dask.delayed` well.\n\nSome improvements by actual parallelization of file reading, but weâ€™re limited by IO throughput.\n\n::: {#60b8ded4 .cell tags='[\"solution\"]' execution_count=35}\n``` {.python .cell-code}\ndask.compute(sums)\n```\n:::\n\n\n::: {#ca4e68cd .cell tags='[\"solution\"]' execution_count=36}\n``` {.python .cell-code}\ndask.compute(counts)\n```\n:::\n\n\n::: {#d693b02d .cell tags='[\"solution\"]' execution_count=37}\n``` {.python .cell-code}\n%%time\n\n# This is just one possible solution, there are\n# several ways to do this using `dask.delayed`\n\n\n@dask.delayed\ndef read_file(filename):\n    # Read in file\n    return pd.read_csv(filename)\n\n\nsums = []\ncounts = []\nfor fn in filenames:\n    # Delayed read in file\n    df = read_file(fn)\n\n    # Groupby origin airport\n    by_origin = df.groupby(\"Origin\")\n\n    # Sum of all departure delays by origin\n    total = by_origin.DepDelay.sum()\n\n    # Number of flights by origin\n    count = by_origin.DepDelay.count()\n\n    # Save the intermediates\n    sums.append(total)\n    counts.append(count)\n\n# Combine intermediates to get total mean-delay-per-origin\ntotal_delays = dask.delayed(sum)(sums)\nn_flights = dask.delayed(sum)(counts)\nmean, *_ = dask.compute(total_delays / n_flights)\n```\n:::\n\n\n::: {#22e6873b .cell tags='[\"solution\"]' execution_count=38}\n``` {.python .cell-code}\ndask.delayed(sum)(sums).visualize()\n```\n:::\n\n\n::: {#98f30f07 .cell tags='[\"solution\"]' execution_count=39}\n``` {.python .cell-code}\nsum(sums).visualize()\n```\n:::\n\n\n:::solution\nDelaying the sum could be interestingâ€¯in case to make more balancedâ€¯loads between tasks.\n:::\n\n## Close the Client\n\nBefore moving on to the next exercise, make sure to close your client or stop this kernel.\n\n::: {#278012cf .cell execution_count=40}\n``` {.python .cell-code}\nclient.close()\n```\n:::\n\n\n",
    "supporting": [
      "5_1_Dask_delayed_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}