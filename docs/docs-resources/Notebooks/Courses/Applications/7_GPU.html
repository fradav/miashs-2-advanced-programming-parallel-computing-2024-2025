<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="François-David Collin">
<meta name="author" content="Ghislain Durif">

<title>GPU computing with Numba (introduction)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="7_GPU_files/libs/clipboard/clipboard.min.js"></script>
<script src="7_GPU_files/libs/quarto-html/quarto.js"></script>
<script src="7_GPU_files/libs/quarto-html/popper.min.js"></script>
<script src="7_GPU_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="7_GPU_files/libs/quarto-html/anchor.min.js"></script>
<link href="7_GPU_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="7_GPU_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="7_GPU_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="7_GPU_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="7_GPU_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="7_GPU_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="7_GPU_files/libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="7_GPU.ipynb" download="7_GPU.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">GPU computing with Numba (introduction)</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">François-David Collin <a href="mailto:francois-david.collin@umontpellier.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            CNRS
          </p>
        <p class="affiliation">
            IMAG
          </p>
        <p class="affiliation">
            Paul-Valéry Montpellier 3 University
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Ghislain Durif <a href="mailto:ghislain.durif@ens-lyon.fr" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            CNRS
          </p>
        <p class="affiliation">
            LBMC
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<blockquote class="blockquote">
<p>“<a href="https://numba.pydata.org/">Numba</a> is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.”</p>
</blockquote>
<p>Numba offers GPU support (through CUDA). See the official <a href="https://numba.pydata.org/numba-doc/latest/cuda/index.html">documentation</a> or this <a href="https://nyu-cds.github.io/python-numba/05-cuda/">NYU course</a></p>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<p>Several important terms in the topic of CUDA programming are listed here:</p>
<ul>
<li><p>host: the CPU along with the system memory (RAM)</p></li>
<li><p>device: the GPU</p></li>
<li><p>host memory: the system main memory</p></li>
<li><p>device memory: onboard memory on a GPU card</p></li>
<li><p>kernel: a GPU function launched by the host and executed on the device</p></li>
<li><p>device function: a GPU function executed on the device which can only be called from the device (i.e.&nbsp;from a kernel or another device function)</p></li>
</ul>
</section>
<section id="first-example-array-reduction" class="level2">
<h2 class="anchored" data-anchor-id="first-example-array-reduction">First example: array reduction</h2>
<p>In our examples: - 128 threads (on 64 CPU cores) to run CPU computing - Nvidia A10 GPU to run GPU computing</p>
<table class="caption-top table">
<caption>A10 Technical Specifications and Features</caption>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td>FP32</td>
<td>31.2 teraFLOPS</td>
</tr>
<tr class="even">
<td>TF32 Tensor Core</td>
<td>62.5 teraFLOPS</td>
</tr>
<tr class="odd">
<td>BFLOAT16 Tensor Core</td>
<td>125 teraFLOPS</td>
</tr>
<tr class="even">
<td>FP16 Tensor Core</td>
<td>125 teraFLOPS</td>
</tr>
<tr class="odd">
<td>INT8 Tensor Core</td>
<td>250 TOPS</td>
</tr>
<tr class="even">
<td>INT4 Tensor Core</td>
<td>500 TOPS</td>
</tr>
<tr class="odd">
<td>RT Core</td>
<td>72 RT Cores</td>
</tr>
<tr class="even">
<td>Encode/decode</td>
<td>1 encoder2 decoder (+AV1 decode)</td>
</tr>
<tr class="odd">
<td>GPU memory</td>
<td>24GB GDDR6</td>
</tr>
<tr class="even">
<td>GPU memory bandwidth</td>
<td>600GB/s</td>
</tr>
<tr class="odd">
<td>Interconnect</td>
<td>PCIe Gen4 64GB/s</td>
</tr>
<tr class="even">
<td>Form factors</td>
<td>Single-slot, full-height, full-length (FHFL)</td>
</tr>
<tr class="odd">
<td>Max thermal design power (TDP)</td>
<td>150W</td>
</tr>
<tr class="even">
<td>vGPU software support</td>
<td>NVIDIA Virtual PC, NVIDIA Virtual Applications, NVIDIA RTX Virtual Workstation, NVIDIA Virtual Compute Server, NVIDIA AI Enterprise</td>
</tr>
</tbody>
</table>
<section id="requirements" class="level3">
<h3 class="anchored" data-anchor-id="requirements">requirements</h3>
<div id="17defb31" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># import matplotlib.pyplot as plt</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda, set_num_threads</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>pio.renderers.default <span class="op">=</span> <span class="st">"notebook+plotly_mimetype+svg"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>set_num_threads(<span class="dv">128</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba.core.errors <span class="im">import</span> NumbaPerformanceWarning</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>warnings.simplefilter(<span class="st">'ignore'</span>, category<span class="op">=</span>NumbaPerformanceWarning)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="reduction-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="reduction-algorithm">Reduction algorithm</h3>
<div id="b10d8be8" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.reduce</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sum_reduce(a, b):</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Attention</strong>: the first call to <code>sum_reduce</code> will trigger a compilation step so with Numba operators, we always need to run a blank run first (on small data to avoid high computation time).</p>
<div id="58aff839" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># blank run</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">10</span>).astype(np.float32)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>sum_reduce(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="toy-example" class="level3">
<h3 class="anchored" data-anchor-id="toy-example">Toy example</h3>
<div id="df46936b" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generate a vector array of dimension 1E7 with random float32 elements</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000000</span>).astype(np.float32)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e4f56cee" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy sum reduction</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">800</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="779825ea" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>sum_reduce(A)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda sum reduction</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">800</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="which-one-is-faster" class="level4">
<h4 class="anchored" data-anchor-id="which-one-is-faster">Which one is faster ?</h4>
<div id="6f9dc0e6" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy sum reduction</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>t1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="48c8a31d" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda sum reduction</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>t2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="the-gpu-is-slower" class="level4">
<h4 class="anchored" data-anchor-id="the-gpu-is-slower">The GPU is slower????</h4>
</section>
</section>
<section id="benchmark-numpy-array-sum-vs-cuda-sum-reduction" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-numpy-array-sum-vs-cuda-sum-reduction">Benchmark numpy array sum vs cuda sum reduction</h3>
<div id="1b10ecd5" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark1(N):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy sum reduction</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda sum reduction</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="checking-increasing-vector-size" class="level4">
<h4 class="anchored" data-anchor-id="checking-increasing-vector-size">Checking increasing vector size</h4>
<div id="405ab0c5" class="cell" data-execution_count="10">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Powers of 2 vector</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>vec_size <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>exp <span class="cf">for</span> exp <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>,<span class="dv">28</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="540026ff" class="cell" data-execution_count="11">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check the list size candidates</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>px.scatter(y<span class="op">=</span>vec_size,width<span class="op">=</span><span class="dv">600</span>,labels<span class="op">=</span>{<span class="st">'y'</span>:<span class="st">"Data length"</span>,<span class="st">'x'</span>:<span class="st">"Vector index"</span>},log_y<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="run-the-benchmark" class="level4">
<h4 class="anchored" data-anchor-id="run-the-benchmark">Run the benchmark</h4>
<div id="5b5c80e6" class="cell" data-execution_count="12">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark1(N)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">1</span>]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="results" class="level4">
<h4 class="anchored" data-anchor-id="results">Results</h4>
<div id="1ae6152e" class="cell" data-execution_count="13">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df_res <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>px.line(df_res, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>It is confirmed!!! Why bother using GPU???</strong></p>
</section>
<section id="any-idea" class="level4">
<h4 class="anchored" data-anchor-id="any-idea">Any idea ?</h4>
</section>
</section>
<section id="bottleneck" class="level3">
<h3 class="anchored" data-anchor-id="bottleneck">Bottleneck</h3>
<ul>
<li>Host to device (GPU) memory copy</li>
</ul>
</section>
<section id="solution" class="level3">
<h3 class="anchored" data-anchor-id="solution">Solution</h3>
<ul>
<li>Copy data to device (GPU) before running the computations</li>
</ul>
<p>See Numba dedicated <a href="https://numba.pydata.org/numba-doc/latest/cuda/memory.html">page</a> for memory management.</p>
<div id="e8fe368f" class="cell" data-execution_count="14">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark2(N):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"sum of </span><span class="sc">{</span>N<span class="sc">}</span><span class="ss"> elements"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy sum reduction</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># copy data to device</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda sum reduction</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A_gpu)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="294f082e" class="cell" data-execution_count="15">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark2(N)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">1</span>]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="83fd6443" class="cell" data-execution_count="16">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># results</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>df_res2 <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>px.line(df_res2, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>GPU is better to do high throughput computing with larger matrices.</strong></p>
</section>
<section id="gpu-memory-overflow" class="level3">
<h3 class="anchored" data-anchor-id="gpu-memory-overflow">GPU memory overflow</h3>
<div id="099bff2f" class="cell" data-note="GPU memory overflow on small gpu" data-execution_count="17">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="bu">int</span>(<span class="fl">2E9</span>)).astype(np.float32)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    sum_reduce(A)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>To avoid memory overflow:</strong> - <a href="https://www.kernel-operations.io/keops/index.html">KeOps</a>: Kernel Operations (including matrix operations and reduction) on the GPU, with autodiff, without memory overflows</p>
<div id="d422ad39" class="cell" data-execution_count="18">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: CPU memory overflow</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="bu">int</span>(<span class="fl">1E12</span>)).astype(np.float32)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    sum_reduce(A)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="complex-operation-on-gpu" class="level3">
<h3 class="anchored" data-anchor-id="complex-operation-on-gpu">Complex operation on GPU ?</h3>
<p><span class="math display">\sum_i \vert x_i \vert = \sum_i (-1)^{I_{\{x_i &lt; 0\}}} x_i = \sum_{i,x_i \geq 0} x_i - \sum_{i,x_i &lt; 0} x_i</span></p>
<p>Example: if <span class="math inline">x = [-1, 3, 5, -2]</span> then we want to compute <span class="math inline">1 + 3 + 5 + 2</span></p>
<div id="4e489159" class="cell" data-execution_count="19">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Numpy reduce</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numpy_reduce(vec):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (vec <span class="op">*</span> np.where(vec <span class="op">&lt;</span> <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).<span class="bu">sum</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># numba cpu reduce</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> njit, prange, set_num_threads</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>set_num_threads(<span class="dv">8</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="at">@njit</span>(parallel<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numba_reduce(A):</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Without "parallel=True" in the jit-decorator</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the prange statement is equivalent to range</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> prange(A.shape[<span class="dv">0</span>]):</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> A[i] <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>            s <span class="op">+=</span> <span class="op">-</span>A[i]</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            s <span class="op">+=</span> A[i]</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda reduce</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.reduce</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_reduce(a, b):</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> b <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">-</span> b</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">+</span> b</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span>).astype(np.float32)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>numba_reduce(A)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>cuda_reduce(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="1a0ec900" class="cell" data-execution_count="20">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># benchmark</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark3(N):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"complex operations on </span><span class="sc">{</span>N<span class="sc">}</span><span class="ss"> elements"</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy reduction</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o numpy_reduce(A)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numba reduction</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o numba_reduce(A)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda reduction</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    t3 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o cuda_reduce(A_gpu)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average, t3.average</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="5bd245df" class="cell" data-execution_count="21">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>resspeedup <span class="op">=</span> []</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark3(N)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numba_128c'</span>: time_res[<span class="dv">1</span>],</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">2</span>],</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    resspeedup.append({</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy/numba'</span>: time_res[<span class="dv">0</span>]<span class="op">/</span>time_res[<span class="dv">1</span>],</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy/cuda'</span>: time_res[<span class="dv">0</span>]<span class="op">/</span>time_res[<span class="dv">2</span>]</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    })</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="64654eb6" class="cell" data-execution_count="22">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># results</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>df_res <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(df_res, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'numba_128c'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>df_resspeedup <span class="op">=</span> pd.DataFrame(resspeedup)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(df_resspeedup, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy/numba'</span>, <span class="st">'numpy/cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="numba-for-gpu-next-level" class="level2">
<h2 class="anchored" data-anchor-id="numba-for-gpu-next-level">Numba for GPU: next level</h2>
<section id="gpu-management" class="level3">
<h3 class="anchored" data-anchor-id="gpu-management">GPU management</h3>
<div id="1881f9a9" class="cell" data-execution_count="23">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to check available GPU</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gpu <span class="kw">in</span> cuda.list_devices():</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(gpu.name)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>cuda.get_current_device().name</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="kernel-declaration" class="level3">
<h3 class="anchored" data-anchor-id="kernel-declaration">Kernel declaration</h3>
<p>kernel function = GPU function meant to be called from CPU code</p>
<p>Characteristics: - kernels <strong>cannot explicitly return a value</strong> (all result data must be written to an array passed to the function) - kernels <strong>explicitly declare their thread hierarchy when called</strong>: i.e.&nbsp;the number of thread blocks and the number of threads per block</p>
<p><strong>Attention</strong>: Kernel function are compiled on their first call, we always need to run a blank run first (on small data to avoid high computation time).</p>
<div id="77f53f4b" class="cell" data-execution_count="24">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_kernel(io_array):</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Code for kernel.</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># code here</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="4a5a308a" class="cell" data-execution_count="25">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the data array - usually initialized some other way</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> numpy.ones(<span class="dv">256</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of threads in a block</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> <span class="dv">32</span> </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the number of thread blocks in the grid</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (data.size <span class="op">+</span> (threadsperblock <span class="op">-</span> <span class="dv">1</span>)) <span class="op">//</span> threadsperblock</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Now start the kernel</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>my_kernel[blockspergrid, threadsperblock](data)</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><strong>Important</strong>: you have to choose the number of blocks per grid (and hence the block size) and the number of threads per block. The product of the two will give the total number of threads launched.</p>
</section>
<section id="choosing-the-block-size" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-block-size">Choosing the block size</h3>
<p><a href="https://nyu-cds.github.io/python-numba/05-cuda/">Credit</a></p>
<p>The two-level thread hierarchy is important for the following reasons:</p>
<ul>
<li>On the software side, the block size determines how many threads share a given area of shared memory.</li>
<li>On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide.</li>
</ul>
<p>The block size you choose depends on a range of factors, including:</p>
<ul>
<li>The size of the data array</li>
<li>The size of the shared mempory per block (e.g.&nbsp;64KB)</li>
<li>The maximum number of threads per block supported by the hardware (e.g.&nbsp;512 or 1024)</li>
<li>The maximum number of threads per multiprocessor (MP) (e.g.&nbsp;2048)</li>
<li>The maximum number of blocks per MP (e.g.&nbsp;32)</li>
<li>The number of threads that can be executed concurrently (a “warp” i.e.&nbsp;32)</li>
</ul>
<p>The execution of threads in a warp has a big effect on the computational throughput. If all threads in a warp are executing the same instruction then they can all be executed in parallel. But if one or more threads is executing a different instruction, the warp has to be split into groups of threads, and these groups execute serially.</p>
<p><strong>Rules of thumb for threads per block:</strong></p>
<ul>
<li>Should be a round multiple of the warp size (32)</li>
<li>A good place to start is 128-512 but benchmarking is required to determine the optimal value.</li>
</ul>
<p>Each streaming multiprocessor (SP) on the GPU must have enough active warps to achieve maximum throughput. In other words, the blocksize is usually selected to maximize the “occupancy”. See the <a href="http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls">CUDA Occupancy Calculator spreadsheet</a>for more details.</p>
</section>
<section id="thread-positioning" class="level3">
<h3 class="anchored" data-anchor-id="thread-positioning">Thread positioning</h3>
<p><a href="https://nyu-cds.github.io/python-numba/05-cuda/">Credit</a></p>
<p>When running a kernel, the kernel function’s code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for. More complex algorithms may define more complex responsibilities, but the underlying principle is the same.</p>
<p>To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1-dimensional declarations of equivalent sizes, this doesn’t change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.</p>
</section>
<section id="matrix-product" class="level3">
<h3 class="anchored" data-anchor-id="matrix-product">Matrix product</h3>
<p>Numpy version for comparison</p>
<div id="d122c59d" class="cell" data-execution_count="26">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy version</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numpy_matmul(A, B):</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> np.matmul(A, B)</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> C</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="naive-cuda-version" class="level4">
<h4 class="anchored" data-anchor-id="naive-cuda-version">Naive cuda version</h4>
<div id="d49f4739" class="cell" data-execution_count="27">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_matmul(A, B, C):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform matrix multiplication of C = A * B</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    row, col <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row <span class="op">&lt;</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> col <span class="op">&lt;</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(A.shape[<span class="dv">1</span>]):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> A[row, k] <span class="op">*</span> B[k, col]</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        C[row, col] <span class="op">=</span> tmp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><img src="../figs/matmul.png" class="img-fluid"> <a href="https://nyu-cds.github.io/python-numba/05-cuda/">Credit</a></p>
</section>
<section id="run" class="level4">
<h4 class="anchored" data-anchor-id="run">Run</h4>
<div id="f963e340" class="cell" data-execution_count="28">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data 1000x1000 matrix</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span>).astype(np.float32).reshape((<span class="dv">1000</span>,<span class="dv">1000</span>))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span>).astype(np.float32).reshape((<span class="dv">1000</span>,<span class="dv">1000</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="003426c4" class="cell" data-execution_count="29">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy run</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o numpy_matmul(A, B)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="b661fc14" class="cell" data-execution_count="30">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Copy the arrays to the device</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>B_gpu <span class="op">=</span> cuda.to_device(B)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Allocate memory on the device for the result</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>C_gpu <span class="op">=</span> cuda.device_array((<span class="dv">24</span>, <span class="dv">22</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="373e342a" class="cell" data-execution_count="31">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the blocks</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>blockspergrid_x <span class="op">=</span> <span class="bu">int</span>(math.ceil(A.shape[<span class="dv">0</span>] <span class="op">/</span> threadsperblock[<span class="dv">0</span>]))</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>blockspergrid_y <span class="op">=</span> <span class="bu">int</span>(math.ceil(B.shape[<span class="dv">1</span>] <span class="op">/</span> threadsperblock[<span class="dv">1</span>]))</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (blockspergrid_x, blockspergrid_y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="2648684b" class="cell" data-execution_count="32">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA matrix multiplication</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_matmul1(A, B, C):</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform square matrix multiplication of C = A * B</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    i, j <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> j <span class="op">&lt;</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(A.shape[<span class="dv">1</span>]):</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> A[i, k] <span class="op">*</span> B[k, j]</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        C[i, j] <span class="op">=</span> tmp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="283a3867" class="cell" data-execution_count="33">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda run</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o cuda_matmul1[blockspergrid, threadsperblock](A_gpu, B_gpu, C_gpu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fd6b3bc4" class="cell" data-execution_count="34">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the result from the GPU</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> C_gpu.copy_to_host()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="621f5624" class="cell" data-execution_count="35">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda, float32</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Controls threads per block and shared memory usage.</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># The computation will be done on blocks of TPBxTPB elements.</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>TPB <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_fast_matmul(A, B, C):</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define an array in the shared memory</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The size and type of the arrays must be known at compile time</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    sA <span class="op">=</span> cuda.shared.array(shape<span class="op">=</span>(TPB, TPB), dtype<span class="op">=</span>float32)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    sB <span class="op">=</span> cuda.shared.array(shape<span class="op">=</span>(TPB, TPB), dtype<span class="op">=</span>float32)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    tx <span class="op">=</span> cuda.threadIdx.x</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    ty <span class="op">=</span> cuda.threadIdx.y</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>    bpg <span class="op">=</span> cuda.gridDim.x    <span class="co"># blocks per grid</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;=</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> y <span class="op">&gt;=</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quit if (x, y) is outside of valid C boundary</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread computes one element in the result matrix.</span></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The dot product is chunked into dot products of TPB-long vectors.</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>    tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bpg):</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Preload data into shared memory</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        sA[tx, ty] <span class="op">=</span> A[x, ty <span class="op">+</span> i <span class="op">*</span> TPB]</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        sB[tx, ty] <span class="op">=</span> B[tx <span class="op">+</span> i <span class="op">*</span> TPB, y]</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wait until all threads finish preloading</span></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Computes partial product on the shared memory</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(TPB):</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> sA[tx, j] <span class="op">*</span> sB[j, ty]</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wait until all threads finish computing</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>    C[x, y] <span class="op">=</span> tmp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="778a838d" class="cell" data-execution_count="36">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the blocks</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> (TPB, TPB)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>blockspergrid_x <span class="op">=</span> <span class="bu">int</span>(math.ceil(A.shape[<span class="dv">0</span>] <span class="op">/</span> threadsperblock[<span class="dv">1</span>]))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>blockspergrid_y <span class="op">=</span> <span class="bu">int</span>(math.ceil(B.shape[<span class="dv">1</span>] <span class="op">/</span> threadsperblock[<span class="dv">0</span>]))</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (blockspergrid_x, blockspergrid_y)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Start the kernel </span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o cuda_fast_matmul[blockspergrid, threadsperblock](A_gpu, B_gpu, C_gpu)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!-- -->

</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb37" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> GPU computing with Numba (introduction)</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "</span><span class="co">[</span><span class="ot">Numba</span><span class="co">](https://numba.pydata.org/)</span><span class="at"> is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code."</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>Numba offers GPU support (through CUDA). See the official <span class="co">[</span><span class="ot">documentation</span><span class="co">](https://numba.pydata.org/numba-doc/latest/cuda/index.html)</span> or this <span class="co">[</span><span class="ot">NYU course</span><span class="co">](https://nyu-cds.github.io/python-numba/05-cuda/)</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## Terminology</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>Several important terms in the topic of CUDA programming are listed here:</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>host: the CPU along with the system memory (RAM)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>device: the GPU</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>host memory: the system main memory</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>device memory: onboard memory on a GPU card</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>kernel: a GPU function launched by the host and executed on the device</span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## First example: array reduction</span></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>In our examples: - 128 threads (on 64 CPU cores) to run CPU computing - Nvidia A10 GPU to run GPU computing</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a>|                                |                                                                                                                                     |</span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>|------------------------------------|------------------------------------|</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>| FP32                           | 31.2 teraFLOPS                                                                                                                      |</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>| TF32 Tensor Core               | 62.5 teraFLOPS                                                                                                                      |</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a>| BFLOAT16 Tensor Core           | 125 teraFLOPS                                                                                                                       |</span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a>| FP16 Tensor Core               | 125 teraFLOPS                                                                                                                       |</span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>| INT8 Tensor Core               | 250 TOPS                                                                                                                            |</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>| INT4 Tensor Core               | 500 TOPS                                                                                                                            |</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a>| RT Core                        | 72 RT Cores                                                                                                                         |</span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a>| Encode/decode                  | 1 encoder2 decoder (+AV1 decode)                                                                                                    |</span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>| GPU memory                     | 24GB GDDR6                                                                                                                          |</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>| GPU memory bandwidth           | 600GB/s                                                                                                                             |</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>| Interconnect                   | PCIe Gen4 64GB/s                                                                                                                    |</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a>| Form factors                   | Single-slot, full-height, full-length (FHFL)                                                                                        |</span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a>| Max thermal design power (TDP) | 150W                                                                                                                                |</span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>| vGPU software support          | NVIDIA Virtual PC, NVIDIA Virtual Applications, NVIDIA RTX Virtual Workstation, NVIDIA Virtual Compute Server, NVIDIA AI Enterprise |</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>: A10 Technical Specifications and Features</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a><span class="fu">### requirements</span></span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-53"><a href="#cb37-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-54"><a href="#cb37-54" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb37-55"><a href="#cb37-55" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb37-56"><a href="#cb37-56" aria-hidden="true" tabindex="-1"></a><span class="co"># import matplotlib.pyplot as plt</span></span>
<span id="cb37-57"><a href="#cb37-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-58"><a href="#cb37-58" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda, set_num_threads</span>
<span id="cb37-59"><a href="#cb37-59" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.io <span class="im">as</span> pio</span>
<span id="cb37-60"><a href="#cb37-60" aria-hidden="true" tabindex="-1"></a>pio.renderers.default <span class="op">=</span> <span class="st">"notebook+plotly_mimetype+svg"</span></span>
<span id="cb37-61"><a href="#cb37-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-62"><a href="#cb37-62" aria-hidden="true" tabindex="-1"></a>set_num_threads(<span class="dv">128</span>)</span>
<span id="cb37-63"><a href="#cb37-63" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba.core.errors <span class="im">import</span> NumbaPerformanceWarning</span>
<span id="cb37-64"><a href="#cb37-64" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb37-65"><a href="#cb37-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-66"><a href="#cb37-66" aria-hidden="true" tabindex="-1"></a>warnings.simplefilter(<span class="st">'ignore'</span>, category<span class="op">=</span>NumbaPerformanceWarning)</span>
<span id="cb37-67"><a href="#cb37-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-68"><a href="#cb37-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-69"><a href="#cb37-69" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reduction algorithm</span></span>
<span id="cb37-70"><a href="#cb37-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-73"><a href="#cb37-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-74"><a href="#cb37-74" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.reduce</span></span>
<span id="cb37-75"><a href="#cb37-75" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sum_reduce(a, b):</span>
<span id="cb37-76"><a href="#cb37-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a <span class="op">+</span> b</span>
<span id="cb37-77"><a href="#cb37-77" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-78"><a href="#cb37-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-79"><a href="#cb37-79" aria-hidden="true" tabindex="-1"></a>**Attention**: the first call to <span class="in">`sum_reduce`</span> will trigger a compilation step so with Numba operators, we always need to run a blank run first (on small data to avoid high computation time).</span>
<span id="cb37-80"><a href="#cb37-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-83"><a href="#cb37-83" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-84"><a href="#cb37-84" aria-hidden="true" tabindex="-1"></a><span class="co"># blank run</span></span>
<span id="cb37-85"><a href="#cb37-85" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">10</span>).astype(np.float32)</span>
<span id="cb37-86"><a href="#cb37-86" aria-hidden="true" tabindex="-1"></a>sum_reduce(A)</span>
<span id="cb37-87"><a href="#cb37-87" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-88"><a href="#cb37-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-89"><a href="#cb37-89" aria-hidden="true" tabindex="-1"></a><span class="fu">### Toy example</span></span>
<span id="cb37-90"><a href="#cb37-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-93"><a href="#cb37-93" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-94"><a href="#cb37-94" aria-hidden="true" tabindex="-1"></a><span class="co"># generate a vector array of dimension 1E7 with random float32 elements</span></span>
<span id="cb37-95"><a href="#cb37-95" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000000</span>).astype(np.float32)</span>
<span id="cb37-96"><a href="#cb37-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-97"><a href="#cb37-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-100"><a href="#cb37-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-101"><a href="#cb37-101" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy sum reduction</span></span>
<span id="cb37-102"><a href="#cb37-102" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">800</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span>
<span id="cb37-103"><a href="#cb37-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-104"><a href="#cb37-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-107"><a href="#cb37-107" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-108"><a href="#cb37-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| scrolled: true</span></span>
<span id="cb37-109"><a href="#cb37-109" aria-hidden="true" tabindex="-1"></a>sum_reduce(A)</span>
<span id="cb37-110"><a href="#cb37-110" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda sum reduction</span></span>
<span id="cb37-111"><a href="#cb37-111" aria-hidden="true" tabindex="-1"></a>t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">800</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A)</span>
<span id="cb37-112"><a href="#cb37-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-113"><a href="#cb37-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-114"><a href="#cb37-114" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Which one is faster ?</span></span>
<span id="cb37-115"><a href="#cb37-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-118"><a href="#cb37-118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-119"><a href="#cb37-119" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy sum reduction</span></span>
<span id="cb37-120"><a href="#cb37-120" aria-hidden="true" tabindex="-1"></a>t1</span>
<span id="cb37-121"><a href="#cb37-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-122"><a href="#cb37-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-125"><a href="#cb37-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-126"><a href="#cb37-126" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda sum reduction</span></span>
<span id="cb37-127"><a href="#cb37-127" aria-hidden="true" tabindex="-1"></a>t2</span>
<span id="cb37-128"><a href="#cb37-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-129"><a href="#cb37-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-130"><a href="#cb37-130" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The GPU is slower????</span></span>
<span id="cb37-131"><a href="#cb37-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-132"><a href="#cb37-132" aria-hidden="true" tabindex="-1"></a><span class="fu">### Benchmark numpy array sum vs cuda sum reduction</span></span>
<span id="cb37-133"><a href="#cb37-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-136"><a href="#cb37-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-137"><a href="#cb37-137" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark1(N):</span>
<span id="cb37-138"><a href="#cb37-138" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb37-139"><a href="#cb37-139" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy sum reduction</span></span>
<span id="cb37-140"><a href="#cb37-140" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span>
<span id="cb37-141"><a href="#cb37-141" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda sum reduction</span></span>
<span id="cb37-142"><a href="#cb37-142" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A)</span>
<span id="cb37-143"><a href="#cb37-143" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb37-144"><a href="#cb37-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average</span>
<span id="cb37-145"><a href="#cb37-145" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-146"><a href="#cb37-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-147"><a href="#cb37-147" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Checking increasing vector size</span></span>
<span id="cb37-148"><a href="#cb37-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-151"><a href="#cb37-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-152"><a href="#cb37-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Powers of 2 vector</span></span>
<span id="cb37-153"><a href="#cb37-153" aria-hidden="true" tabindex="-1"></a>vec_size <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>exp <span class="cf">for</span> exp <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">12</span>,<span class="dv">28</span>)]</span>
<span id="cb37-154"><a href="#cb37-154" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-155"><a href="#cb37-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-158"><a href="#cb37-158" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-159"><a href="#cb37-159" aria-hidden="true" tabindex="-1"></a><span class="co"># check the list size candidates</span></span>
<span id="cb37-160"><a href="#cb37-160" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb37-161"><a href="#cb37-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-162"><a href="#cb37-162" aria-hidden="true" tabindex="-1"></a>px.scatter(y<span class="op">=</span>vec_size,width<span class="op">=</span><span class="dv">600</span>,labels<span class="op">=</span>{<span class="st">'y'</span>:<span class="st">"Data length"</span>,<span class="st">'x'</span>:<span class="st">"Vector index"</span>},log_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-163"><a href="#cb37-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-164"><a href="#cb37-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-165"><a href="#cb37-165" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Run the benchmark</span></span>
<span id="cb37-166"><a href="#cb37-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-169"><a href="#cb37-169" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-170"><a href="#cb37-170" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb37-171"><a href="#cb37-171" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.notebook <span class="im">import</span> tqdm</span>
<span id="cb37-172"><a href="#cb37-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-173"><a href="#cb37-173" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb37-174"><a href="#cb37-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-175"><a href="#cb37-175" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb37-176"><a href="#cb37-176" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-177"><a href="#cb37-177" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark1(N)</span>
<span id="cb37-178"><a href="#cb37-178" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-179"><a href="#cb37-179" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb37-180"><a href="#cb37-180" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb37-181"><a href="#cb37-181" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb37-182"><a href="#cb37-182" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">1</span>]</span>
<span id="cb37-183"><a href="#cb37-183" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb37-184"><a href="#cb37-184" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-185"><a href="#cb37-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-186"><a href="#cb37-186" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Results</span></span>
<span id="cb37-187"><a href="#cb37-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-190"><a href="#cb37-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-191"><a href="#cb37-191" aria-hidden="true" tabindex="-1"></a>df_res <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb37-192"><a href="#cb37-192" aria-hidden="true" tabindex="-1"></a>px.line(df_res, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-193"><a href="#cb37-193" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-194"><a href="#cb37-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-195"><a href="#cb37-195" aria-hidden="true" tabindex="-1"></a>**It is confirmed!!! Why bother using GPU???**</span>
<span id="cb37-196"><a href="#cb37-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-197"><a href="#cb37-197" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Any idea ?</span></span>
<span id="cb37-198"><a href="#cb37-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-199"><a href="#cb37-199" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bottleneck</span></span>
<span id="cb37-200"><a href="#cb37-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-201"><a href="#cb37-201" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Host to device (GPU) memory copy</span>
<span id="cb37-202"><a href="#cb37-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-203"><a href="#cb37-203" aria-hidden="true" tabindex="-1"></a><span class="fu">### Solution</span></span>
<span id="cb37-204"><a href="#cb37-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-205"><a href="#cb37-205" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Copy data to device (GPU) before running the computations</span>
<span id="cb37-206"><a href="#cb37-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-207"><a href="#cb37-207" aria-hidden="true" tabindex="-1"></a>See Numba dedicated <span class="co">[</span><span class="ot">page</span><span class="co">](https://numba.pydata.org/numba-doc/latest/cuda/memory.html)</span> for memory management.</span>
<span id="cb37-208"><a href="#cb37-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-211"><a href="#cb37-211" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-212"><a href="#cb37-212" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark2(N):</span>
<span id="cb37-213"><a href="#cb37-213" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"sum of </span><span class="sc">{</span>N<span class="sc">}</span><span class="ss"> elements"</span>)</span>
<span id="cb37-214"><a href="#cb37-214" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb37-215"><a href="#cb37-215" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy sum reduction</span></span>
<span id="cb37-216"><a href="#cb37-216" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o A.<span class="bu">sum</span>()</span>
<span id="cb37-217"><a href="#cb37-217" aria-hidden="true" tabindex="-1"></a>    <span class="co"># copy data to device</span></span>
<span id="cb37-218"><a href="#cb37-218" aria-hidden="true" tabindex="-1"></a>    A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb37-219"><a href="#cb37-219" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda sum reduction</span></span>
<span id="cb37-220"><a href="#cb37-220" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>q <span class="op">-</span>o sum_reduce(A_gpu)</span>
<span id="cb37-221"><a href="#cb37-221" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb37-222"><a href="#cb37-222" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average</span>
<span id="cb37-223"><a href="#cb37-223" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-224"><a href="#cb37-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-227"><a href="#cb37-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-228"><a href="#cb37-228" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb37-229"><a href="#cb37-229" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb37-230"><a href="#cb37-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-231"><a href="#cb37-231" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb37-232"><a href="#cb37-232" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-233"><a href="#cb37-233" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark2(N)</span>
<span id="cb37-234"><a href="#cb37-234" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-235"><a href="#cb37-235" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb37-236"><a href="#cb37-236" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb37-237"><a href="#cb37-237" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb37-238"><a href="#cb37-238" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">1</span>]</span>
<span id="cb37-239"><a href="#cb37-239" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb37-240"><a href="#cb37-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-241"><a href="#cb37-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-244"><a href="#cb37-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-245"><a href="#cb37-245" aria-hidden="true" tabindex="-1"></a><span class="co"># results</span></span>
<span id="cb37-246"><a href="#cb37-246" aria-hidden="true" tabindex="-1"></a>df_res2 <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb37-247"><a href="#cb37-247" aria-hidden="true" tabindex="-1"></a>px.line(df_res2, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-248"><a href="#cb37-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-249"><a href="#cb37-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-250"><a href="#cb37-250" aria-hidden="true" tabindex="-1"></a>**GPU is better to do high throughput computing with larger matrices.**</span>
<span id="cb37-251"><a href="#cb37-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-252"><a href="#cb37-252" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPU memory overflow</span></span>
<span id="cb37-253"><a href="#cb37-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-256"><a href="#cb37-256" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-257"><a href="#cb37-257" aria-hidden="true" tabindex="-1"></a><span class="co">#| error: true</span></span>
<span id="cb37-258"><a href="#cb37-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| Note: GPU memory overflow on small gpu</span></span>
<span id="cb37-259"><a href="#cb37-259" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb37-260"><a href="#cb37-260" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="bu">int</span>(<span class="fl">2E9</span>)).astype(np.float32)</span>
<span id="cb37-261"><a href="#cb37-261" aria-hidden="true" tabindex="-1"></a>    sum_reduce(A)</span>
<span id="cb37-262"><a href="#cb37-262" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb37-263"><a href="#cb37-263" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span>
<span id="cb37-264"><a href="#cb37-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-265"><a href="#cb37-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-266"><a href="#cb37-266" aria-hidden="true" tabindex="-1"></a>**To avoid memory overflow:** - <span class="co">[</span><span class="ot">KeOps</span><span class="co">](https://www.kernel-operations.io/keops/index.html)</span>: Kernel Operations (including matrix operations and reduction) on the GPU, with autodiff, without memory overflows</span>
<span id="cb37-267"><a href="#cb37-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-270"><a href="#cb37-270" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-271"><a href="#cb37-271" aria-hidden="true" tabindex="-1"></a><span class="co">#| error: true</span></span>
<span id="cb37-272"><a href="#cb37-272" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: CPU memory overflow</span></span>
<span id="cb37-273"><a href="#cb37-273" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb37-274"><a href="#cb37-274" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="bu">int</span>(<span class="fl">1E12</span>)).astype(np.float32)</span>
<span id="cb37-275"><a href="#cb37-275" aria-hidden="true" tabindex="-1"></a>    sum_reduce(A)</span>
<span id="cb37-276"><a href="#cb37-276" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb37-277"><a href="#cb37-277" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(e)</span>
<span id="cb37-278"><a href="#cb37-278" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-279"><a href="#cb37-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-280"><a href="#cb37-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Complex operation on GPU ?</span></span>
<span id="cb37-281"><a href="#cb37-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-282"><a href="#cb37-282" aria-hidden="true" tabindex="-1"></a>$$\sum_i \vert x_i \vert = \sum_i (-1)^{I_{<span class="sc">\{</span>x_i &lt; 0<span class="sc">\}</span>}} x_i = \sum_{i,x_i \geq 0} x_i - \sum_{i,x_i &lt; 0} x_i$$</span>
<span id="cb37-283"><a href="#cb37-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-284"><a href="#cb37-284" aria-hidden="true" tabindex="-1"></a>Example: if $x = <span class="co">[</span><span class="ot">-1, 3, 5, -2</span><span class="co">]</span>$ then we want to compute $1 + 3 + 5 + 2$</span>
<span id="cb37-285"><a href="#cb37-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-288"><a href="#cb37-288" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-289"><a href="#cb37-289" aria-hidden="true" tabindex="-1"></a><span class="co"># Numpy reduce</span></span>
<span id="cb37-290"><a href="#cb37-290" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numpy_reduce(vec):</span>
<span id="cb37-291"><a href="#cb37-291" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (vec <span class="op">*</span> np.where(vec <span class="op">&lt;</span> <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)).<span class="bu">sum</span>()</span>
<span id="cb37-292"><a href="#cb37-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-293"><a href="#cb37-293" aria-hidden="true" tabindex="-1"></a><span class="co"># numba cpu reduce</span></span>
<span id="cb37-294"><a href="#cb37-294" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> njit, prange, set_num_threads</span>
<span id="cb37-295"><a href="#cb37-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-296"><a href="#cb37-296" aria-hidden="true" tabindex="-1"></a>set_num_threads(<span class="dv">8</span>)</span>
<span id="cb37-297"><a href="#cb37-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-298"><a href="#cb37-298" aria-hidden="true" tabindex="-1"></a><span class="at">@njit</span>(parallel<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-299"><a href="#cb37-299" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numba_reduce(A):</span>
<span id="cb37-300"><a href="#cb37-300" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb37-301"><a href="#cb37-301" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Without "parallel=True" in the jit-decorator</span></span>
<span id="cb37-302"><a href="#cb37-302" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the prange statement is equivalent to range</span></span>
<span id="cb37-303"><a href="#cb37-303" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> prange(A.shape[<span class="dv">0</span>]):</span>
<span id="cb37-304"><a href="#cb37-304" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> A[i] <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb37-305"><a href="#cb37-305" aria-hidden="true" tabindex="-1"></a>            s <span class="op">+=</span> <span class="op">-</span>A[i]</span>
<span id="cb37-306"><a href="#cb37-306" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb37-307"><a href="#cb37-307" aria-hidden="true" tabindex="-1"></a>            s <span class="op">+=</span> A[i]</span>
<span id="cb37-308"><a href="#cb37-308" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> s</span>
<span id="cb37-309"><a href="#cb37-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-310"><a href="#cb37-310" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda reduce</span></span>
<span id="cb37-311"><a href="#cb37-311" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.reduce</span></span>
<span id="cb37-312"><a href="#cb37-312" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_reduce(a, b):</span>
<span id="cb37-313"><a href="#cb37-313" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> b <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb37-314"><a href="#cb37-314" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">-</span> b</span>
<span id="cb37-315"><a href="#cb37-315" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb37-316"><a href="#cb37-316" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> a <span class="op">+</span> b</span>
<span id="cb37-317"><a href="#cb37-317" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-318"><a href="#cb37-318" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span>).astype(np.float32)</span>
<span id="cb37-319"><a href="#cb37-319" aria-hidden="true" tabindex="-1"></a>numba_reduce(A)</span>
<span id="cb37-320"><a href="#cb37-320" aria-hidden="true" tabindex="-1"></a>cuda_reduce(A)</span>
<span id="cb37-321"><a href="#cb37-321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-322"><a href="#cb37-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-325"><a href="#cb37-325" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-326"><a href="#cb37-326" aria-hidden="true" tabindex="-1"></a><span class="co"># benchmark</span></span>
<span id="cb37-327"><a href="#cb37-327" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark3(N):</span>
<span id="cb37-328"><a href="#cb37-328" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"complex operations on </span><span class="sc">{</span>N<span class="sc">}</span><span class="ss"> elements"</span>)</span>
<span id="cb37-329"><a href="#cb37-329" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>N).astype(np.float32)</span>
<span id="cb37-330"><a href="#cb37-330" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numpy reduction</span></span>
<span id="cb37-331"><a href="#cb37-331" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o numpy_reduce(A)</span>
<span id="cb37-332"><a href="#cb37-332" aria-hidden="true" tabindex="-1"></a>    <span class="co"># numba reduction</span></span>
<span id="cb37-333"><a href="#cb37-333" aria-hidden="true" tabindex="-1"></a>    t2 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o numba_reduce(A)</span>
<span id="cb37-334"><a href="#cb37-334" aria-hidden="true" tabindex="-1"></a>    <span class="co"># cuda reduction</span></span>
<span id="cb37-335"><a href="#cb37-335" aria-hidden="true" tabindex="-1"></a>    A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb37-336"><a href="#cb37-336" aria-hidden="true" tabindex="-1"></a>    t3 <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">2</span> <span class="op">-</span>n <span class="dv">5</span> <span class="op">-</span>q <span class="op">-</span>o cuda_reduce(A_gpu)</span>
<span id="cb37-337"><a href="#cb37-337" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output</span></span>
<span id="cb37-338"><a href="#cb37-338" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t1.average, t2.average, t3.average</span>
<span id="cb37-339"><a href="#cb37-339" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-340"><a href="#cb37-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-343"><a href="#cb37-343" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-344"><a href="#cb37-344" aria-hidden="true" tabindex="-1"></a><span class="co"># run the benchmark</span></span>
<span id="cb37-345"><a href="#cb37-345" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb37-346"><a href="#cb37-346" aria-hidden="true" tabindex="-1"></a>resspeedup <span class="op">=</span> []</span>
<span id="cb37-347"><a href="#cb37-347" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tqdm(vec_size):</span>
<span id="cb37-348"><a href="#cb37-348" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-349"><a href="#cb37-349" aria-hidden="true" tabindex="-1"></a>    time_res <span class="op">=</span> benchmark3(N)</span>
<span id="cb37-350"><a href="#cb37-350" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-351"><a href="#cb37-351" aria-hidden="true" tabindex="-1"></a>    res.append({</span>
<span id="cb37-352"><a href="#cb37-352" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb37-353"><a href="#cb37-353" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy'</span>: time_res[<span class="dv">0</span>],</span>
<span id="cb37-354"><a href="#cb37-354" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numba_128c'</span>: time_res[<span class="dv">1</span>],</span>
<span id="cb37-355"><a href="#cb37-355" aria-hidden="true" tabindex="-1"></a>        <span class="st">'cuda'</span>: time_res[<span class="dv">2</span>],</span>
<span id="cb37-356"><a href="#cb37-356" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb37-357"><a href="#cb37-357" aria-hidden="true" tabindex="-1"></a>    resspeedup.append({</span>
<span id="cb37-358"><a href="#cb37-358" aria-hidden="true" tabindex="-1"></a>        <span class="st">'N'</span>: N,</span>
<span id="cb37-359"><a href="#cb37-359" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy/numba'</span>: time_res[<span class="dv">0</span>]<span class="op">/</span>time_res[<span class="dv">1</span>],</span>
<span id="cb37-360"><a href="#cb37-360" aria-hidden="true" tabindex="-1"></a>        <span class="st">'numpy/cuda'</span>: time_res[<span class="dv">0</span>]<span class="op">/</span>time_res[<span class="dv">2</span>]</span>
<span id="cb37-361"><a href="#cb37-361" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb37-362"><a href="#cb37-362" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-363"><a href="#cb37-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-366"><a href="#cb37-366" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-367"><a href="#cb37-367" aria-hidden="true" tabindex="-1"></a><span class="co"># results</span></span>
<span id="cb37-368"><a href="#cb37-368" aria-hidden="true" tabindex="-1"></a>df_res <span class="op">=</span> pd.DataFrame(res)</span>
<span id="cb37-369"><a href="#cb37-369" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(df_res, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy'</span>, <span class="st">'numba_128c'</span>, <span class="st">'cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-370"><a href="#cb37-370" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb37-371"><a href="#cb37-371" aria-hidden="true" tabindex="-1"></a>df_resspeedup <span class="op">=</span> pd.DataFrame(resspeedup)</span>
<span id="cb37-372"><a href="#cb37-372" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.line(df_resspeedup, x<span class="op">=</span><span class="st">'N'</span>, y<span class="op">=</span>[<span class="st">'numpy/numba'</span>, <span class="st">'numpy/cuda'</span>], log_y<span class="op">=</span><span class="va">True</span>, log_x<span class="op">=</span><span class="va">True</span>, width<span class="op">=</span><span class="dv">600</span>)</span>
<span id="cb37-373"><a href="#cb37-373" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb37-374"><a href="#cb37-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-375"><a href="#cb37-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-376"><a href="#cb37-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## Numba for GPU: next level</span></span>
<span id="cb37-377"><a href="#cb37-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-378"><a href="#cb37-378" aria-hidden="true" tabindex="-1"></a><span class="fu">### GPU management</span></span>
<span id="cb37-379"><a href="#cb37-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-382"><a href="#cb37-382" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-383"><a href="#cb37-383" aria-hidden="true" tabindex="-1"></a><span class="co"># to check available GPU</span></span>
<span id="cb37-384"><a href="#cb37-384" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb37-385"><a href="#cb37-385" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> gpu <span class="kw">in</span> cuda.list_devices():</span>
<span id="cb37-386"><a href="#cb37-386" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(gpu.name)</span>
<span id="cb37-387"><a href="#cb37-387" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-388"><a href="#cb37-388" aria-hidden="true" tabindex="-1"></a>cuda.get_current_device().name</span>
<span id="cb37-389"><a href="#cb37-389" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-390"><a href="#cb37-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-391"><a href="#cb37-391" aria-hidden="true" tabindex="-1"></a><span class="fu">### Kernel declaration</span></span>
<span id="cb37-392"><a href="#cb37-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-393"><a href="#cb37-393" aria-hidden="true" tabindex="-1"></a>kernel function = GPU function meant to be called from CPU code</span>
<span id="cb37-394"><a href="#cb37-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-395"><a href="#cb37-395" aria-hidden="true" tabindex="-1"></a>Characteristics: - kernels **cannot explicitly return a value** (all result data must be written to an array passed to the function) - kernels **explicitly declare their thread hierarchy when called**: i.e. the number of thread blocks and the number of threads per block</span>
<span id="cb37-396"><a href="#cb37-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-397"><a href="#cb37-397" aria-hidden="true" tabindex="-1"></a>**Attention**: Kernel function are compiled on their first call, we always need to run a blank run first (on small data to avoid high computation time).</span>
<span id="cb37-398"><a href="#cb37-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-401"><a href="#cb37-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-402"><a href="#cb37-402" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb37-403"><a href="#cb37-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-404"><a href="#cb37-404" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb37-405"><a href="#cb37-405" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_kernel(io_array):</span>
<span id="cb37-406"><a href="#cb37-406" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb37-407"><a href="#cb37-407" aria-hidden="true" tabindex="-1"></a><span class="co">    Code for kernel.</span></span>
<span id="cb37-408"><a href="#cb37-408" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb37-409"><a href="#cb37-409" aria-hidden="true" tabindex="-1"></a>    <span class="co"># code here</span></span>
<span id="cb37-410"><a href="#cb37-410" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-411"><a href="#cb37-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-414"><a href="#cb37-414" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-415"><a href="#cb37-415" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy</span>
<span id="cb37-416"><a href="#cb37-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-417"><a href="#cb37-417" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the data array - usually initialized some other way</span></span>
<span id="cb37-418"><a href="#cb37-418" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> numpy.ones(<span class="dv">256</span>)</span>
<span id="cb37-419"><a href="#cb37-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-420"><a href="#cb37-420" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the number of threads in a block</span></span>
<span id="cb37-421"><a href="#cb37-421" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> <span class="dv">32</span> </span>
<span id="cb37-422"><a href="#cb37-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-423"><a href="#cb37-423" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the number of thread blocks in the grid</span></span>
<span id="cb37-424"><a href="#cb37-424" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (data.size <span class="op">+</span> (threadsperblock <span class="op">-</span> <span class="dv">1</span>)) <span class="op">//</span> threadsperblock</span>
<span id="cb37-425"><a href="#cb37-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-426"><a href="#cb37-426" aria-hidden="true" tabindex="-1"></a><span class="co"># Now start the kernel</span></span>
<span id="cb37-427"><a href="#cb37-427" aria-hidden="true" tabindex="-1"></a>my_kernel[blockspergrid, threadsperblock](data)</span>
<span id="cb37-428"><a href="#cb37-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-429"><a href="#cb37-429" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb37-430"><a href="#cb37-430" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data)</span>
<span id="cb37-431"><a href="#cb37-431" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-432"><a href="#cb37-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-433"><a href="#cb37-433" aria-hidden="true" tabindex="-1"></a>**Important**: you have to choose the number of blocks per grid (and hence the block size) and the number of threads per block. The product of the two will give the total number of threads launched.</span>
<span id="cb37-434"><a href="#cb37-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-435"><a href="#cb37-435" aria-hidden="true" tabindex="-1"></a><span class="fu">### Choosing the block size</span></span>
<span id="cb37-436"><a href="#cb37-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-437"><a href="#cb37-437" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Credit</span><span class="co">](https://nyu-cds.github.io/python-numba/05-cuda/)</span></span>
<span id="cb37-438"><a href="#cb37-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-439"><a href="#cb37-439" aria-hidden="true" tabindex="-1"></a>The two-level thread hierarchy is important for the following reasons:</span>
<span id="cb37-440"><a href="#cb37-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-441"><a href="#cb37-441" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>On the software side, the block size determines how many threads share a given area of shared memory.</span>
<span id="cb37-442"><a href="#cb37-442" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide.</span>
<span id="cb37-443"><a href="#cb37-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-444"><a href="#cb37-444" aria-hidden="true" tabindex="-1"></a>The block size you choose depends on a range of factors, including:</span>
<span id="cb37-445"><a href="#cb37-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-446"><a href="#cb37-446" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The size of the data array</span>
<span id="cb37-447"><a href="#cb37-447" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The size of the shared mempory per block (e.g. 64KB)</span>
<span id="cb37-448"><a href="#cb37-448" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)</span>
<span id="cb37-449"><a href="#cb37-449" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The maximum number of threads per multiprocessor (MP) (e.g. 2048)</span>
<span id="cb37-450"><a href="#cb37-450" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The maximum number of blocks per MP (e.g. 32)</span>
<span id="cb37-451"><a href="#cb37-451" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>The number of threads that can be executed concurrently (a "warp" i.e. 32)</span>
<span id="cb37-452"><a href="#cb37-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-453"><a href="#cb37-453" aria-hidden="true" tabindex="-1"></a>The execution of threads in a warp has a big effect on the computational throughput. If all threads in a warp are executing the same instruction then they can all be executed in parallel. But if one or more threads is executing a different instruction, the warp has to be split into groups of threads, and these groups execute serially.</span>
<span id="cb37-454"><a href="#cb37-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-455"><a href="#cb37-455" aria-hidden="true" tabindex="-1"></a>**Rules of thumb for threads per block:**</span>
<span id="cb37-456"><a href="#cb37-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-457"><a href="#cb37-457" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>Should be a round multiple of the warp size (32)</span>
<span id="cb37-458"><a href="#cb37-458" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>A good place to start is 128-512 but benchmarking is required to determine the optimal value.</span>
<span id="cb37-459"><a href="#cb37-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-460"><a href="#cb37-460" aria-hidden="true" tabindex="-1"></a>Each streaming multiprocessor (SP) on the GPU must have enough active warps to achieve maximum throughput. In other words, the blocksize is usually selected to maximize the "occupancy". See the <span class="co">[</span><span class="ot">CUDA Occupancy Calculator spreadsheet</span><span class="co">](http://developer.download.nvidia.com/compute/cuda/CUDA_Occupancy_calculator.xls)</span>for more details.</span>
<span id="cb37-461"><a href="#cb37-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-462"><a href="#cb37-462" aria-hidden="true" tabindex="-1"></a><span class="fu">### Thread positioning</span></span>
<span id="cb37-463"><a href="#cb37-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-464"><a href="#cb37-464" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Credit</span><span class="co">](https://nyu-cds.github.io/python-numba/05-cuda/)</span></span>
<span id="cb37-465"><a href="#cb37-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-466"><a href="#cb37-466" aria-hidden="true" tabindex="-1"></a>When running a kernel, the kernel function's code is executed by every thread once. It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for. More complex algorithms may define more complex responsibilities, but the underlying principle is the same.</span>
<span id="cb37-467"><a href="#cb37-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-468"><a href="#cb37-468" aria-hidden="true" tabindex="-1"></a>To help deal with multi-dimensional arrays, CUDA allows you to specify multi-dimensional blocks and grids. In the example above, you could make blockspergrid and threadsperblock tuples of one, two or three integers. Compared to 1-dimensional declarations of equivalent sizes, this doesn't change anything to the efficiency or behaviour of generated code, but can help you write your algorithms in a more natural way.</span>
<span id="cb37-469"><a href="#cb37-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-470"><a href="#cb37-470" aria-hidden="true" tabindex="-1"></a><span class="fu">### Matrix product</span></span>
<span id="cb37-471"><a href="#cb37-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-472"><a href="#cb37-472" aria-hidden="true" tabindex="-1"></a>Numpy version for comparison</span>
<span id="cb37-473"><a href="#cb37-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-476"><a href="#cb37-476" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-477"><a href="#cb37-477" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy version</span></span>
<span id="cb37-478"><a href="#cb37-478" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> numpy_matmul(A, B):</span>
<span id="cb37-479"><a href="#cb37-479" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> np.matmul(A, B)</span>
<span id="cb37-480"><a href="#cb37-480" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> C</span>
<span id="cb37-481"><a href="#cb37-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-482"><a href="#cb37-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-483"><a href="#cb37-483" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Naive cuda version</span></span>
<span id="cb37-484"><a href="#cb37-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-487"><a href="#cb37-487" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-488"><a href="#cb37-488" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb37-489"><a href="#cb37-489" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_matmul(A, B, C):</span>
<span id="cb37-490"><a href="#cb37-490" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform matrix multiplication of C = A * B</span></span>
<span id="cb37-491"><a href="#cb37-491" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb37-492"><a href="#cb37-492" aria-hidden="true" tabindex="-1"></a>    row, col <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb37-493"><a href="#cb37-493" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> row <span class="op">&lt;</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> col <span class="op">&lt;</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb37-494"><a href="#cb37-494" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb37-495"><a href="#cb37-495" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(A.shape[<span class="dv">1</span>]):</span>
<span id="cb37-496"><a href="#cb37-496" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> A[row, k] <span class="op">*</span> B[k, col]</span>
<span id="cb37-497"><a href="#cb37-497" aria-hidden="true" tabindex="-1"></a>        C[row, col] <span class="op">=</span> tmp</span>
<span id="cb37-498"><a href="#cb37-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-499"><a href="#cb37-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-500"><a href="#cb37-500" aria-hidden="true" tabindex="-1"></a><span class="al">![](../figs/matmul.png)</span> <span class="co">[</span><span class="ot">Credit</span><span class="co">](https://nyu-cds.github.io/python-numba/05-cuda/)</span></span>
<span id="cb37-501"><a href="#cb37-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-502"><a href="#cb37-502" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Run</span></span>
<span id="cb37-503"><a href="#cb37-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-506"><a href="#cb37-506" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-507"><a href="#cb37-507" aria-hidden="true" tabindex="-1"></a><span class="co"># data 1000x1000 matrix</span></span>
<span id="cb37-508"><a href="#cb37-508" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span>).astype(np.float32).reshape((<span class="dv">1000</span>,<span class="dv">1000</span>))</span>
<span id="cb37-509"><a href="#cb37-509" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span><span class="dv">1000</span><span class="op">**</span><span class="dv">2</span>).astype(np.float32).reshape((<span class="dv">1000</span>,<span class="dv">1000</span>))</span>
<span id="cb37-510"><a href="#cb37-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-511"><a href="#cb37-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-514"><a href="#cb37-514" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-515"><a href="#cb37-515" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy run</span></span>
<span id="cb37-516"><a href="#cb37-516" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o numpy_matmul(A, B)</span>
<span id="cb37-517"><a href="#cb37-517" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-518"><a href="#cb37-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-521"><a href="#cb37-521" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-522"><a href="#cb37-522" aria-hidden="true" tabindex="-1"></a><span class="co"># Copy the arrays to the device</span></span>
<span id="cb37-523"><a href="#cb37-523" aria-hidden="true" tabindex="-1"></a>A_gpu <span class="op">=</span> cuda.to_device(A)</span>
<span id="cb37-524"><a href="#cb37-524" aria-hidden="true" tabindex="-1"></a>B_gpu <span class="op">=</span> cuda.to_device(B)</span>
<span id="cb37-525"><a href="#cb37-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-526"><a href="#cb37-526" aria-hidden="true" tabindex="-1"></a><span class="co"># Allocate memory on the device for the result</span></span>
<span id="cb37-527"><a href="#cb37-527" aria-hidden="true" tabindex="-1"></a>C_gpu <span class="op">=</span> cuda.device_array((<span class="dv">24</span>, <span class="dv">22</span>))</span>
<span id="cb37-528"><a href="#cb37-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-529"><a href="#cb37-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-532"><a href="#cb37-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-533"><a href="#cb37-533" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the blocks</span></span>
<span id="cb37-534"><a href="#cb37-534" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb37-535"><a href="#cb37-535" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">16</span>)</span>
<span id="cb37-536"><a href="#cb37-536" aria-hidden="true" tabindex="-1"></a>blockspergrid_x <span class="op">=</span> <span class="bu">int</span>(math.ceil(A.shape[<span class="dv">0</span>] <span class="op">/</span> threadsperblock[<span class="dv">0</span>]))</span>
<span id="cb37-537"><a href="#cb37-537" aria-hidden="true" tabindex="-1"></a>blockspergrid_y <span class="op">=</span> <span class="bu">int</span>(math.ceil(B.shape[<span class="dv">1</span>] <span class="op">/</span> threadsperblock[<span class="dv">1</span>]))</span>
<span id="cb37-538"><a href="#cb37-538" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (blockspergrid_x, blockspergrid_y)</span>
<span id="cb37-539"><a href="#cb37-539" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-540"><a href="#cb37-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-543"><a href="#cb37-543" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-544"><a href="#cb37-544" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb37-545"><a href="#cb37-545" aria-hidden="true" tabindex="-1"></a><span class="co"># CUDA matrix multiplication</span></span>
<span id="cb37-546"><a href="#cb37-546" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb37-547"><a href="#cb37-547" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb37-548"><a href="#cb37-548" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_matmul1(A, B, C):</span>
<span id="cb37-549"><a href="#cb37-549" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Perform square matrix multiplication of C = A * B</span></span>
<span id="cb37-550"><a href="#cb37-550" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb37-551"><a href="#cb37-551" aria-hidden="true" tabindex="-1"></a>    i, j <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb37-552"><a href="#cb37-552" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> j <span class="op">&lt;</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb37-553"><a href="#cb37-553" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb37-554"><a href="#cb37-554" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(A.shape[<span class="dv">1</span>]):</span>
<span id="cb37-555"><a href="#cb37-555" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> A[i, k] <span class="op">*</span> B[k, j]</span>
<span id="cb37-556"><a href="#cb37-556" aria-hidden="true" tabindex="-1"></a>        C[i, j] <span class="op">=</span> tmp</span>
<span id="cb37-557"><a href="#cb37-557" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-558"><a href="#cb37-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-561"><a href="#cb37-561" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-562"><a href="#cb37-562" aria-hidden="true" tabindex="-1"></a><span class="co"># cuda run</span></span>
<span id="cb37-563"><a href="#cb37-563" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o cuda_matmul1[blockspergrid, threadsperblock](A_gpu, B_gpu, C_gpu)</span>
<span id="cb37-564"><a href="#cb37-564" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-565"><a href="#cb37-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-568"><a href="#cb37-568" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-569"><a href="#cb37-569" aria-hidden="true" tabindex="-1"></a><span class="co"># get the result from the GPU</span></span>
<span id="cb37-570"><a href="#cb37-570" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> C_gpu.copy_to_host()</span>
<span id="cb37-571"><a href="#cb37-571" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-572"><a href="#cb37-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-575"><a href="#cb37-575" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-576"><a href="#cb37-576" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda, float32</span>
<span id="cb37-577"><a href="#cb37-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-578"><a href="#cb37-578" aria-hidden="true" tabindex="-1"></a><span class="co"># Controls threads per block and shared memory usage.</span></span>
<span id="cb37-579"><a href="#cb37-579" aria-hidden="true" tabindex="-1"></a><span class="co"># The computation will be done on blocks of TPBxTPB elements.</span></span>
<span id="cb37-580"><a href="#cb37-580" aria-hidden="true" tabindex="-1"></a>TPB <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb37-581"><a href="#cb37-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-582"><a href="#cb37-582" aria-hidden="true" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb37-583"><a href="#cb37-583" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cuda_fast_matmul(A, B, C):</span>
<span id="cb37-584"><a href="#cb37-584" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define an array in the shared memory</span></span>
<span id="cb37-585"><a href="#cb37-585" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The size and type of the arrays must be known at compile time</span></span>
<span id="cb37-586"><a href="#cb37-586" aria-hidden="true" tabindex="-1"></a>    sA <span class="op">=</span> cuda.shared.array(shape<span class="op">=</span>(TPB, TPB), dtype<span class="op">=</span>float32)</span>
<span id="cb37-587"><a href="#cb37-587" aria-hidden="true" tabindex="-1"></a>    sB <span class="op">=</span> cuda.shared.array(shape<span class="op">=</span>(TPB, TPB), dtype<span class="op">=</span>float32)</span>
<span id="cb37-588"><a href="#cb37-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-589"><a href="#cb37-589" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> cuda.grid(<span class="dv">2</span>)</span>
<span id="cb37-590"><a href="#cb37-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-591"><a href="#cb37-591" aria-hidden="true" tabindex="-1"></a>    tx <span class="op">=</span> cuda.threadIdx.x</span>
<span id="cb37-592"><a href="#cb37-592" aria-hidden="true" tabindex="-1"></a>    ty <span class="op">=</span> cuda.threadIdx.y</span>
<span id="cb37-593"><a href="#cb37-593" aria-hidden="true" tabindex="-1"></a>    bpg <span class="op">=</span> cuda.gridDim.x    <span class="co"># blocks per grid</span></span>
<span id="cb37-594"><a href="#cb37-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-595"><a href="#cb37-595" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;=</span> C.shape[<span class="dv">0</span>] <span class="kw">and</span> y <span class="op">&gt;=</span> C.shape[<span class="dv">1</span>]:</span>
<span id="cb37-596"><a href="#cb37-596" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Quit if (x, y) is outside of valid C boundary</span></span>
<span id="cb37-597"><a href="#cb37-597" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb37-598"><a href="#cb37-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-599"><a href="#cb37-599" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Each thread computes one element in the result matrix.</span></span>
<span id="cb37-600"><a href="#cb37-600" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The dot product is chunked into dot products of TPB-long vectors.</span></span>
<span id="cb37-601"><a href="#cb37-601" aria-hidden="true" tabindex="-1"></a>    tmp <span class="op">=</span> <span class="fl">0.</span></span>
<span id="cb37-602"><a href="#cb37-602" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(bpg):</span>
<span id="cb37-603"><a href="#cb37-603" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Preload data into shared memory</span></span>
<span id="cb37-604"><a href="#cb37-604" aria-hidden="true" tabindex="-1"></a>        sA[tx, ty] <span class="op">=</span> A[x, ty <span class="op">+</span> i <span class="op">*</span> TPB]</span>
<span id="cb37-605"><a href="#cb37-605" aria-hidden="true" tabindex="-1"></a>        sB[tx, ty] <span class="op">=</span> B[tx <span class="op">+</span> i <span class="op">*</span> TPB, y]</span>
<span id="cb37-606"><a href="#cb37-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-607"><a href="#cb37-607" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wait until all threads finish preloading</span></span>
<span id="cb37-608"><a href="#cb37-608" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()</span>
<span id="cb37-609"><a href="#cb37-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-610"><a href="#cb37-610" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Computes partial product on the shared memory</span></span>
<span id="cb37-611"><a href="#cb37-611" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(TPB):</span>
<span id="cb37-612"><a href="#cb37-612" aria-hidden="true" tabindex="-1"></a>            tmp <span class="op">+=</span> sA[tx, j] <span class="op">*</span> sB[j, ty]</span>
<span id="cb37-613"><a href="#cb37-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-614"><a href="#cb37-614" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wait until all threads finish computing</span></span>
<span id="cb37-615"><a href="#cb37-615" aria-hidden="true" tabindex="-1"></a>        cuda.syncthreads()</span>
<span id="cb37-616"><a href="#cb37-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-617"><a href="#cb37-617" aria-hidden="true" tabindex="-1"></a>    C[x, y] <span class="op">=</span> tmp</span>
<span id="cb37-618"><a href="#cb37-618" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb37-619"><a href="#cb37-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-622"><a href="#cb37-622" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb37-623"><a href="#cb37-623" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the blocks</span></span>
<span id="cb37-624"><a href="#cb37-624" aria-hidden="true" tabindex="-1"></a>threadsperblock <span class="op">=</span> (TPB, TPB)</span>
<span id="cb37-625"><a href="#cb37-625" aria-hidden="true" tabindex="-1"></a>blockspergrid_x <span class="op">=</span> <span class="bu">int</span>(math.ceil(A.shape[<span class="dv">0</span>] <span class="op">/</span> threadsperblock[<span class="dv">1</span>]))</span>
<span id="cb37-626"><a href="#cb37-626" aria-hidden="true" tabindex="-1"></a>blockspergrid_y <span class="op">=</span> <span class="bu">int</span>(math.ceil(B.shape[<span class="dv">1</span>] <span class="op">/</span> threadsperblock[<span class="dv">0</span>]))</span>
<span id="cb37-627"><a href="#cb37-627" aria-hidden="true" tabindex="-1"></a>blockspergrid <span class="op">=</span> (blockspergrid_x, blockspergrid_y)</span>
<span id="cb37-628"><a href="#cb37-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-629"><a href="#cb37-629" aria-hidden="true" tabindex="-1"></a><span class="co"># Start the kernel </span></span>
<span id="cb37-630"><a href="#cb37-630" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>r <span class="dv">5</span> <span class="op">-</span>n <span class="dv">20</span> <span class="op">-</span>q <span class="op">-</span>o cuda_fast_matmul[blockspergrid, threadsperblock](A_gpu, B_gpu, C_gpu)</span>
<span id="cb37-631"><a href="#cb37-631" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>