[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced programming and parallel computing",
    "section": "",
    "text": "Preface\nThis is the course and materials for the lecture on “Advanced programming and parallel computing” at the Paul Valery University of Montpellier, France.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Prerequisites",
    "section": "",
    "text": "1.1 Infrastructure\nIn this course, we will be using the following infrastructure:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#infrastructure",
    "href": "intro.html#infrastructure",
    "title": "1  Prerequisites",
    "section": "",
    "text": "a Jupyterhub instance, hosted direcly at IMAG the lab where I work. Don’t use it directly, I’ll provide a speficic link for each of you to connect. This serve as a “hub” to launch your own JupyterLab instance.\na JupyterLab instance, hosted on a cluster MESO@LR. You will have exatly one instance each. This instance will be, depending on the context of the course, configured to run on essentially two different ways:\n\neither it will be configured to run on a single node, with a single CPU. This is the case for the first part of the course, where we will focus on the basics of parallel programming.\nor it will be configured to run on multiple nodes, . This is the case for the second part of the course, where we will focus on distributed programming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#software-required",
    "href": "intro.html#software-required",
    "title": "1  Prerequisites",
    "section": "1.2 Software required",
    "text": "1.2 Software required\nYou will need to install the following software on your computer: Visual Studio Code (VSCode), a free and open-source code editor. You’ll have to install the following extensions:\n\n Python extension to have everything you need to work with Python.\n Jupyter to have everything you need to work with Jupyter notebooks.\n JupyterHub to connect to the JupyterHub instance.\n Live Share to enable collaborative editing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-use-the-jupyterlab-instance-in-vscode",
    "href": "intro.html#how-to-use-the-jupyterlab-instance-in-vscode",
    "title": "1  Prerequisites",
    "section": "1.3 How to use the JupyterLab instance in VSCode",
    "text": "1.3 How to use the JupyterLab instance in VSCode\n\n1.3.1 Prerequisite: get your own JupyterLab instance url\nIn the table I provided you, you should have a line with:\n\nyour name\nyour username (which begin with e_miashs-XX where XX is the number of your account)1\na onesecret link type of link : \nAs the decryption key, simply provide your username: \nAnd then you get the token for your jupyterhub instance: , save it somewhere.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis works only once. If you close the tab or didn’t properly save the link you’ll have to ask me for a new onesecret link.\n\n\n\n\n1.3.2 Connect to the JupyterLab instance with VSCode\n\nFirst, bring down the command palette with Ctrl+Shift+P (or Cmd+Shift+P on macOS). Then, choose “Jupyter: Launch Interactive Window”. \nIt will open a new tab in vscode, click on “Select kernel” like this: \nChoose “Existing Jupyter Server” : \nThen it will ask you for the URL of the JupyterLab instance, which is https://jupyterhub.imag.umontpellier.fr : \nEnter the login name, which is e_miashs-XX : \nPaste the token you got from the onesecret link, and then press enter: \nThen it will ask you for a display name for the instance, you can put whatever you want, and press enter: \nAnd finally, you have to choose for the kernel, choose “Python 3” and voilà \n\n\n\n\n\n\n\n\nFirst cell is slow\n\n\n\nIt could take a few seconds when you enter your first cell of code for the kernel to start.\n\n\n\n\n\n\n\n\n\nAll is remote!\n\n\n\nAs you can see, from now on, every python interactive execution you do in VSCode will be done on your remote JupyterLab instance (the shell hostname returned the name of the cluster node on which the instance is running). Therefore, the code you execute can’t use any local file on your computer, even if your code is in a local file. Generally everything you’ll need as resources and files will be provided by me.\n\n\n\n\n\n\n\n\nIn case you really need to use a local file\n\n\n\nIn the case you need to use a local file, you’ll have to upload it to your JupyterLab instance, by using direct shell commands in the interactive window for example.\n\n\n\n\n\n\n\n\nBrowser vs VSCode\n\n\n\nOf course, you can use the JupyterLab instance directly in your browser, but I strongly recommend you to use VSCode, because:\n\nIt will be much more convenient to work with.\nWe will be able to use collaborative editing in practice sessions.\n\n\n\n\n\n\n\n\n\nLosing connection\n\n\n\nWhen you’ll lose your connection to the JupyterLab instance in VSCode (for example if you close the tab, exit from VSCode, or simply your laptop goes to sleep), you also lose the current state of your session in the stance (you have to reexecute all the cells you executed to restore the state of the session).\nIt also could ask for your username again, just press enter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#collaborative-editing",
    "href": "intro.html#collaborative-editing",
    "title": "1  Prerequisites",
    "section": "1.4 Collaborative editing",
    "text": "1.4 Collaborative editing\n\nIn the discord channel, I’ll provide you a link to join a collaborative editing session. Don’t click on it, just copy it: \nThen open a new “blank” window in VSCode, which will be exclusively for collaborative session. \nThen, click on the “Live Share” button in the bottom left corner of the window \nClick on the “Join” button \nEither choose anonymous or sign in with your github/microsoft account \n\n\n\n\n\n\n\nAnonymous Guest Name\n\n\n\nIf you choose to sign in, you’ll have to authorize VSCode to access your github/microsoft account. If you choose anonymous, you’ll have to choose a username. Please choose a username that is easily identifiable as yours.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Prerequisites",
    "section": "",
    "text": "In the case of any problem, contact me ASAP, and specify your username.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html",
    "href": "Courses/1_Intro.html",
    "title": "2  Introduction to parallel computing",
    "section": "",
    "text": "3 Parallel computing: the intuition",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing",
    "href": "Courses/1_Intro.html#computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.1 Computing ?",
    "text": "3.1 Computing ?\n\na computation = a succession of tasks to complete\na task \\approx a single command/action or a group of commands/actions\n\n\n\nExample 1:\n# task i:\n# sum of elements at index i\n# from two vectors\nfor i in range(10):\n    res[i] = a[i] + b[i]\n\nExample 2:\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#why-parallel-computing",
    "href": "Courses/1_Intro.html#why-parallel-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.2 Why parallel computing?",
    "text": "3.2 Why parallel computing?\n\nObjective: accelerate computations &lt;=&gt; reduce computation time\nIdea: run multiple tasks in parallel instead of sequentially",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-1",
    "href": "Courses/1_Intro.html#context-level-1",
    "title": "2  Introduction to parallel computing",
    "section": "3.3 Context (level 1)",
    "text": "3.3 Context (level 1)\n\ndifferent tasks to complete\none or more workers to complete the tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#sequential-computing",
    "href": "Courses/1_Intro.html#sequential-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.4 Sequential computing",
    "text": "3.4 Sequential computing\n\n\n\nn tasks to complete (n&gt;1)\n1 worker\n\nTotal time =\n\n\n(exercise)\n\\sum_{i=1}^n t_i \\sim O(n)\\ with t_i time to complete task i}\n\n\n\n      ┌────────┐\n      │worker 1│\n      └────────┘\n  ┌─\n  │     task 1\n  │       │\n  │       ▼\n  │     task 2\n  │       │\n  │       ▼\n  │     task 3\n  │       .\n  │       .\n  ▼       .\n\nTime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-computing-the-most-simple-case",
    "href": "Courses/1_Intro.html#parallel-computing-the-most-simple-case",
    "title": "2  Introduction to parallel computing",
    "section": "3.5 Parallel computing (the most simple case)",
    "text": "3.5 Parallel computing (the most simple case)\n      ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐\n      │worker 1│  │worker 2│  │worker 3│  │worker 4│  ...\n      └────────┘  └────────┘  └────────┘  └────────┘\n  ┌─\n  │     task 1      task 2      task 3      task 4    ...\n  ▼\n\nTime\n\nn tasks to complete (n&gt;1)\np workers (p&gt;=n)\n\n\n\nTotal time (exercise)\n\n\\underset{i=1,\\dots,n}{\\text{max}}\\{t_i\\}\\sim O(1)\\ with t_i time to complete task i\n\n\nPotential bottleneck? (exercise)\n\nnot enough workers to complete all tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#task-scheduling",
    "href": "Courses/1_Intro.html#task-scheduling",
    "title": "2  Introduction to parallel computing",
    "section": "3.6 Task scheduling",
    "text": "3.6 Task scheduling\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\n\nNeed: assign multiple tasks to each worker (and manage this assignment)\n\n      ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐\n      │worker 1│  │worker 2│  │worker 3│  │worker 4│  ...\n      └────────┘  └────────┘  └────────┘  └────────┘\n  ┌─\n  │     task 1      task 2      task 3      task 4    ...\n  │       │           │           │           │\n  │       ▼           ▼           ▼           ▼\n  │    task p+1    task p+2    task p+3    task p+4\n  │       │           │           │           │\n  │       ▼           ▼           ▼           ▼\n  │       .           .           .           .\n  │       .           .           .           .\n  ▼       .           .           .           .\n\nTime\n\n\nTotal time (exercise)\n\n\\underset{k=1,\\dots,p}{\\text{max}}\\{T_k\\}\\sim O(n/p)\\ with T_k = \\sum_{i\\in I_k} t_i, total time to complete all tasks assigned to worker k (where I_k is the set of indexes of tasks assigned to worker k)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-parallel-computing-simple-case",
    "href": "Courses/1_Intro.html#illustration-parallel-computing-simple-case",
    "title": "2  Introduction to parallel computing",
    "section": "3.7 Illustration: parallel computing (simple case)",
    "text": "3.7 Illustration: parallel computing (simple case)\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\nNumber of workers: 1, 2, 4, 6, 8\n\nWhy is the time gain not linear?\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-2",
    "href": "Courses/1_Intro.html#context-level-2",
    "title": "2  Introduction to parallel computing",
    "section": "3.8 Context (level 2)",
    "text": "3.8 Context (level 2)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources1\n\nPotential bottleneck? (exercise)\n\nnot enough resources for all workers",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#resource-management",
    "href": "Courses/1_Intro.html#resource-management",
    "title": "2  Introduction to parallel computing",
    "section": "3.9 Resource management",
    "text": "3.9 Resource management\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\n\nNeed:\n\nassign workers to each resource (and manage this assignment)\n\nTotal time = ? (exercise)\nPotential issues? (exercise)\n\n       ┌──────────┐      ┌──────────┐\n       │resource 1│      │resource 2│      ...\n       └──────────┘      └──────────┘\n  ┌─\n  │       task 1            task 2         ...\n  │     (worker 1)        (worker 2)\n  │         │                 │\n  │         ▼                 ▼\n  │       task 3            task 4\n  │     (worker 3)        (worker 4)\n  │         │                 │\n  │         ▼                 ▼\n  │      task p+1          task p+2\n  │     (worker 1)        (worker 2)\n  │         │                 │\n  │         ▼                 ▼\n  │      task p+3          task p+4\n  │     (worker 3)        (worker 4)\n  │         │                 │\n  │         ▼                 ▼\n  │         .                 .\n  │         .                 .\n  ▼         .                 .\n\nTime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#resource-management-1",
    "href": "Courses/1_Intro.html#resource-management-1",
    "title": "2  Introduction to parallel computing",
    "section": "3.10 Resource management",
    "text": "3.10 Resource management\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\\sim O(n/q)\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_i = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks assigned done on resource \\ell)\nPotential issues? multiple workers want to use the same working resources\n\nthey have to wait for their turn (workers are not working all the time)\nrisk to jam2 resource access (organizing resource access takes time)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-overhead-for-resource-access",
    "href": "Courses/1_Intro.html#illustration-overhead-for-resource-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.11 Illustration: overhead for resource access",
    "text": "3.11 Illustration: overhead for resource access\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\n8 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-3-realistic",
    "href": "Courses/1_Intro.html#context-level-3-realistic",
    "title": "2  Introduction to parallel computing",
    "section": "3.12 Context (level 3: realistic)",
    "text": "3.12 Context (level 3: realistic)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources\n\nInput/Output (I/O)\n\nInput: each task requires some materials (data) to be completed, these materials are stored in a storage area (memory)\nOutput: each task returns a result that need to be put in the storage area (memory)\n\nExamples: vector/matrix/array operations, process the content of multiple files",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#inputoutput-management",
    "href": "Courses/1_Intro.html#inputoutput-management",
    "title": "2  Introduction to parallel computing",
    "section": "3.13 Input/Output management",
    "text": "3.13 Input/Output management\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\ntasks need input (data) and produce output (results)\n\nNeed:\n\nload input (data) from storage when needed by a worker to complete a task\nwrite output (result) to storage when a task is completed\n\nTotal time = ? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-computing-realistic-model",
    "href": "Courses/1_Intro.html#parallel-computing-realistic-model",
    "title": "2  Introduction to parallel computing",
    "section": "3.14 Parallel computing: realistic model",
    "text": "3.14 Parallel computing: realistic model\n┌──────────┐\n│resource 1│     load        task 1         write       load        task 3         write\n└──────────┘    data 1 ──► (worker 1) ──► result 1 ──► data 3 ──► (worker 3) ──► result 3 ──► . . .\n\n\n┌──────────┐\n│resource 2│     load        task 2         write       load        task 4         write\n└──────────┘    data 2 ──► (worker 2) ──► result 2 ──► data 4 ──► (worker 4) ──► result 4 ──► . . .\n\n     .\n     .\n     .\n\n             └─────────────────────────────────────────────────────────────────────────────────────►\n                                                                                                 Time",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing-time-and-potential-bottleneck",
    "href": "Courses/1_Intro.html#computing-time-and-potential-bottleneck",
    "title": "2  Introduction to parallel computing",
    "section": "3.15 Computing time and potential bottleneck",
    "text": "3.15 Computing time and potential bottleneck\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_{i,\\text{in}} + t_i + t_{i,\\text{out}} = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks done on resource \\ell)\nPotential bottlenecks:\n\ninput (data) are not ready/available when a worker need them to complete a task (the worker have to wait)\noutput (results) cannot be written when a worker complete a task (the worker have to wait)\n\nOverhead on memory access\n\nconcurrent access to a memory space when reading input and/or when writing output\nconcurrent data transfer from or to memory (the “pipe” are jammed)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-1-overhead-for-io-access",
    "href": "Courses/1_Intro.html#illustration-1-overhead-for-io-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.16 Illustration 1: overhead for I/O access",
    "text": "3.16 Illustration 1: overhead for I/O access\n\n\n\na task\n\nsimulate a vector of 10 values\ncompute the mean\n\nObjective: run 10000 tasks\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-2-overhead-for-io-access",
    "href": "Courses/1_Intro.html#illustration-2-overhead-for-io-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.17 Illustration 2: overhead for I/O access",
    "text": "3.17 Illustration 2: overhead for I/O access\n\n\n\na task = “compute the sum of a given row in a matrix”\nObjective: compute all row-wise sums for a 10000 \\times 1000 matrix (i.e. 10000 tasks)\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#the-vocabulary-of-parallel-computing",
    "href": "Courses/1_Intro.html#the-vocabulary-of-parallel-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.18 The vocabulary of parallel computing",
    "text": "3.18 The vocabulary of parallel computing\n\ntasks = a command or a group of commands\nworker = a program or a sub-program (like a thread or a sub-process) → Software\nworking resources = processing units → Hardware\ninput = data\noutput = result\nstorage = memory\n\nAttention: “worker” may sometimes refer to a working resource in the literature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#task-synchronization",
    "href": "Courses/1_Intro.html#task-synchronization",
    "title": "2  Introduction to parallel computing",
    "section": "3.19 Task synchronization",
    "text": "3.19 Task synchronization\n\nSometimes tasks cannot be done in parallel\n\nSpecific case: output of task i_1 is input of task i_2\nNeed: wait for task i_1 before task i_2 starts\n\n\n\n\nExample 1:\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)\n\nExample 2:\n\ntask 1: train a predictive model\ntask 2: use the trained model to predict new labels",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing-resources",
    "href": "Courses/1_Intro.html#computing-resources",
    "title": "2  Introduction to parallel computing",
    "section": "4.1 Computing resources",
    "text": "4.1 Computing resources\n\na single computer\n\na persistent memory (hard drive) with very slow access\na non-persistent shared memory (RAM) with faster access\none or more computing units called CPUs3 (central processing units) linked to the RAM\nmaybe one or more GPUs (graphical processing units) linked to the RAM\n\nmultiple computers linked through a network (very slow communication)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#cpu-central-processing-unit",
    "href": "Courses/1_Intro.html#cpu-central-processing-unit",
    "title": "2  Introduction to parallel computing",
    "section": "4.2 CPU (central processing unit)",
    "text": "4.2 CPU (central processing unit)\n\n\n\nmulti-core CPU: multiple computing units (called “cores”) in a single processor\ndifferent level of local memory called “cache”\nto run a computation: transfer data from shared memory to local cache (and vice-versa for results) \\rightarrow potential bottleneck\n\n\n           ┌─────────────────┬───────────────────────┐\n           │                 │                       │\n┌──────────┴──┐     ┌─────── │ ────────┐    ┌─────── │ ────────┐\n│ MEMORY      │     │ CPU1   │         │    │ CPU2   │         │\n│             │     │ ┌──────┴───────┐ │    │ ┌──────┴───────┐ │\n│             │     │ │ Local Memory │ │    │ │ Local Memory │ │\n│             │     │ └──────┬───────┘ │    │ └──────┬───────┘ │\n│             │     │        │         │    │        │         │\n│             │     │ ┌───┐  │  ┌───┐  │    │ ┌───┐  │  ┌───┐  │\n│             │     │ │ C ├──┼──┤ C │  │    │ │ C ├──┼──┤ C │  │\n│             │     │ └───┘  │  └───┘  │    │ └───┘  │  └───┘  │\n└─────────────┘     │        │         │    │        │         │\n                    │ ┌───┐  │  ┌───┐  │    │ ┌───┐  │  ┌───┐  │\n                    │ │ C ├──┴──┤ C │  │    │ │ C ├──┴──┤ C │  │\n                    │ └───┘     └───┘  │    │ └───┘     └───┘  │\n                    │                  │    │                  │\n                    └──────────────────┘    └──────────────────┘",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#gpu-graphical-processing-units",
    "href": "Courses/1_Intro.html#gpu-graphical-processing-units",
    "title": "2  Introduction to parallel computing",
    "section": "4.3 GPU (graphical processing units)",
    "text": "4.3 GPU (graphical processing units)\n\n\n\n“many-core” computing card\nlocal memory\nslower connection to shared memory than CPUs\nto run a computation: transfer data from host shared memory to local memory (and vice-versa for results)\n\n\\rightarrow potential bottleneck\n\n         ┌───────────────────────────────────────────┐\n         │                                           │\n         │ ┌─────────────────┐                       │\n         │ │                 │                       │\n┌────────┴─┴──┐     ┌─────── │ ────────┐    ┌─────── │ ─────────┐\n│ MEMORY      │     │ CPU1   │         │    │ GPU    │          │\n│             │     │ ┌──────┴───────┐ │    │ ┌──────┴───────┐  │\n│             │     │ │ Local Memory │ │    │ │ Local Memory │  │\n│             │     │ └──────┬───────┘ │    │ └──────┬───────┘  │\n│             │     │        │         │    │        │          │\n│             │     │ ┌───┐  │  ┌───┐  │    │  ┌─┬─┬─┼─┬─┬─┬─┐  │\n│             │     │ │ C ├──┼──┤ C │  │    │  │C│C│C│C│C│C│C│  │\n│             │     │ └───┘  │  └───┘  │    │  ├─┼─┼─┼─┼─┼─┼─┤  │\n└─────────────┘     │        │         │    │  │C│C│C│C│C│C│C│  │\n                    │ ┌───┐  │  ┌───┐  │    │  ├─┼─┼─┼─┼─┼─┼─┤  │\n                    │ │ C ├──┴──┤ C │  │    │  │C│C│C│C│C│C│C│  │\n                    │ └───┘     └───┘  │    │  └─┴─┴─┴─┴─┴─┴─┘  │\n                    │                  │    │                   │\n                    └──────────────────┘    └───────────────────┘\n\n\n\n\n\nSource: wikimedia.org\n\n\n\n\n\nCPU\nGPU\n\n\n\n\ntens (10x) of computing units (“cores”)\nthousand (1000x) of computing units (“cores”)\n\n\ncomputing units capable of more complex operations\ncomputing units only capable of more simple operations\n\n\nlarger cache memory per computing unit\nvery small cache memory per computing unit\n\n\nfaster access to RAM\nslower access to RAM\n\n\n\\rightarrow efficient for general purpose parallel programming (e.g. check conditions)\n\\rightarrow fast for massively parallel computations based on simple elementary operations (e.g. linear algebra)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum",
    "href": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.1 Row/Column-wise matrix sum",
    "text": "5.1 Row/Column-wise matrix sum\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N \\times P\n\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\n\n\nRow-wise sum: vector C = [c_{i}]_{i=1:N} of size N where c_{i} = \\sum_{j=1}^P a_{ij}\nColumn-wise sum: vector D = [d_{j}]_{j=1:P} of size P where d_{j} = \\sum_{i=1}^N a_{ij}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#row-wise-sum",
    "href": "Courses/1_Intro.html#row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.2 Row-wise sum",
    "text": "5.2 Row-wise sum\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\ \\ \\rightarrow \\ \\ \\begin{bmatrix}\n    \\vdots \\\\\n    \\vdots \\\\\n    \\sum_{j=1}^{P} a_{ij} \\\\\n    \\vdots\\\\\n    \\vdots\\\\\n  \\end{bmatrix}_{N \\times 1}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#column-wise-sum",
    "href": "Courses/1_Intro.html#column-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.3 Column-wise sum",
    "text": "5.3 Column-wise sum\n\n\\begin{array}{c}\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\\\\n  \\downarrow \\ \\ \\ \\ \\ \\ \\\\\n\\begin{bmatrix}\n   \\ \\dots \\  & \\dots & \\sum_{i=1}^{N} a_{ij} & \\dots & \\ \\dots \\ \\\\\n  \\end{bmatrix}_{1\\times P}\\\\\n\\end{array}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.4 Row/Column-wise matrix sum algorithm",
    "text": "5.4 Row/Column-wise matrix sum algorithm\n\n\nRow-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nColumn-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\n\nExercise: parallel algorithm?\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\n\nExercise: any concurrent access to memory by the parallel tasks ? in input (reading) ? in output (writing) ?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#concurrent-access-to-memory",
    "href": "Courses/1_Intro.html#concurrent-access-to-memory",
    "title": "2  Introduction to parallel computing",
    "section": "5.5 Concurrent access to memory",
    "text": "5.5 Concurrent access to memory\nSolution 1:\n\nreading (input): no concurrent access\nwriting (output): no concurrent access\n\nSolution 2:\n\nreading (input): no concurrent access\nwriting (output): what happen if tasks j_1 and j_2 need to simultaneously update vecD[i]?\n\n\n\\rightarrow need for synchronization (with a time cost)\n\n\n\nSolution 3?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\nSolution 4?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  @parallel\n  for j in range(P):\n    vecD[i] += matA[i,j]\n(Concurrent access between tasks to update vecD[i])\n\n\nAny other issue ?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#cost-of-parallel-task-management",
    "href": "Courses/1_Intro.html#cost-of-parallel-task-management",
    "title": "2  Introduction to parallel computing",
    "section": "5.6 Cost of parallel task management",
    "text": "5.6 Cost of parallel task management\n\n\n@parallel\nfor i in range(N):\n  for j in range(P):\n    ...\n1 launch of N parallel tasks running each P operations\n\\rightarrow N “long” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(N)\n\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    ...\nP launches of N parallel tasks running each 1 operation\n\\rightarrow N \\times P “short” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(NP)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-column-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#parallel-column-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.7 Parallel column-wise matrix sum algorithm",
    "text": "5.7 Parallel column-wise matrix sum algorithm\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[j] += matA[i,j]\nConcurrent access between tasks to update vecD[j]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.8 Illustration: column/row-wise matrix sum algorithm",
    "text": "5.8 Illustration: column/row-wise matrix sum algorithm\nParallel column-wise vs parallel row-wise matrix sum algorithms\n\n\n\nMatrix 10000 \\times 10000\nObjective: run 10000 tasks\nResources: 64 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\nExercise 1: why the performance degradation?\n\n{\\rightarrow overhead for memory access}\n\nExercise 2: why the performance difference?\n\n{\\rightarrow impact of array storage order}\n\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#array-storage-order",
    "href": "Courses/1_Intro.html#array-storage-order",
    "title": "2  Introduction to parallel computing",
    "section": "5.9 Array storage order",
    "text": "5.9 Array storage order\nMatrix in memory = a big array of contiguous rows or columns\n\n\n\n5.9.1 Row-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n5.9.2 Column-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{21} & a_{31} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{12} & a_{22} & a_{32} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{13} & a_{23} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#accessing-array-elements-in-memory",
    "href": "Courses/1_Intro.html#accessing-array-elements-in-memory",
    "title": "2  Introduction to parallel computing",
    "section": "5.10 Accessing array elements in memory",
    "text": "5.10 Accessing array elements in memory\nMemory access: read data from memory by block\n\n\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#row-major-order-and-row-wise-sum",
    "href": "Courses/1_Intro.html#row-major-order-and-row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.11 Row-major order and row-wise sum",
    "text": "5.11 Row-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\ncompute res = res + a_{12}\ncompute res = res + a_{13}\n\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#column-major-order-and-row-wise-sum",
    "href": "Courses/1_Intro.html#column-major-order-and-row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.12 Column-major order and row-wise sum",
    "text": "5.12 Column-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\nload \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{12}\nload \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{13} More memory accesses \\rightarrow time consuming\n\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#memory-access-to-large-data",
    "href": "Courses/1_Intro.html#memory-access-to-large-data",
    "title": "2  Introduction to parallel computing",
    "section": "5.13 Memory access to large data",
    "text": "5.13 Memory access to large data\nExample: “big” matrix (4 \\times 6) \\tiny \\begin{array}{|c|c|c|c|c|c|c|}\\hline a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\ \\hline a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\ \\hline a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\ \\hline a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46} \\\\ \\hline\\end{array}\nStorage in memory (row major):\n\\tiny\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|ccc}\n\\hline\na_{41} & a_{42} & \\cdot & \\cdot & \\cdot \\\\\n\\hline\n\\end{array}\n\nAccess by sub-blocks4 of data (e.g. sub-block of rows or columns)\n\nExample: load block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache to access a_{11}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#impact-of-data-dimension",
    "href": "Courses/1_Intro.html#impact-of-data-dimension",
    "title": "2  Introduction to parallel computing",
    "section": "5.14 Impact of data dimension",
    "text": "5.14 Impact of data dimension\nSum of row 1 (row major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} res = res + a_{11} + a_{12} + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{15} & a_{16} \\\\ \\hline \\end{array} res = res + a_{14} + a_{15} + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#impact-of-data-dimension-ii",
    "href": "Courses/1_Intro.html#impact-of-data-dimension-ii",
    "title": "2  Introduction to parallel computing",
    "section": "5.15 Impact of data dimension, II",
    "text": "5.15 Impact of data dimension, II\nSum of row 1 (column major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} res = res + a_{11}\naccess block \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} res = res + a_{12}\naccess block \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} res = res + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{24} & a_{34} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{15} & a_{25} & a_{35} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{16} & a_{26} & a_{36} \\\\ \\hline \\end{array} res = res + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#matrix-product",
    "href": "Courses/1_Intro.html#matrix-product",
    "title": "2  Introduction to parallel computing",
    "section": "5.16 Matrix product",
    "text": "5.16 Matrix product\n\n\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N\\times{}P\nMatrix B = [b_{jk}]_{j=1:P}^{k=1:Q} of dimension N\\times{}P\nMatrix product: C = A \\times B = [c_{ik}]_{i=1:N}^{k=1:Q} of dimension N\\times{}Q where\n\nc_{ik} = \\sum_{j=1}^P a_{ij} \\times b_{jk}\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#matrix-product-algorithm",
    "href": "Courses/1_Intro.html#matrix-product-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.17 Matrix product algorithm",
    "text": "5.17 Matrix product algorithm\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]\nExercise: parallel algorithm?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#naive-parallel-matrix-product",
    "href": "Courses/1_Intro.html#naive-parallel-matrix-product",
    "title": "2  Introduction to parallel computing",
    "section": "5.18 (naive) Parallel matrix product",
    "text": "5.18 (naive) Parallel matrix product\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\n@parallel\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#divide-and-conquer-procedure",
    "href": "Courses/1_Intro.html#divide-and-conquer-procedure",
    "title": "2  Introduction to parallel computing",
    "section": "5.19 Divide-and-conquer procedure",
    "text": "5.19 Divide-and-conquer procedure\n\nDivide output and input matrices into blocks:\n\n\nC = \\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix},\\,\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix},\\,\nB = \\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix}\n\n\nCompute C = A \\times B by blocks:\n\n\n\\begin{darray}{rcl}\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix}  & =\n& \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix} \\\\\n& = & \\begin{bmatrix}\nA_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\\\\nA_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\\\\n\\end{bmatrix}\n\\end{darray}\n\n\nPossible parallelization over sub-block products A_{ik} \\times B_{kj} and then over result sums \\rightarrow see also “tiled implementation”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#fibonacci-sequence",
    "href": "Courses/1_Intro.html#fibonacci-sequence",
    "title": "2  Introduction to parallel computing",
    "section": "5.20 Fibonacci sequence",
    "text": "5.20 Fibonacci sequence\nInitialization: f_0 = 0, f_1 = 1\nIteration: f_i = f_{i-1} + f_{i-2} for any i \\geq 2\nSequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, …\n\n\nAlgorithm:\nn = 100\nfib = np.zeros(100)\nfib[0] = 0\nfib[1] = 1\nfor i in range(2,n):\n    res[i] = res[i-1] + res[i-2]\n\n\nParallel version? (NO!) (at least not directly)\n\\rightarrow result i depends of result from previous iterations (i-1 and i-2) \n\\rightarrow dependency between iterations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#markov-chain",
    "href": "Courses/1_Intro.html#markov-chain",
    "title": "2  Introduction to parallel computing",
    "section": "5.21 Markov chain",
    "text": "5.21 Markov chain\nMarkov chain: sequence of random variables (X_i)_{i&gt;1} such that \\mathbb{P}(X_i = x_i \\vert X_1 = x_1, X_2 = x_2, \\dots , X_{i-1} = x_{i-1}) = \\mathbb{P}(X_i = x_i \\vert X_{i-1} = x_{i-1})\nX_i\\in S where S is the state space\n\n\nExample:\n\ntwo states E and A, i.e S = \\{A, E\\}\ntransition probability matrix:\n\n\n\\begin{array}{cl}\n\\begin{array}{cc}\nA & E\n\\end{array} \\\\\n\\left(\\begin{array}{cc}\n0.6 & 0.4 \\\\\n0.3 & 0.7 \\\\\n\\end{array}\\right) &\n\\begin{array}{l}\nA \\\\\nE\n\\end{array}\n\\end{array}\n\n\n\n\n\n\nwikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#markov-chain-simulation-algorithm",
    "href": "Courses/1_Intro.html#markov-chain-simulation-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.22 Markov chain simulation algorithm",
    "text": "5.22 Markov chain simulation algorithm\n\nPick an initial state X_0 = x with x\\in S\nFor in in 1,\\dots, N:\n\nSimulate X_i\\in S from the probability distribution given by state X_{i-1}\n\n\nFor the simulation:\n\nIf X_{i-1} = A then \\mathbb{P}(X_i = A) = 0.6 and \\mathbb{P}(X_i = E) = 0.4\nIf X_{i-1} = E then \\mathbb{P}(X_i = A) = 0.7 and \\mathbb{P}(X_i = E) = 0.3\n\nExercise: parallel algorithm?\n\nNO!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#take-home-message",
    "href": "Courses/1_Intro.html#take-home-message",
    "title": "2  Introduction to parallel computing",
    "section": "6.1 Take home message",
    "text": "6.1 Take home message\n\nParallel computing can be used to run computations faster (i.e. save time)\nRelationship between time gain and number of tasks run in parallel is not linear\nPotential bottlenecks leading to potential performance loss:\n\nmanagement of parallel tasks\noverhead for computing resource access\noverhead for memory access\nconcurrent memory access by parallel tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#footnotes",
    "href": "Courses/1_Intro.html#footnotes",
    "title": "2  Introduction to parallel computing",
    "section": "",
    "text": "i.e. a set of tools/machines used by a worker to complete a task↩︎\noverhead on resource access↩︎\nor “processors”↩︎\nthe size of the block depends on the size of the cache↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html",
    "href": "Courses/2_Concepts.html",
    "title": "3  Advanced concepts in parallel programming",
    "section": "",
    "text": "4 Why parallel computing",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#trend-over-50years",
    "href": "Courses/2_Concepts.html#trend-over-50years",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.1 Trend over ~50years",
    "text": "4.1 Trend over ~50years\n\n\nMoore’s Law (doubling the transistor counts every two years) is live\nSingle thread performance hit a wall in 2000s\nAlong with typical power usage and frequency\nNumber of logical cores is doubling every ~3 years since mid-2000\n\n\n\n\n\nOriginal data up to the year 2010 collected and plotted by M. Horowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten New plot and data collected for 2010-2021 by K. Rupp\n\n\n\nGithub repo for data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#computing-units",
    "href": "Courses/2_Concepts.html#computing-units",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.2 Computing units",
    "text": "4.2 Computing units\n\n\nCPU :\n\n4/8/16+ execution cores (depending on context, laptop, desktop, server)\nHyperthreading (Intel) or SMT (AMD), x2\nVector units (multiple instructions processed on a vector of data)\n\nGPU computing : 100/1000 “simple” cores per card",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#the-reality",
    "href": "Courses/2_Concepts.html#the-reality",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.3 The reality",
    "text": "4.3 The reality\n\n\n\nA serial application only accesses 0.8% of the processing power of a 16-core CPU.\n\n\n0.08\\% = \\frac{1}{16 * 2 (cores + hyperthreading) * \\frac{256 (bitwide vector unit}{64(bit double)} = 128}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#faster-for-less-development",
    "href": "Courses/2_Concepts.html#faster-for-less-development",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.1 Faster for less development",
    "text": "5.1 Faster for less development\n\\frac{S_{up}}{T_{par}} \\gg \\frac{S_{up}}{T_{seq}}\nRatio of speedup improvment S_{up} over time of development (T_{seq|par}) comparison.\nFrom a development time perspective, return on investment (speedup) is often several magnitudes of order better than pure “serial/sequential” improvment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#scaling",
    "href": "Courses/2_Concepts.html#scaling",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.2 Scaling",
    "text": "5.2 Scaling\nSimple “divide and conquer” strategies in parallel programming allow to handle data with previously almost untractable sizes and scale before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency",
    "href": "Courses/2_Concepts.html#energy-efficiency",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.3 Energy efficiency",
    "text": "5.3 Energy efficiency\n\n\n\n\n\n\nNote\n\n\n\nThis is a huge one, in the present context 😬\n\n\nDifficult to estimate but the Thermal Design Power (TDP), given by hardware manufacturers, is a good rule of thumb. Just factor the number of units, and usual proportionality rules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency-a-bunch-of-cpus",
    "href": "Courses/2_Concepts.html#energy-efficiency-a-bunch-of-cpus",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.4 Energy efficiency, a bunch of CPUs",
    "text": "5.4 Energy efficiency, a bunch of CPUs\nExample of “standard” use : 20 16-core Intel Xeon E5-4660 which is 120~W of TDP\nP = (20~Processors) * (120~W/~Processors) * (24~hours) = 57.60~kWhrs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency-just-a-few-big-gpus",
    "href": "Courses/2_Concepts.html#energy-efficiency-just-a-few-big-gpus",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.5 Energy efficiency, just a few (big) GPUs",
    "text": "5.5 Energy efficiency, just a few (big) GPUs\nA Tesla V100 GPU is of 300~W of TDP. Let’s use 4 of them.\nP = (4~GPUs) * (300~W/~GPUs) * (24~hours) = 28.80~kWhrs\n\\Longrightarrow half of the power use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#terms-and-definitions",
    "href": "Courses/2_Concepts.html#terms-and-definitions",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.1 Terms and definitions",
    "text": "6.1 Terms and definitions\n\nSpeedup S_{up}(N): ratio of the time of execution in serial and parallel mode\nNumber of computing units N\nP (resp. S) is the parallel (resp. serial) fraction of the time spent in the parallel (resp. serial) part of the program (P+S=1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law",
    "href": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.2 Asymptote of parallel computing : Amdahl’s Law",
    "text": "6.2 Asymptote of parallel computing : Amdahl’s Law\nThere P is the fraction of the time spent in the parallel part of the program in a sequential execution.\nS_{up}(N) \\le \\frac{1}{S+\\frac{P}{N}}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "href": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.3 Asymptote of parallel computing : Amdahl’s Law, Graphic",
    "text": "6.3 Asymptote of parallel computing : Amdahl’s Law, Graphic\n\n\n\nIdeal speedup : 100% of the code parallelized; 90%, 75%, and 50% : limited by the fractions of code that remain serial. (Robey and Zamora 2021)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#more-with-almost-less-the-pump-it-up-approach",
    "href": "Courses/2_Concepts.html#more-with-almost-less-the-pump-it-up-approach",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.4 More with (almost) less : the pump it up approach",
    "text": "6.4 More with (almost) less : the pump it up approach\nGustafson’s law\nThere now, P is the fraction of the time spent in the parallel part of the program in a parallel execution.\n\n\n\n\nWhen the size of the problem grows up proportionnaly to the number of computing units.\nS_{up}(N) \\le N - S*(N-1)\nwhere N is the number of computing units and S the serial fraction as before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#more-with-almost-less-graphic",
    "href": "Courses/2_Concepts.html#more-with-almost-less-graphic",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.5 More with (almost) less : graphic",
    "text": "6.5 More with (almost) less : graphic\n\n\n\nLinear growth with the number of processor (and data size too)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#strong-vs-weak-scaling-definitions",
    "href": "Courses/2_Concepts.html#strong-vs-weak-scaling-definitions",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.6 Strong vs Weak Scaling, definitions",
    "text": "6.6 Strong vs Weak Scaling, definitions\n\n\nStrong Scaling\n\nStrong scaling represents the time to solution with respect to the number of processors for a fixed total size.\n\n\n\\Rightarrow Amdahl’s law\n\nWeak Scaling\n\nWeak scaling represents the time to solution with respect to the number of processors for a fixed-sized problem per processor.\n\n\n\\Rightarrow Gustafson’s law",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#strong-vs-weak-scaling-schemas",
    "href": "Courses/2_Concepts.html#strong-vs-weak-scaling-schemas",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.7 Strong vs Weak Scaling, schemas",
    "text": "6.7 Strong vs Weak Scaling, schemas\n\n\n┌────────────────────────────────────┐\n│                 1000               │\n│         ┌───────────────────┐      │\n│         │                   │      │           1 processor\n│         │                   │      │\n│         │                   │      │\n│ 1000    │                   │      │           \n│         │                   │      │\n│         │                   │      │\n│         └───────────────────┘      │\n│        ┌─────────┐  ┌─────────┐    │\n│        │         │  │         │    │\n│ 500    │         │  │         │    │\n│        │         │  │         │    │\n│        └─────────┘  └─────────┘    │\n│           500                      │           4 processors\n│        ┌─────────┐  ┌─────────┐    │\n│        │         │  │         │    │\n│        │         │  │         │    │\n│        │         │  │         │    │\n│        └─────────┘  └─────────┘    │\n│      250                           │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│ 250 │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │           16 processors\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n└────────────────────────────────────┘\n\n┌───────────────────────────────────────────────────────────┐\n│                         1000                              │\n│                      ┌─────────┐                          │\n│                      │         │                          │\n│              1000    │      ───┼──┐                       │\n│                      │         │  │                       │\n│                      └─────────┘  │                       │\n│                   1000            │                       │\n│                 ┌─────────┐  ┌────┼────┐                  │\n│                 │         │  │    │    │                  │\n│           1000  │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 └─────────┘  └────┼────┘                  │\n│                 ┌─────────┐  ┌────┼────┐                  │\n│                 │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 └─────────┘  └────┼────┘                  │\n│                                   │         1000          │\n│    ┌─────────┐  ┌─────────┐  ┌────┼────┐  ┌─────────┐     │\n│    │         │  │         │  │    │    │  │         │     │\n│    │         │  │         │  │    ▼    │  │         │1000 │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n└───────────────────────────────────────────────────────────┘",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#flynns-taxonomy",
    "href": "Courses/2_Concepts.html#flynns-taxonomy",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.1 Flynn’s taxonomy",
    "text": "7.1 Flynn’s taxonomy\n\n\n\n\nSimple Instruction\nMultiple Instructions\n\n\n\n\nSimple Data\n\n\n\n\nMultiple Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#a-different-approach",
    "href": "Courses/2_Concepts.html#a-different-approach",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.2 A different approach",
    "text": "7.2 A different approach\n\n\n\nParallelism level\nHardware\nSoftware\nParallelism extraction\n\n\n\n\nInstruction\nSIMD (or VLIW)\nIntrinsics\nCompiler\n\n\nThread\nMulti-core RTOS\nLibrary or language extension\nPartitioning/Scheduling (dependency control)\n\n\nTask\nMulti-core (w/o RTOS)\nProcesses (OS level)\nPartitioning/Scheduling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#multi-processing-vs-multi-threading",
    "href": "Courses/2_Concepts.html#multi-processing-vs-multi-threading",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.3 Multi-processing vs Multi-threading",
    "text": "7.3 Multi-processing vs Multi-threading\n\n\n                   Main Process\n                 ┌─────────────┐\n                 │             │\n                 │   CPU       │\n                 ├─────────────┤\n                 │             │\n                 │   Memory    │\n                 └─┬────┬────┬─┘\n                   │    │    │\n                   │    │    │\n       ┌───────────┘    │    └───────────┐\n       │                │                │\n┌──────▼──────┐  ┌──────▼──────┐  ┌──────▼──────┐\n│             │  │             │  │             │\n│   CPU       │  │   CPU       │  │   CPU       │\n├─────────────┤  ├─────────────┤  ├─────────────┤\n│             │  │             │  │             │\n│   Memory    │  │   Memory    │  │   Memory    │\n└─────────────┘  └─────────────┘  └─────────────┘\n\n   Process 1        Process 1        Process 1\n\n┌──────────────────────────────────────┐\n│            MAIN PROCESS              │\n│                                      │\n│                                      │\n│     ┌──────────┐                     │\n│     │          │                     │\n│     │   CPU    │   ┌───────────┐     │\n│     │          │   │           │     │\n│     └──────────┘   │   Memory  │     │\n│                    │           │     │\n│                    └───────────┘     │\n│                                      │\n│                                      │\n│                                      │\n│      ┌─┐         ┌─┐         ┌─┐     │\n│      │┼│         │┼│         │┼│     │\n│      │┴│         │┴│         │┴│     │\n│      ▼▼▼         ▼▼▼         ▼▼▼     │\n│   Thread 1    Thread 2    Thread 3   │\n│      ┌─┐         ┌─┐         ┌─┐     │\n│      │┼│         │┼│         │┼│     │\n│      │┴│         │┴│         │┴│     │\n│      ▼▼▼         ▼▼▼         ▼▼▼     │\n│                                      │\n└──────────────────────────────────────┘\n\n\n\n\n\n\nMulti-processing\nMulti-threading\n\n\n\n\nMemory\nExclusive\nShared\n\n\nCommunication\nInter-process\nAt caller site\n\n\nCreation overhead\nHeavy\nMinimal\n\n\nConcurrency\nAt OS level\nLibrary/language",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Robey, R., and Y. Zamora. 2021. Parallel and High Performance\nComputing. Manning. https://www.manning.com/books/parallel-and-high-performance-computing.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Appendix A — Applications",
    "section": "",
    "text": "Original (In Percent Format)\nOnline Html (Corrected)\nNotebook (Corrected)\n\n\n\n\nNumpy Workout\nSolution\nNotebook\n\n\nMultiProcessing, Strong Scaling\n\n\n\n\nMultiprocessing in Python 3\nSolution\nNotebook\n\n\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Appendix B — Slides in reveal.js",
    "section": "",
    "text": "Numpy Workout\nIntroduction to parallel computing\nAdvanced concepts in parallel programming\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Slides in reveal.js</span>"
    ]
  }
]