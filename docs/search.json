[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced programming and parallel computing",
    "section": "",
    "text": "Preface\nThis is the course and materials for the lecture on “Advanced programming and parallel computing” at the Paul Valery University of Montpellier, France.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Prerequisites",
    "section": "",
    "text": "1.1 Infrastructure\nIn this course, we will be using the following infrastructure:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#infrastructure",
    "href": "intro.html#infrastructure",
    "title": "1  Prerequisites",
    "section": "",
    "text": "a Jupyterhub instance, hosted direcly at IMAG the lab where I work. Don’t use it directly, I’ll provide a speficic link for each of you to connect. This serve as a “hub” to launch your own JupyterLab instance.\na JupyterLab instance, hosted on a cluster MESO@LR. You will have exatly one instance each. This instance will be, depending on the context of the course, configured to run on essentially two different ways:\n\neither it will be configured to run on a single node, with a single CPU. This is the case for the first part of the course, where we will focus on the basics of parallel programming.\nor it will be configured to run on multiple nodes, . This is the case for the second part of the course, where we will focus on distributed programming.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#software-required",
    "href": "intro.html#software-required",
    "title": "1  Prerequisites",
    "section": "1.2 Software required",
    "text": "1.2 Software required\nYou will need to install the following software on your computer: Visual Studio Code (VSCode), a free and open-source code editor. You’ll have to install the following extensions:\n\n Python extension to have everything you need to work with Python.\n Jupyter to have everything you need to work with Jupyter notebooks.\n JupyterHub to connect to the JupyterHub instance.\n Live Share to enable collaborative editing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-use-the-jupyterlab-instance-in-vscode",
    "href": "intro.html#how-to-use-the-jupyterlab-instance-in-vscode",
    "title": "1  Prerequisites",
    "section": "1.3 How to use the JupyterLab instance in VSCode",
    "text": "1.3 How to use the JupyterLab instance in VSCode\n\n1.3.1 Prerequisite: get your own JupyterLab instance url\nIn the table I provided you, you should have a line with:\n\nyour name\nyour username (which begin with e_miashs-XX where XX is the number of your account)1\na onesecret link type of link : \nAs the decryption key, simply provide your username: \nAnd then you get the token for your jupyterhub instance: , save it somewhere.\n\n\n\n\n\n\n\nWarning\n\n\n\nThis works only once. If you close the tab or didn’t properly save the link you’ll have to ask me for a new onesecret link.\n\n\n\n\n1.3.2 Connect to the JupyterLab instance with VSCode\n\nFirst, bring down the command palette with Ctrl+Shift+P (or Cmd+Shift+P on macOS). Then, choose “Jupyter: Launch Interactive Window”. \nIt will open a new tab in vscode, click on “Select kernel” like this: \nChoose “Existing Jupyter Server” : \nThen it will ask you for the URL of the JupyterLab instance, which is https://jupyterhub.imag.umontpellier.fr : \nEnter the login name, which is e_miashs-XX : \nPaste the token you got from the onesecret link, and then press enter: \nThen it will ask you for a display name for the instance, you can put whatever you want, and press enter: \nAnd finally, you have to choose for the kernel, choose “Python 3” and voilà \n\n\nAlternative way to connect to the JupyterLab instance\n\n\n\n\n\n\nDon’t use JupyterHub\n\n\n\nAs today (09/23/2024), the jupyterhub extension in VSCode could be not working properly, as indicated in this issue.\nIf this is the case for you, you could use the old way to connect to the JupyterLab instance, by using the token you got from the onesecret link, as explained in the following steps.\n\n\n\nFirst, bring down the command palette with Ctrl+Shift+P (or Cmd+Shift+P on macOS). Then, choose “Jupyter: Launch Interactive Window”. \nIt will open a new tab in vscode, click on “Select kernel” like this: \nChoose “Existing Jupyter Server” : \nThen it will ask you for the URL of the Jupyter instance, which is https://jupyterhub.imag.umontpellier.fr/user/e_miashs-XX/?token=XXXXXXXXXXXXX :  where e_miashs-XX is your username and XXXXXXXXXXXXX is the token you got from the onesecret link.\nThen it will ask you for a display name for the instance, you can put whatever you want, and press enter: \nAnd finally, you have to choose for the kernel, choose “Python 3” and voilà",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#connected",
    "href": "intro.html#connected",
    "title": "1  Prerequisites",
    "section": "1.4 Connected !",
    "text": "1.4 Connected !\n\n\n\n\n\n\n\nFirst cell is slow\n\n\n\nIt could take a few seconds when you enter your first cell of code for the kernel to start.\n\n\n\n\n\n\n\n\n\nAll is remote!\n\n\n\nAs you can see, from now on, every python interactive execution you do in VSCode will be done on your remote JupyterLab instance (the shell hostname returned the name of the cluster node on which the instance is running). Therefore, the code you execute can’t use any local file on your computer, even if your code is in a local file. Generally everything you’ll need as resources and files will be provided by me.\n\n\n\n\n\n\n\n\nIn case you really need to use a local file\n\n\n\nIn the case you need to use a local file, you’ll have to upload it to your JupyterLab instance, by using direct shell commands in the interactive window for example.\n\n\n\n\n\n\n\n\nBrowser vs VSCode\n\n\n\nOf course, you can use the JupyterLab instance directly in your browser, but I strongly recommend you to use VSCode, because:\n\nIt will be much more convenient to work with.\nWe will be able to use collaborative editing in practice sessions.\n\n\n\n\n\n\n\n\n\nLosing connection\n\n\n\nWhen you’ll lose your connection to the JupyterLab instance in VSCode (for example if you close the tab, exit from VSCode, or simply your laptop goes to sleep), you also lose the current state of your session in the stance (you have to reexecute all the cells you executed to restore the state of the session).\nIt also could ask for your username again, just press enter.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#collaborative-editing",
    "href": "intro.html#collaborative-editing",
    "title": "1  Prerequisites",
    "section": "1.5 Collaborative editing",
    "text": "1.5 Collaborative editing\n\nIn the discord channel, I’ll provide you a link to join a collaborative editing session. Don’t click on it, just copy it: \nThen open a new “blank” window in VSCode, which will be exclusively for collaborative session. \nThen, click on the “Live Share” button in the bottom left corner of the window \nClick on the “Join” button \nEither choose anonymous or sign in with your github/microsoft account \n\n\n\n\n\n\n\nAnonymous Guest Name\n\n\n\nIf you choose to sign in, you’ll have to authorize VSCode to access your github/microsoft account. If you choose anonymous, you’ll have to choose a username. Please choose a username that is easily identifiable as yours.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Prerequisites",
    "section": "",
    "text": "In the case of any problem, contact me ASAP, and specify your username.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Prerequisites</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html",
    "href": "Courses/1_Intro.html",
    "title": "2  Introduction to parallel computing",
    "section": "",
    "text": "3 Parallel computing: the intuition",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing",
    "href": "Courses/1_Intro.html#computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.1 Computing ?",
    "text": "3.1 Computing ?\n\na computation = a succession of tasks to complete\na task \\approx a single command/action or a group of commands/actions\n\n\n\n\n\n\n\nExample 1:\n\n\nExample 2:\n\n\n\n\n# task i:\n# sum of elements at index i\n# from two vectors\nfor i in range(10):\n    res[i] = a[i] + b[i]\n\n\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#why-parallel-computing",
    "href": "Courses/1_Intro.html#why-parallel-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.2 Why parallel computing?",
    "text": "3.2 Why parallel computing?\n\nObjective: accelerate computations &lt;=&gt; reduce computation time\nIdea: run multiple tasks in parallel instead of sequentially",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-1",
    "href": "Courses/1_Intro.html#context-level-1",
    "title": "2  Introduction to parallel computing",
    "section": "3.3 Context (level 1)",
    "text": "3.3 Context (level 1)\n\ndifferent tasks to complete\none or more workers to complete the tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#sequential-computing",
    "href": "Courses/1_Intro.html#sequential-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.4 Sequential computing",
    "text": "3.4 Sequential computing\n\n\n\n\n\n\n\nn tasks to complete (n&gt;1)\n1 worker\n\nTotal time (exercise)\n\n\\sum_{i=1}^n t_i \\sim O(n)\\ with t_i time to complete task i}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-computing-the-most-simple-case",
    "href": "Courses/1_Intro.html#parallel-computing-the-most-simple-case",
    "title": "2  Introduction to parallel computing",
    "section": "3.5 Parallel computing (the most simple case)",
    "text": "3.5 Parallel computing (the most simple case)\n\n\n\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&gt;=n)\n\n\n\n\n\n\n\nTotal time (exercise)\n\n\\underset{i=1,\\dots,n}{\\text{max}}\\{t_i\\}\\sim O(1)\\ with t_i time to complete task i\n\n\n\nPotential bottleneck? (exercise)\n\nnot enough workers to complete all tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#task-scheduling",
    "href": "Courses/1_Intro.html#task-scheduling",
    "title": "2  Introduction to parallel computing",
    "section": "3.6 Task scheduling",
    "text": "3.6 Task scheduling\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\n\nNeed: assign multiple tasks to each worker (and manage this assignment)\n\n      ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐\n      │worker 1│  │worker 2│  │worker 3│  │worker 4│  ...\n      └────────┘  └────────┘  └────────┘  └────────┘\n  ┌─\n  │     task 1      task 2      task 3      task 4    ...\n  │       │           │           │           │\n  │       ▼           ▼           ▼           ▼\n  │    task p+1    task p+2    task p+3    task p+4\n  │       │           │           │           │\n  │       ▼           ▼           ▼           ▼\n  │       .           .           .           .\n  │       .           .           .           .\n  ▼       .           .           .           .\n\nTime\n\n\nTotal time (exercise)\n\n\\underset{k=1,\\dots,p}{\\text{max}}\\{T_k\\}\\sim O(n/p)\\ with T_k = \\sum_{i\\in I_k} t_i, total time to complete all tasks assigned to worker k (where I_k is the set of indexes of tasks assigned to worker k)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-parallel-computing-simple-case",
    "href": "Courses/1_Intro.html#illustration-parallel-computing-simple-case",
    "title": "2  Introduction to parallel computing",
    "section": "3.7 Illustration: parallel computing (simple case)",
    "text": "3.7 Illustration: parallel computing (simple case)\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\nNumber of workers: 1, 2, 4, 6, 8\n\nWhy is the time gain not linear?\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-2",
    "href": "Courses/1_Intro.html#context-level-2",
    "title": "2  Introduction to parallel computing",
    "section": "3.8 Context (level 2)",
    "text": "3.8 Context (level 2)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources1\n\nPotential bottleneck? (exercise)\n\nnot enough resources for all workers",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#resource-management",
    "href": "Courses/1_Intro.html#resource-management",
    "title": "2  Introduction to parallel computing",
    "section": "3.9 Resource management",
    "text": "3.9 Resource management\n\n\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\n\nNeed:\n\nassign workers to each resource (and manage this assignment)\n\nTotal time = ? (exercise)\nPotential issues? (exercise)\n\n       ┌──────────┐      ┌──────────┐\n       │resource 1│      │resource 2│      ...\n       └──────────┘      └──────────┘\n  ┌─\n  │       task 1            task 2         ...\n  │     (worker 1)        (worker 2)\n  │         │                 │\n  │         ▼                 ▼\n  │       task 3            task 4\n  │     (worker 3)        (worker 4)\n  │         │                 │\n  │         ▼                 ▼\n  │      task p+1          task p+2\n  │     (worker 1)        (worker 2)\n  │         │                 │\n  │         ▼                 ▼\n  │      task p+3          task p+4\n  │     (worker 3)        (worker 4)\n  │         │                 │\n  │         ▼                 ▼\n  │         .                 .\n  │         .                 .\n  ▼         .                 .\n\nTime",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#resource-management-1",
    "href": "Courses/1_Intro.html#resource-management-1",
    "title": "2  Introduction to parallel computing",
    "section": "3.10 Resource management",
    "text": "3.10 Resource management\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\\sim O(n/q)\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_i = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks assigned done on resource \\ell)\nPotential issues? multiple workers want to use the same working resources\n\nthey have to wait for their turn (workers are not working all the time)\nrisk to jam2 resource access (organizing resource access takes time)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-overhead-for-resource-access",
    "href": "Courses/1_Intro.html#illustration-overhead-for-resource-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.11 Illustration: overhead for resource access",
    "text": "3.11 Illustration: overhead for resource access\n\n\n\na task = “wait 1 \\mus”\nObjective: run 100 tasks\n8 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#context-level-3-realistic",
    "href": "Courses/1_Intro.html#context-level-3-realistic",
    "title": "2  Introduction to parallel computing",
    "section": "3.12 Context (level 3: realistic)",
    "text": "3.12 Context (level 3: realistic)\n\ndifferent tasks to complete\nmultiple workers to complete the tasks\none or more working resources\n\nInput/Output (I/O)\n\nInput: each task requires some materials (data) to be completed, these materials are stored in a storage area (memory)\nOutput: each task returns a result that need to be put in the storage area (memory)\n\nExamples: vector/matrix/array operations, process the content of multiple files",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#inputoutput-management",
    "href": "Courses/1_Intro.html#inputoutput-management",
    "title": "2  Introduction to parallel computing",
    "section": "3.13 Input/Output management",
    "text": "3.13 Input/Output management\n\nn tasks to complete (n&gt;1)\np workers (p&lt;n)\nq working resources (q&lt;p)\ntasks need input (data) and produce output (results)\n\nNeed:\n\nload input (data) from storage when needed by a worker to complete a task\nwrite output (result) to storage when a task is completed\n\nTotal time = ? (exercise)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-computing-realistic-model",
    "href": "Courses/1_Intro.html#parallel-computing-realistic-model",
    "title": "2  Introduction to parallel computing",
    "section": "3.14 Parallel computing: realistic model",
    "text": "3.14 Parallel computing: realistic model\n┌──────────┐\n│resource 1│     load        task 1         write       load        task 3         write\n└──────────┘    data 1 ──► (worker 1) ──► result 1 ──► data 3 ──► (worker 3) ──► result 3 ──► . . .\n\n\n┌──────────┐\n│resource 2│     load        task 2         write       load        task 4         write\n└──────────┘    data 2 ──► (worker 2) ──► result 2 ──► data 4 ──► (worker 4) ──► result 4 ──► . . .\n\n     .\n     .\n     .\n\n             └─────────────────────────────────────────────────────────────────────────────────────►\n                                                                                                 Time",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing-time-and-potential-bottleneck",
    "href": "Courses/1_Intro.html#computing-time-and-potential-bottleneck",
    "title": "2  Introduction to parallel computing",
    "section": "3.15 Computing time and potential bottleneck",
    "text": "3.15 Computing time and potential bottleneck\nTotal time = \\text{max}_{\\ell=1,\\dots,q}\\{\\tau_\\ell\\}\nwith \\tau_\\ell = \\sum_{i\\in J_\\ell} t_{i,\\text{in}} + t_i + t_{i,\\text{out}} = total time to complete all tasks done on resource \\ell (where J_\\ell is the set of indexes of tasks done on resource \\ell)\nPotential bottlenecks:\n\ninput (data) are not ready/available when a worker need them to complete a task (the worker have to wait)\noutput (results) cannot be written when a worker complete a task (the worker have to wait)\n\nOverhead on memory access\n\nconcurrent access to a memory space when reading input and/or when writing output\nconcurrent data transfer from or to memory (the “pipe” are jammed)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-1-overhead-for-io-access",
    "href": "Courses/1_Intro.html#illustration-1-overhead-for-io-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.16 Illustration 1: overhead for I/O access",
    "text": "3.16 Illustration 1: overhead for I/O access\n\n\n\na task\n\nsimulate a vector of 10 values\ncompute the mean\n\nObjective: run 10000 tasks\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n10 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-2-overhead-for-io-access",
    "href": "Courses/1_Intro.html#illustration-2-overhead-for-io-access",
    "title": "2  Introduction to parallel computing",
    "section": "3.17 Illustration 2: overhead for I/O access",
    "text": "3.17 Illustration 2: overhead for I/O access\n\n\n\na task = “compute the sum of a given row in a matrix”\nObjective: compute all row-wise sums for a 10000 \\times 1000 matrix (i.e. 10000 tasks)\nResources: 8 computing units\nNumber of workers: 1, 2, 4, 6, 8\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#the-vocabulary-of-parallel-computing",
    "href": "Courses/1_Intro.html#the-vocabulary-of-parallel-computing",
    "title": "2  Introduction to parallel computing",
    "section": "3.18 The vocabulary of parallel computing",
    "text": "3.18 The vocabulary of parallel computing\n\ntasks = a command or a group of commands\nworker = a program or a sub-program (like a thread or a sub-process) → Software\nworking resources = processing units → Hardware\ninput = data\noutput = result\nstorage = memory\n\nAttention: “worker” may sometimes refer to a working resource in the literature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#task-synchronization",
    "href": "Courses/1_Intro.html#task-synchronization",
    "title": "2  Introduction to parallel computing",
    "section": "3.19 Task synchronization",
    "text": "3.19 Task synchronization\n\nSometimes tasks cannot be done in parallel\n\nSpecific case: output of task i_1 is input of task i_2\nNeed: wait for task i_1 before task i_2 starts\n\n\n\n\nExample 1:\n# task 1: matrix product\nC = A @ B\n# task 2: colwise sum over matrix C\nnp.sum(C,axis=0)\n\nExample 2:\n\ntask 1: train a predictive model\ntask 2: use the trained model to predict new labels",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#computing-resources",
    "href": "Courses/1_Intro.html#computing-resources",
    "title": "2  Introduction to parallel computing",
    "section": "4.1 Computing resources",
    "text": "4.1 Computing resources\n\na single computer\n\na persistent memory (hard drive) with very slow access\na non-persistent shared memory (RAM) with faster access\none or more computing units called CPUs3 (central processing units) linked to the RAM\nmaybe one or more GPUs (graphical processing units) linked to the RAM\n\nmultiple computers linked through a network (very slow communication)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#cpu-central-processing-unit",
    "href": "Courses/1_Intro.html#cpu-central-processing-unit",
    "title": "2  Introduction to parallel computing",
    "section": "4.2 CPU (central processing unit)",
    "text": "4.2 CPU (central processing unit)\n\n\n\nmulti-core CPU: multiple computing units (called “cores”) in a single processor\ndifferent level of local memory called “cache”\nto run a computation: transfer data from shared memory to local cache (and vice-versa for results) \\rightarrow potential bottleneck\n\n\n           ┌─────────────────┬───────────────────────┐\n           │                 │                       │\n┌──────────┴──┐     ┌─────── │ ────────┐    ┌─────── │ ────────┐\n│ MEMORY      │     │ CPU1   │         │    │ CPU2   │         │\n│             │     │ ┌──────┴───────┐ │    │ ┌──────┴───────┐ │\n│             │     │ │ Local Memory │ │    │ │ Local Memory │ │\n│             │     │ └──────┬───────┘ │    │ └──────┬───────┘ │\n│             │     │        │         │    │        │         │\n│             │     │ ┌───┐  │  ┌───┐  │    │ ┌───┐  │  ┌───┐  │\n│             │     │ │ C ├──┼──┤ C │  │    │ │ C ├──┼──┤ C │  │\n│             │     │ └───┘  │  └───┘  │    │ └───┘  │  └───┘  │\n└─────────────┘     │        │         │    │        │         │\n                    │ ┌───┐  │  ┌───┐  │    │ ┌───┐  │  ┌───┐  │\n                    │ │ C ├──┴──┤ C │  │    │ │ C ├──┴──┤ C │  │\n                    │ └───┘     └───┘  │    │ └───┘     └───┘  │\n                    │                  │    │                  │\n                    └──────────────────┘    └──────────────────┘",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#gpu-graphical-processing-units",
    "href": "Courses/1_Intro.html#gpu-graphical-processing-units",
    "title": "2  Introduction to parallel computing",
    "section": "4.3 GPU (graphical processing units)",
    "text": "4.3 GPU (graphical processing units)\n\n\n\n“many-core” computing card\nlocal memory\nslower connection to shared memory than CPUs\nto run a computation: transfer data from host shared memory to local memory (and vice-versa for results)\n\n\\rightarrow potential bottleneck\n\n         ┌───────────────────────────────────────────┐\n         │                                           │\n         │ ┌─────────────────┐                       │\n         │ │                 │                       │\n┌────────┴─┴──┐     ┌─────── │ ────────┐    ┌─────── │ ─────────┐\n│ MEMORY      │     │ CPU1   │         │    │ GPU    │          │\n│             │     │ ┌──────┴───────┐ │    │ ┌──────┴───────┐  │\n│             │     │ │ Local Memory │ │    │ │ Local Memory │  │\n│             │     │ └──────┬───────┘ │    │ └──────┬───────┘  │\n│             │     │        │         │    │        │          │\n│             │     │ ┌───┐  │  ┌───┐  │    │  ┌─┬─┬─┼─┬─┬─┬─┐  │\n│             │     │ │ C ├──┼──┤ C │  │    │  │C│C│C│C│C│C│C│  │\n│             │     │ └───┘  │  └───┘  │    │  ├─┼─┼─┼─┼─┼─┼─┤  │\n└─────────────┘     │        │         │    │  │C│C│C│C│C│C│C│  │\n                    │ ┌───┐  │  ┌───┐  │    │  ├─┼─┼─┼─┼─┼─┼─┤  │\n                    │ │ C ├──┴──┤ C │  │    │  │C│C│C│C│C│C│C│  │\n                    │ └───┘     └───┘  │    │  └─┴─┴─┴─┴─┴─┴─┘  │\n                    │                  │    │                   │\n                    └──────────────────┘    └───────────────────┘\n\n\n\n\nSource: wikimedia.org\n\n\n\n\n\n\nCPU\nGPU\n\n\n\n\ntens (10x) of computing units (“cores”)\nthousand (1000x) of computing units (“cores”)\n\n\ncomputing units capable of more complex operations\ncomputing units only capable of more simple operations\n\n\nlarger cache memory per computing unit\nvery small cache memory per computing unit\n\n\nfaster access to RAM\nslower access to RAM\n\n\n\\rightarrow efficient for general purpose parallel programming (e.g. check conditions)\n\\rightarrow fast for massively parallel computations based on simple elementary operations (e.g. linear algebra)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum",
    "href": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.1 Row/Column-wise matrix sum",
    "text": "5.1 Row/Column-wise matrix sum\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N \\times P\n\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\n\n\nRow-wise sum: vector C = [c_{i}]_{i=1:N} of size N where c_{i} = \\sum_{j=1}^P a_{ij}\nColumn-wise sum: vector D = [d_{j}]_{j=1:P} of size P where d_{j} = \\sum_{i=1}^N a_{ij}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#row-wise-sum",
    "href": "Courses/1_Intro.html#row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.2 Row-wise sum",
    "text": "5.2 Row-wise sum\n\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\ \\ \\rightarrow \\ \\ \\begin{bmatrix}\n    \\vdots \\\\\n    \\vdots \\\\\n    \\sum_{j=1}^{P} a_{ij} \\\\\n    \\vdots\\\\\n    \\vdots\\\\\n  \\end{bmatrix}_{N \\times 1}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#column-wise-sum",
    "href": "Courses/1_Intro.html#column-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.3 Column-wise sum",
    "text": "5.3 Column-wise sum\n\n\\begin{array}{c}\n\\begin{bmatrix}\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\  & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & a_{ij} & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\\\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n    \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ & \\ \\ \\ \\cdot \\ \\ \\ \\\\\n  \\end{bmatrix}_{N \\times P}\\\\\n  \\downarrow \\ \\ \\ \\ \\ \\ \\\\\n\\begin{bmatrix}\n   \\ \\dots \\  & \\dots & \\sum_{i=1}^{N} a_{ij} & \\dots & \\ \\dots \\ \\\\\n  \\end{bmatrix}_{1\\times P}\\\\\n\\end{array}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#rowcolumn-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.4 Row/Column-wise matrix sum algorithm",
    "text": "5.4 Row/Column-wise matrix sum algorithm\n\n\nRow-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nColumn-wise sum:\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\n\nExercise: parallel algorithm?\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[i] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\n\nExercise: any concurrent access to memory by the parallel tasks ? in input (reading) ? in output (writing) ?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#concurrent-access-to-memory",
    "href": "Courses/1_Intro.html#concurrent-access-to-memory",
    "title": "2  Introduction to parallel computing",
    "section": "5.5 Concurrent access to memory",
    "text": "5.5 Concurrent access to memory\nSolution 1:\n\nreading (input): no concurrent access\nwriting (output): no concurrent access\n\nSolution 2:\n\nreading (input): no concurrent access\nwriting (output): what happen if tasks j_1 and j_2 need to simultaneously update vecD[i]?\n\n\n\\rightarrow need for synchronization (with a time cost)\n\n\n\nSolution 3?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    vecD[i] += matA[i,j]\n\nSolution 4?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(N)\n# algorithm\nfor i in range(N):\n  @parallel\n  for j in range(P):\n    vecD[i] += matA[i,j]\n(Concurrent access between tasks to update vecD[i])\n\n\nAny other issue ?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#cost-of-parallel-task-management",
    "href": "Courses/1_Intro.html#cost-of-parallel-task-management",
    "title": "2  Introduction to parallel computing",
    "section": "5.6 Cost of parallel task management",
    "text": "5.6 Cost of parallel task management\n\n\n@parallel\nfor i in range(N):\n  for j in range(P):\n    ...\n1 launch of N parallel tasks running each P operations\n\\rightarrow N “long” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(N)\n\nfor j in range(P):\n  @parallel\n  for i in range(N):\n    ...\nP launches of N parallel tasks running each 1 operation\n\\rightarrow N \\times P “short” parallel tasks\nCost (in time) to launch parallel tasks \\sim O(NP)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#parallel-column-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#parallel-column-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.7 Parallel column-wise matrix sum algorithm",
    "text": "5.7 Parallel column-wise matrix sum algorithm\n\n\nSolution 1?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor j in range(P):\n  for i in range(N):\n    vecD[j] += matA[i,j]\n\nSolution 2?\n# input\nmatA = np.array(...).reshape(N,P)\n# output\nvecD = np.zeros(P)\n# algorithm\n@parallel\nfor i in range(N):\n  for j in range(P):\n    vecD[j] += matA[i,j]\nConcurrent access between tasks to update vecD[j]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "href": "Courses/1_Intro.html#illustration-columnrow-wise-matrix-sum-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.8 Illustration: column/row-wise matrix sum algorithm",
    "text": "5.8 Illustration: column/row-wise matrix sum algorithm\nParallel column-wise vs parallel row-wise matrix sum algorithms\n\n\n\nMatrix 10000 \\times 10000\nObjective: run 10000 tasks\nResources: 64 computing units\nNumber of workers: 1, 2, 4, 8, 16, 32\n\nExercise 1: why the performance degradation?\n\n{\\rightarrow overhead for memory access}\n\nExercise 2: why the performance difference?\n\n{\\rightarrow impact of array storage order}\n\n\n\n\n20 repetitions in each configurations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#array-storage-order",
    "href": "Courses/1_Intro.html#array-storage-order",
    "title": "2  Introduction to parallel computing",
    "section": "5.9 Array storage order",
    "text": "5.9 Array storage order\nMatrix in memory = a big array of contiguous rows or columns\n\n\n\n5.9.1 Row-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n5.9.2 Column-major\nMemory: \n\\begin{array}{|c|c|c|}\n\\hline\na_{11} & a_{21} & a_{31} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{12} & a_{22} & a_{32} \\\\\n\\hline\n\\end{array}\\\n\\begin{array}{|c|c|c|}\n\\hline\na_{13} & a_{23} & a_{33} \\\\\n\\hline\n\\end{array}\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#accessing-array-elements-in-memory",
    "href": "Courses/1_Intro.html#accessing-array-elements-in-memory",
    "title": "2  Introduction to parallel computing",
    "section": "5.10 Accessing array elements in memory",
    "text": "5.10 Accessing array elements in memory\nMemory access: read data from memory by block\n\n\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\nTo access a_{11}: load \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#row-major-order-and-row-wise-sum",
    "href": "Courses/1_Intro.html#row-major-order-and-row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.11 Row-major order and row-wise sum",
    "text": "5.11 Row-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\ncompute res = res + a_{12}\ncompute res = res + a_{13}\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#column-major-order-and-row-wise-sum",
    "href": "Courses/1_Intro.html#column-major-order-and-row-wise-sum",
    "title": "2  Introduction to parallel computing",
    "section": "5.12 Column-major order and row-wise sum",
    "text": "5.12 Column-major order and row-wise sum\nTo compute a_{11} + a_{12} + a_{13} ?\n\n\n\n\ninit res =0\nload \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{11}\nload \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{12}\nload \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} into cache\ncompute res = res + a_{13} More memory accesses \\rightarrow time consuming\n\n\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#memory-access-to-large-data",
    "href": "Courses/1_Intro.html#memory-access-to-large-data",
    "title": "2  Introduction to parallel computing",
    "section": "5.13 Memory access to large data",
    "text": "5.13 Memory access to large data\nExample: “big” matrix (4 \\times 6) \\tiny \\begin{array}{|c|c|c|c|c|c|c|}\\hline a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\ \\hline a_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\ \\hline a_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\ \\hline a_{41} & a_{42} & a_{43} & a_{44} & a_{45} & a_{46} \\\\ \\hline\\end{array}\nStorage in memory (row major):\n\\tiny\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{21} & a_{22} & a_{23} & a_{24} & a_{25} & a_{26} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\na_{31} & a_{32} & a_{33} & a_{34} & a_{35} & a_{36} \\\\\n\\hline\n\\end{array}\n\\begin{array}{|c|c|ccc}\n\\hline\na_{41} & a_{42} & \\cdot & \\cdot & \\cdot \\\\\n\\hline\n\\end{array}\n\nAccess by sub-blocks4 of data (e.g. sub-block of rows or columns)\n\nExample: load block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} into cache to access a_{11}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#impact-of-data-dimension",
    "href": "Courses/1_Intro.html#impact-of-data-dimension",
    "title": "2  Introduction to parallel computing",
    "section": "5.14 Impact of data dimension",
    "text": "5.14 Impact of data dimension\nSum of row 1 (row major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{12} & a_{13} \\\\ \\hline \\end{array} res = res + a_{11} + a_{12} + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{15} & a_{16} \\\\ \\hline \\end{array} res = res + a_{14} + a_{15} + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#impact-of-data-dimension-ii",
    "href": "Courses/1_Intro.html#impact-of-data-dimension-ii",
    "title": "2  Introduction to parallel computing",
    "section": "5.15 Impact of data dimension, II",
    "text": "5.15 Impact of data dimension, II\nSum of row 1 (column major):\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{11} & a_{21} & a_{31} \\\\ \\hline \\end{array} res = res + a_{11}\naccess block \\begin{array}{|c|c|c|} \\hline a_{12} & a_{22} & a_{32} \\\\ \\hline \\end{array} res = res + a_{12}\naccess block \\begin{array}{|c|c|c|} \\hline a_{13} & a_{23} & a_{33} \\\\ \\hline \\end{array} res = res + a_{13}\n\n\n\naccess block \\begin{array}{|c|c|c|} \\hline a_{14} & a_{24} & a_{34} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{15} & a_{25} & a_{35} \\\\ \\hline \\end{array} res = res + a_{14}\naccess block \\begin{array}{|c|c|c|} \\hline a_{16} & a_{26} & a_{36} \\\\ \\hline \\end{array} res = res + a_{16}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#matrix-product",
    "href": "Courses/1_Intro.html#matrix-product",
    "title": "2  Introduction to parallel computing",
    "section": "5.16 Matrix product",
    "text": "5.16 Matrix product\n\n\n\nMatrix A = [a_{ij}]_{i=1:N}^{j=1:P} of dimension N\\times{}P\nMatrix B = [b_{jk}]_{j=1:P}^{k=1:Q} of dimension N\\times{}P\nMatrix product: C = A \\times B = [c_{ik}]_{i=1:N}^{k=1:Q} of dimension N\\times{}Q where\n\nc_{ik} = \\sum_{j=1}^P a_{ij} \\times b_{jk}\n\n\n\nSource: wikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#matrix-product-algorithm",
    "href": "Courses/1_Intro.html#matrix-product-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.17 Matrix product algorithm",
    "text": "5.17 Matrix product algorithm\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]\nExercise: parallel algorithm?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#naive-parallel-matrix-product",
    "href": "Courses/1_Intro.html#naive-parallel-matrix-product",
    "title": "2  Introduction to parallel computing",
    "section": "5.18 (naive) Parallel matrix product",
    "text": "5.18 (naive) Parallel matrix product\n# input\nmatA = np.array(...).reshape(N,P)\nmatA = np.array(...).reshape(P,Q)\n# output\nmatC = np.zeros((N,Q))\n# algorithm\n@parallel\nfor i in range(N):\n  for k in range(Q):\n    for j in range(P):\n      matC[i,k] += matA[i,j] * matB[j,k]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#divide-and-conquer-procedure",
    "href": "Courses/1_Intro.html#divide-and-conquer-procedure",
    "title": "2  Introduction to parallel computing",
    "section": "5.19 Divide-and-conquer procedure",
    "text": "5.19 Divide-and-conquer procedure\n\nDivide output and input matrices into blocks:\n\n\nC = \\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix},\\,\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix},\\,\nB = \\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix}\n\n\nCompute C = A \\times B by blocks:\n\n\n\\begin{darray}{rcl}\n\\begin{bmatrix}\nC_{11} & C_{12} \\\\\nC_{21} & C_{22} \\\\\n\\end{bmatrix}  & =\n& \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nB_{11} & B_{12} \\\\\nB_{21} & B_{22} \\\\\n\\end{bmatrix} \\\\\n& = & \\begin{bmatrix}\nA_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\\\\nA_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22}\\\\\n\\end{bmatrix}\n\\end{darray}\n\n\nPossible parallelization over sub-block products A_{ik} \\times B_{kj} and then over result sums \\rightarrow see also “tiled implementation”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#fibonacci-sequence",
    "href": "Courses/1_Intro.html#fibonacci-sequence",
    "title": "2  Introduction to parallel computing",
    "section": "5.20 Fibonacci sequence",
    "text": "5.20 Fibonacci sequence\nInitialization: f_0 = 0, f_1 = 1\nIteration: f_i = f_{i-1} + f_{i-2} for any i \\geq 2\nSequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, …\n\n\nAlgorithm:\nn = 100\nfib = np.zeros(100)\nfib[0] = 0\nfib[1] = 1\nfor i in range(2,n):\n    res[i] = res[i-1] + res[i-2]\n\n\nParallel version? (NO!) (at least not directly)\n\\rightarrow result i depends of result from previous iterations (i-1 and i-2) \n\\rightarrow dependency between iterations",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#markov-chain",
    "href": "Courses/1_Intro.html#markov-chain",
    "title": "2  Introduction to parallel computing",
    "section": "5.21 Markov chain",
    "text": "5.21 Markov chain\nMarkov chain: sequence of random variables (X_i)_{i&gt;1} such that \\mathbb{P}(X_i = x_i \\vert X_1 = x_1, X_2 = x_2, \\dots , X_{i-1} = x_{i-1}) = \\mathbb{P}(X_i = x_i \\vert X_{i-1} = x_{i-1})\nX_i\\in S where S is the state space\n\n\nExample:\n\ntwo states E and A, i.e S = \\{A, E\\}\ntransition probability matrix:\n\n\n\\begin{array}{cl}\n\\begin{array}{cc}\nA & E\n\\end{array} \\\\\n\\left(\\begin{array}{cc}\n0.6 & 0.4 \\\\\n0.3 & 0.7 \\\\\n\\end{array}\\right) &\n\\begin{array}{l}\nA \\\\\nE\n\\end{array}\n\\end{array}\n\n\n\n\n\n\nwikimedia.org",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#markov-chain-simulation-algorithm",
    "href": "Courses/1_Intro.html#markov-chain-simulation-algorithm",
    "title": "2  Introduction to parallel computing",
    "section": "5.22 Markov chain simulation algorithm",
    "text": "5.22 Markov chain simulation algorithm\n\nPick an initial state X_0 = x with x\\in S\nFor in in 1,\\dots, N:\n\nSimulate X_i\\in S from the probability distribution given by state X_{i-1}\n\n\nFor the simulation:\n\nIf X_{i-1} = A then \\mathbb{P}(X_i = A) = 0.6 and \\mathbb{P}(X_i = E) = 0.4\nIf X_{i-1} = E then \\mathbb{P}(X_i = A) = 0.7 and \\mathbb{P}(X_i = E) = 0.3\n\nExercise: parallel algorithm?\n\nNO!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#take-home-message",
    "href": "Courses/1_Intro.html#take-home-message",
    "title": "2  Introduction to parallel computing",
    "section": "6.1 Take home message",
    "text": "6.1 Take home message\n\nParallel computing can be used to run computations faster (i.e. save time)\nRelationship between time gain and number of tasks run in parallel is not linear\nPotential bottlenecks leading to potential performance loss:\n\nmanagement of parallel tasks\noverhead for computing resource access\noverhead for memory access\nconcurrent memory access by parallel tasks",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/1_Intro.html#footnotes",
    "href": "Courses/1_Intro.html#footnotes",
    "title": "2  Introduction to parallel computing",
    "section": "",
    "text": "i.e. a set of tools/machines used by a worker to complete a task↩︎\noverhead on resource access↩︎\nor “processors”↩︎\nthe size of the block depends on the size of the cache↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to parallel computing</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html",
    "href": "Courses/2_Concepts.html",
    "title": "3  Advanced concepts in parallel programming",
    "section": "",
    "text": "4 Why parallel computing",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#trend-over-50years",
    "href": "Courses/2_Concepts.html#trend-over-50years",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.1 Trend over ~50years",
    "text": "4.1 Trend over ~50years\n\n\nMoore’s Law (doubling the transistor counts every two years) is live\nSingle thread performance hit a wall in 2000s\nAlong with typical power usage and frequency\nNumber of logical cores is doubling every ~3 years since mid-2000\n\n\n\n\nOriginal data up to the year 2010 collected and plotted by M. Horowitz, F. Labonte, O. Shacham, K. Olukotun, L. Hammond, and C. Batten New plot and data collected for 2010-2021 by K. Rupp\n\n\n\n\nGithub repo for data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#computing-units",
    "href": "Courses/2_Concepts.html#computing-units",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.2 Computing units",
    "text": "4.2 Computing units\n\n\nCPU :\n\n4/8/16+ execution cores (depending on context, laptop, desktop, server)\nHyperthreading (Intel) or SMT (AMD), x2\nVector units (multiple instructions processed on a vector of data)\n\nGPU computing : 100/1000 “simple” cores per card",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#the-reality",
    "href": "Courses/2_Concepts.html#the-reality",
    "title": "3  Advanced concepts in parallel programming",
    "section": "4.3 The reality",
    "text": "4.3 The reality\n\n\nA serial application only accesses 0.8% of the processing power of a 16-core CPU.\n\n\n\n0.08\\% = \\frac{1}{16 * 2 (cores + hyperthreading) * \\frac{256 (bitwide vector unit}{64(bit double)} = 128}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#faster-for-less-development",
    "href": "Courses/2_Concepts.html#faster-for-less-development",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.1 Faster for less development",
    "text": "5.1 Faster for less development\n\\frac{S_{up}}{T_{par}} \\gg \\frac{S_{up}}{T_{seq}}\nRatio of speedup improvment S_{up} over time of development (T_{seq|par}) comparison.\nFrom a development time perspective, return on investment (speedup) is often several magnitudes of order better than pure “serial/sequential” improvment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#scaling",
    "href": "Courses/2_Concepts.html#scaling",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.2 Scaling",
    "text": "5.2 Scaling\nSimple “divide and conquer” strategies in parallel programming allow to handle data with previously almost untractable sizes and scale before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency",
    "href": "Courses/2_Concepts.html#energy-efficiency",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.3 Energy efficiency",
    "text": "5.3 Energy efficiency\n\n\n\n\n\n\nNote\n\n\n\nThis is a huge one, in the present context 😬\n\n\nDifficult to estimate but the Thermal Design Power (TDP), given by hardware manufacturers, is a good rule of thumb. Just factor the number of units, and usual proportionality rules.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency-a-bunch-of-cpus",
    "href": "Courses/2_Concepts.html#energy-efficiency-a-bunch-of-cpus",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.4 Energy efficiency, a bunch of CPUs",
    "text": "5.4 Energy efficiency, a bunch of CPUs\nExample of “standard” use : 20 16-core Intel Xeon E5-4660 which is 120~W of TDP\nP = (20~Processors) * (120~W/~Processors) * (24~hours) = 57.60~kWhrs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#energy-efficiency-just-a-few-big-gpus",
    "href": "Courses/2_Concepts.html#energy-efficiency-just-a-few-big-gpus",
    "title": "3  Advanced concepts in parallel programming",
    "section": "5.5 Energy efficiency, just a few (big) GPUs",
    "text": "5.5 Energy efficiency, just a few (big) GPUs\nA Tesla V100 GPU is of 300~W of TDP. Let’s use 4 of them.\nP = (4~GPUs) * (300~W/~GPUs) * (24~hours) = 28.80~kWhrs\n\\Longrightarrow half of the power use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#terms-and-definitions",
    "href": "Courses/2_Concepts.html#terms-and-definitions",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.1 Terms and definitions",
    "text": "6.1 Terms and definitions\n\nSpeedup S_{up}(N): ratio of the time of execution in serial and parallel mode\nNumber of computing units N\nP (resp. S) is the parallel (resp. serial) fraction of the time spent in the parallel (resp. serial) part of the program (P+S=1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law",
    "href": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.2 Asymptote of parallel computing : Amdahl’s Law",
    "text": "6.2 Asymptote of parallel computing : Amdahl’s Law\nThere P is the fraction of the time spent in the parallel part of the program in a sequential execution.\nS_{up}(N) \\le \\frac{1}{S+\\frac{P}{N}}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "href": "Courses/2_Concepts.html#asymptote-of-parallel-computing-amdahls-law-graphic",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.3 Asymptote of parallel computing : Amdahl’s Law, Graphic",
    "text": "6.3 Asymptote of parallel computing : Amdahl’s Law, Graphic\n\n\nIdeal speedup : 100% of the code parallelized; 90%, 75%, and 50% : limited by the fractions of code that remain serial. (Robey and Zamora 2021)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#more-with-almost-less-the-pump-it-up-approach",
    "href": "Courses/2_Concepts.html#more-with-almost-less-the-pump-it-up-approach",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.4 More with (almost) less : the pump it up approach",
    "text": "6.4 More with (almost) less : the pump it up approach\nGustafson’s law\nThere now, P is the fraction of the time spent in the parallel part of the program in a parallel execution.\n\n\n\n\nWhen the size of the problem grows up proportionnaly to the number of computing units.\nS_{up}(N) \\le N - S*(N-1)\nwhere N is the number of computing units and S the serial fraction as before.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#more-with-almost-less-graphic",
    "href": "Courses/2_Concepts.html#more-with-almost-less-graphic",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.5 More with (almost) less : graphic",
    "text": "6.5 More with (almost) less : graphic\n\n\nLinear growth with the number of processor (and data size too)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#strong-vs-weak-scaling-definitions",
    "href": "Courses/2_Concepts.html#strong-vs-weak-scaling-definitions",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.6 Strong vs Weak Scaling, definitions",
    "text": "6.6 Strong vs Weak Scaling, definitions\n\n\nStrong Scaling\n\nStrong scaling represents the time to solution with respect to the number of processors for a fixed total size.\n\n\n\\Rightarrow Amdahl’s law\n\nWeak Scaling\n\nWeak scaling represents the time to solution with respect to the number of processors for a fixed-sized problem per processor.\n\n\n\\Rightarrow Gustafson’s law",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#strong-vs-weak-scaling-schemas",
    "href": "Courses/2_Concepts.html#strong-vs-weak-scaling-schemas",
    "title": "3  Advanced concepts in parallel programming",
    "section": "6.7 Strong vs Weak Scaling, schemas",
    "text": "6.7 Strong vs Weak Scaling, schemas\n\n\n┌────────────────────────────────────┐\n│                 1000               │\n│         ┌───────────────────┐      │\n│         │                   │      │           1 processor\n│         │                   │      │\n│         │                   │      │\n│ 1000    │                   │      │           \n│         │                   │      │\n│         │                   │      │\n│         └───────────────────┘      │\n│        ┌─────────┐  ┌─────────┐    │\n│        │         │  │         │    │\n│ 500    │         │  │         │    │\n│        │         │  │         │    │\n│        └─────────┘  └─────────┘    │\n│           500                      │           4 processors\n│        ┌─────────┐  ┌─────────┐    │\n│        │         │  │         │    │\n│        │         │  │         │    │\n│        │         │  │         │    │\n│        └─────────┘  └─────────┘    │\n│      250                           │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│ 250 │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │           16 processors\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n│     ┌────┐  ┌────┐  ┌────┐  ┌────┐ │\n│     │    │  │    │  │    │  │    │ │\n│     └────┘  └────┘  └────┘  └────┘ │\n└────────────────────────────────────┘\n\n┌───────────────────────────────────────────────────────────┐\n│                         1000                              │\n│                      ┌─────────┐                          │\n│                      │         │                          │\n│              1000    │      ───┼──┐                       │\n│                      │         │  │                       │\n│                      └─────────┘  │                       │\n│                   1000            │                       │\n│                 ┌─────────┐  ┌────┼────┐                  │\n│                 │         │  │    │    │                  │\n│           1000  │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 └─────────┘  └────┼────┘                  │\n│                 ┌─────────┐  ┌────┼────┐                  │\n│                 │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 │         │  │    │    │                  │\n│                 └─────────┘  └────┼────┘                  │\n│                                   │         1000          │\n│    ┌─────────┐  ┌─────────┐  ┌────┼────┐  ┌─────────┐     │\n│    │         │  │         │  │    │    │  │         │     │\n│    │         │  │         │  │    ▼    │  │         │1000 │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n│    ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    │         │  │         │  │         │  │         │     │\n│    └─────────┘  └─────────┘  └─────────┘  └─────────┘     │\n└───────────────────────────────────────────────────────────┘",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#flynns-taxonomy",
    "href": "Courses/2_Concepts.html#flynns-taxonomy",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.1 Flynn’s taxonomy",
    "text": "7.1 Flynn’s taxonomy\n\n\n\n\nSimple Instruction\nMultiple Instructions\n\n\n\n\nSimple Data\n\n\n\n\nMultiple Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#a-different-approach",
    "href": "Courses/2_Concepts.html#a-different-approach",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.2 A different approach",
    "text": "7.2 A different approach\n\n\n\nParallelism level\nHardware\nSoftware\nParallelism extraction\n\n\n\n\nInstruction\nSIMD (or VLIW)\nIntrinsics\nCompiler\n\n\nThread\nMulti-core RTOS\nLibrary or language extension\nPartitioning/Scheduling (dependency control)\n\n\nTask\nMulti-core (w/o RTOS)\nProcesses (OS level)\nPartitioning/Scheduling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/2_Concepts.html#multi-processing-vs-multi-threading",
    "href": "Courses/2_Concepts.html#multi-processing-vs-multi-threading",
    "title": "3  Advanced concepts in parallel programming",
    "section": "7.3 Multi-processing vs Multi-threading",
    "text": "7.3 Multi-processing vs Multi-threading\n\n\n\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading\n\n\n\n\n\n\n\n\n\n\nMulti-processing\nMulti-threading\n\n\n\n\nMemory\nExclusive\nShared\n\n\nCommunication\nInter-process\nAt caller site\n\n\nCreation overhead\nHeavy\nMinimal\n\n\nConcurrency\nAt OS level\nLibrary/language",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Advanced concepts in parallel programming</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html",
    "href": "Courses/3_Asynchronous.html",
    "title": "4  Asynchronous Programming with Python",
    "section": "",
    "text": "5 Asynchronous, Basics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#what-is-asynchronous-programming",
    "href": "Courses/3_Asynchronous.html#what-is-asynchronous-programming",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.1 What is Asynchronous Programming?",
    "text": "5.1 What is Asynchronous Programming?\n\nAsynchronous programming is a programming paradigm that allows the program to continue executing other tasks before the current task is finished.\nIt is a way to achieve concurrency in a program.\n\n\\Rightarrow it is an abstraction over concurrency, it does not necessarily mean that the program is executed in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#io-bound-vs.-cpu-bound",
    "href": "Courses/3_Asynchronous.html#io-bound-vs.-cpu-bound",
    "title": "4  Asynchronous Programming with Python",
    "section": "5.2 I/O Bound vs. CPU Bound",
    "text": "5.2 I/O Bound vs. CPU Bound\nimport requests\n \n1response = requests.get('https://www.example.com')\n \nitems = response.headers.items()\n \n2headers = [f'{key}: {header}' for key, header in items]\n \n3formatted_headers = '\\n'.join(headers)\n \nwith open('headers.txt', 'w') as file:\n4    file.write(formatted_headers)\n\n1\n\nI/O-bound web request\n\n2\n\nCPU-bound response processing\n\n3\n\nCPU-bound string concatenation\n\n4\n\nI/O-bound write to disk",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#concurrency-vs.-parallelism",
    "href": "Courses/3_Asynchronous.html#concurrency-vs.-parallelism",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.1 Concurrency vs. Parallelism",
    "text": "6.1 Concurrency vs. Parallelism\n\n\nOne baker and two cakes to prepare.\n\nCan preheat the oven while preparing the first cake.\nCan start the second cake while the first one is in the oven.\n\n\\Rightarrow Switching between tasks is concurrency (or concurrent behavior).\n\nTwo bakers and two cakes to prepare.\n\nCan prepare both cakes at the same time.\n\n\\Rightarrow Doing multiple tasks in parallel is parallelism (or parallel behavior).\n\n\n\n\nWith concurrency, we have multiple tasks happening at the same time, but only one we’re actively doing at a given point in time. With parallelism, we have multiple tasks happening and are actively doing more than one simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\n\nWith concurrency, we switch between running two applications. With parallelism, we actively run two applications simultaneously.\n\n\n\n\nFrom Fowler (2022)\n\n\nConcurrency is about multiple independent tasks that can happen.\nParallelism is concurrency AND simultaneous execution.\n\nWhile parallelism implies concurrency, concurrency does not always imply parallelism.\n\\Rightarrow Concurrency is a broader concept than parallelism.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#multitasking",
    "href": "Courses/3_Asynchronous.html#multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.2 Multitasking",
    "text": "6.2 Multitasking\n\n\n\n6.2.1 Preemptive multitasking\n\nThe operating system decides when to switch between tasks.\nThe tasks are not aware of each other.\n\n\n\n\n6.2.2 Cooperative multitasking\n\nIn this model we have to explicitly to decide when to switch between tasks.\nThe tasks are aware of each other.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#benefits-of-cooperative-multitasking",
    "href": "Courses/3_Asynchronous.html#benefits-of-cooperative-multitasking",
    "title": "4  Asynchronous Programming with Python",
    "section": "6.3 Benefits of cooperative multitasking",
    "text": "6.3 Benefits of cooperative multitasking\n\nLess overhead than preemptive multitasking.\nGranular/optimal control over when to switch between tasks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#multi-processing-vs-multi-threading",
    "href": "Courses/3_Asynchronous.html#multi-processing-vs-multi-threading",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.1 Multi-processing vs Multi-threading",
    "text": "7.1 Multi-processing vs Multi-threading\n\n\n\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#processes-and-threads",
    "href": "Courses/3_Asynchronous.html#processes-and-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.2 Processes and threads",
    "text": "7.2 Processes and threads\n\nimport os\nimport threading\n \nprint(f'Python process running with process id: {os.getpid()}')\ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n\nPython process running with process id: 119943\nPython is currently running 8 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#creating-processes",
    "href": "Courses/3_Asynchronous.html#creating-processes",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.3 Creating processes",
    "text": "7.3 Creating processes\n\nimport multiprocessing\nimport os\n \n \ndef hello_from_process():\n    print(f'Hello from child process {os.getpid()}!')\nif __name__ == '__main__':\n    hello_process = multiprocessing.Process(target=hello_from_process)\n    hello_process.start()\n \n    print(f'Hello from parent process {os.getpid()}')\n \n    hello_process.join()\n\nHello from child process 119972!\nHello from parent process 119943",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#creating-threads",
    "href": "Courses/3_Asynchronous.html#creating-threads",
    "title": "4  Asynchronous Programming with Python",
    "section": "7.4 Creating threads",
    "text": "7.4 Creating threads\n\nimport threading\n \n \ndef hello_from_thread():\n    print(f'Hello from thread {threading.current_thread()}!')\n \n \nhello_thread = threading.Thread(target=hello_from_thread)\nhello_thread.start()\n \ntotal_threads = threading.active_count()\nthread_name = threading.current_thread().name\n \nprint(f'Python is currently running {total_threads} thread(s)')\nprint(f'The current thread is {thread_name}')\n \nhello_thread.join()\n\nHello from thread &lt;Thread(Thread-6 (hello_from_thread), started 139685613672192)&gt;!\nPython is currently running 8 thread(s)\nThe current thread is MainThread",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#what-about-python",
    "href": "Courses/3_Asynchronous.html#what-about-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.1 What about Python?",
    "text": "8.1 What about Python?\n\n\nDesigned for sequential and single-core architecture from the beginning\nEverything is interpreted\nAll dynamic (no static types)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#the-gil",
    "href": "Courses/3_Asynchronous.html#the-gil",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.2 The GIL",
    "text": "8.2 The GIL\nAka Global Interpreter Lock\n. . .\n\nThe GIL allows thread usage, you can create threads and launch them: YES!\n\n. . .\n\nbut…\n\n. . .\n\nOnly ONE thread can actually execute code at python level..",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#multi-threaded-parallel-execution",
    "href": "Courses/3_Asynchronous.html#multi-threaded-parallel-execution",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.3 Multi-threaded != Parallel execution",
    "text": "8.3 Multi-threaded != Parallel execution\nMulti-threading doesn’t guarantee parallel execution…\n\n\n\n\n\n\\Longrightarrow Python seems to have started off with the wrong foot by a long way…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#high-performance-python",
    "href": "Courses/3_Asynchronous.html#high-performance-python",
    "title": "4  Asynchronous Programming with Python",
    "section": "8.4 High performance Python 😬",
    "text": "8.4 High performance Python 😬\n\n\n\n\n\n\nBut wait!\n\n\nActually we can run (real) parallel programs with the multiprocessing package.\n\\Rightarrow But this is an “OS level” multiprocessing, with associated huge overhead (relatively)\nPython actually releases the GIL when executing everything that is not Python code (e.g. C/C++ extensions and libraries)\n\\Rightarrow It means we can parallelize our code by using I/O bound and CPU bound libraries that release the GIL (this is the case for most of them)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#socket",
    "href": "Courses/3_Asynchronous.html#socket",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.1 Socket",
    "text": "9.1 Socket\n\n\nWriting bytes to a socket and reading bytes from a socket\n\n\n\n\nFrom Fowler (2022)\n\n\nThis a mailbox metaphor\nBy default, the socket is blocking, i.e. the program will wait until the socket is ready to be read or written.\nWe can make the socket non-blocking, i.e. the program will not wait for the socket to be ready to be read or written. \\Rightarrow Later on, the OS will tell us we received byte and we deal with it.\n\n\n\n\n\n\n\nMaking a non-blocking I/O request returns immediately\ntells the O/S to watch sockets for data \\Rightarrow This allows execute_other_code() to run right away instead of waiting for the I/O requests to finish\nLater, we can be alerted when I/O is complete and process the response.\n\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#event-loop",
    "href": "Courses/3_Asynchronous.html#event-loop",
    "title": "4  Asynchronous Programming with Python",
    "section": "9.2 Event Loop",
    "text": "9.2 Event Loop\n\n\nfrom collections import deque\n \nmessages = deque()\n \nwhile True:\n    if messages:\n        message = messages.pop()\n        process_message(message)\n\n\n\nThe event loop is a loop that runs forever.\nIt checks if there are any messages to process.\nIf there are, it processes them.\nIf there are not, it waits for messages to arrive.\n\n\n\n\n\\Rightarrow In asyncio, the event loop is queue of tasks instead of messages, Tasks are wrapped coroutines.\n\ndef make_request():\n    cpu_bound_setup()\n    io_bound_web_request()\n    cpu_bound_postprocess()\n \ntask_one = make_request()\ntask_two = make_request()\ntask_three = make_request()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#what-is-it",
    "href": "Courses/3_Asynchronous.html#what-is-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.1 What is it?",
    "text": "10.1 What is it?\n\nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \ndef add_one(number: int) -&gt; int:\n    return number + 1\n \n1function_result = add_one(1)\n2coroutine_result = coroutine_add_one(1)\n \nprint(f'Function result is {function_result}\\n\\\n    and the type is {type(function_result)}')\nprint(f'Coroutine result is {coroutine_result}\\n\\\n    and the type is {type(coroutine_result)}')\n\n\n1\n\nfunction call, is executed immediately.\n\n2\n\ncoroutine call, is not executed at all, but returns a coroutine object.\n\n\n\n\nFunction result is 2\n    and the type is &lt;class 'int'&gt;\nCoroutine result is &lt;coroutine object coroutine_add_one at 0x7f0afc9c1a80&gt;\n    and the type is &lt;class 'coroutine'&gt;\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#how-to-execute-a-coroutine",
    "href": "Courses/3_Asynchronous.html#how-to-execute-a-coroutine",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.2 How to execute a coroutine?",
    "text": "10.2 How to execute a coroutine?\nYou need an event loop.\nimport asyncio\n \nasync def coroutine_add_one(number: int) -&gt; int:\n    return number + 1\n \n1result = asyncio.run(coroutine_add_one(1))\n\nprint(result)\n\n1\n\nThis launches the event loop, executes the coroutine, and returns the result.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis code will not work in a Jupyter notebook, because the event loop is already running (by Jupyter itself). So you just have to replace the line 4 by:\nresult = await coroutine_add_one(1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#await-keyword",
    "href": "Courses/3_Asynchronous.html#await-keyword",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.3 await keyword",
    "text": "10.3 await keyword\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \n \nasync def main() -&gt; None:\n1    one_plus_one = await add_one(1)\n2    two_plus_one = await add_one(2)\n    print(one_plus_one)\n    print(two_plus_one)\n \n3await main()\n\n\n1\n\nPause, and wait for the result of add_one(1).\n\n2\n\nPause, and wait for the result of add_one(2).\n\n3\n\nPause, and wait for the result of main(). (outside of a Jupyter notebook, you have to launch the event loop somewhere, like asyncio.run(main()) instead of await main())\n\n\n\n\n2\n3\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "href": "Courses/3_Asynchronous.html#simulating-the-real-thing-with-asyncio.sleep",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.4 Simulating the real thing with asyncio.sleep",
    "text": "10.4 Simulating the real thing with asyncio.sleep\n\nimport asyncio\n \nasync def hello_world_message() -&gt; str:\n1    await asyncio.sleep(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n2    message = await hello_world_message()\n    print(message)\n \nawait main()\n\n\n1\n\nPause hello_world_message for 1 second.\n\n2\n\nPause main until hello_world_message is finished.\n\n\n\n\nHello World!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#utility-function-delayseconds",
    "href": "Courses/3_Asynchronous.html#utility-function-delayseconds",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.5 Utility function delay(seconds)",
    "text": "10.5 Utility function delay(seconds)\n\nimport asyncio\n \n \n1async def delay(delay_seconds: int) -&gt; int:\n2    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n3    return delay_seconds\n\n\n1\n\nTakes an integer of the duration in seconds that we’d like the function to sleep.\n\n2\n\nPrints when sleep begins and ends.\n\n3\n\nReturns that integer to the caller once it has finished sleeping.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#running-two-coroutines",
    "href": "Courses/3_Asynchronous.html#running-two-coroutines",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.6 Running two coroutines",
    "text": "10.6 Running two coroutines\n\nimport asyncio\n \nasync def add_one(number: int) -&gt; int:\n    return number + 1\n \nasync def hello_world_message() -&gt; str:\n    await delay(1)\n    return 'Hello World!'\n \nasync def main() -&gt; None:\n1    message = await hello_world_message()\n2    one_plus_one = await add_one(1)\n    print(one_plus_one)\n    print(message)\n \nawait main()\n\n\n1\n\nPause main until hello_world_message is finished.\n\n2\n\nPause main until add_one is finished.\n\n\n\n\nsleeping for 1 second(s)\nfinished sleeping for 1 second(s)\n2\nHello World!\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#what-to-do-next",
    "href": "Courses/3_Asynchronous.html#what-to-do-next",
    "title": "4  Asynchronous Programming with Python",
    "section": "10.7 What to do next?",
    "text": "10.7 What to do next?\nMoving away from sequential execution and run add_one and hello_world_message concurrently.\nFor that we need…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#creating-tasks",
    "href": "Courses/3_Asynchronous.html#creating-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.1 Creating tasks",
    "text": "11.1 Creating tasks\n\nimport asyncio\n\nasync def main():\n    sleep_for_three = asyncio.create_task(delay(3))\n    print(type(sleep_for_three))\n    result = await sleep_for_three\n    print(result)\n \nawait main()\n\n&lt;class '_asyncio.Task'&gt;\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n3\n\n\n\nthe coroutine is scheduled to run in the event loop as soon as possible.\nbefore, it was just run at the await statement (pausing the caller).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#running-tasks-concurrently",
    "href": "Courses/3_Asynchronous.html#running-tasks-concurrently",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.2 Running tasks concurrently",
    "text": "11.2 Running tasks concurrently\n\nimport asyncio\n \nasync def main():\n    sleep_for_three = \\\n        asyncio.create_task(delay(3))\n    sleep_again = \\\n        asyncio.create_task(delay(3))\n    sleep_once_more = \\\n        asyncio.create_task(delay(3))\n \n    await sleep_for_three\n    await sleep_again\n    await sleep_once_more\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)\n\n\nimport asyncio\n \nasync def hello_every_second():\n    for i in range(2):\n        await asyncio.sleep(1)\n        print(\"I'm running other code while I'm waiting!\")\n \nasync def main():\n    first_delay = asyncio.create_task(delay(3))\n    second_delay = asyncio.create_task(delay(3))\n    await hello_every_second()\n    await first_delay\n    await second_delay\n\nawait main()\n\nsleeping for 3 second(s)\nsleeping for 3 second(s)\nI'm running other code while I'm waiting!\nI'm running other code while I'm waiting!\nfinished sleeping for 3 second(s)\nfinished sleeping for 3 second(s)\n\n\n\n\nFrom Fowler (2022)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#canceling-tasks",
    "href": "Courses/3_Asynchronous.html#canceling-tasks",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.3 Canceling tasks",
    "text": "11.3 Canceling tasks\n\nimport asyncio\nfrom asyncio import CancelledError\n\nasync def main():\n    long_task = asyncio.create_task(delay(10))\n \n    seconds_elapsed = 0\n \n    while not long_task.done():\n        print('Task not finished, checking again in a second.')\n        await asyncio.sleep(1)\n        seconds_elapsed = seconds_elapsed + 1\n        if seconds_elapsed == 5:\n            long_task.cancel()\n \n    try:\n        await long_task\n    except CancelledError:\n        print('Our task was cancelled')\n \nawait main()\n\nTask not finished, checking again in a second.\nsleeping for 10 second(s)\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nTask not finished, checking again in a second.\nOur task was cancelled",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#setting-a-timeout",
    "href": "Courses/3_Asynchronous.html#setting-a-timeout",
    "title": "4  Asynchronous Programming with Python",
    "section": "11.4 Setting a timeout",
    "text": "11.4 Setting a timeout\n\nimport asyncio\n\nasync def main():\n    delay_task = asyncio.create_task(delay(2))\n    try:\n        result = await asyncio.wait_for(delay_task, timeout=1)\n        print(result)\n    except asyncio.exceptions.TimeoutError:\n        print('Got a timeout!')\n        print(f'Was the task cancelled? {delay_task.cancelled()}')\n \nawait main()\n\nsleeping for 2 second(s)\nGot a timeout!\nWas the task cancelled? True",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#introducing-futures",
    "href": "Courses/3_Asynchronous.html#introducing-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.1 Introducing futures",
    "text": "12.1 Introducing futures\n\nfrom asyncio import Future\n \nmy_future = Future()\n \nprint(f'Is my_future done? {my_future.done()}')\n \nmy_future.set_result(42)\n \nprint(f'Is my_future done? {my_future.done()}')\nprint(f'What is the result of my_future? {my_future.result()}')\n\nIs my_future done? False\nIs my_future done? True\nWhat is the result of my_future? 42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#awaiting-futures",
    "href": "Courses/3_Asynchronous.html#awaiting-futures",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.2 Awaiting futures",
    "text": "12.2 Awaiting futures\n\nfrom asyncio import Future\nimport asyncio\n \n \ndef make_request() -&gt; Future:\n    future = Future()\n1    asyncio.create_task(set_future_value(future))\n    return future\n \n \nasync def set_future_value(future) -&gt; None:\n2    await asyncio.sleep(1)\n    future.set_result(42)\n \n \nasync def main():\n    future = make_request()\n    print(f'Is the future done? {future.done()}')\n3    value = await future\n    print(f'Is the future done? {future.done()}')\n    print(value)\n \nawait main()\n\n\n1\n\nCreate a task to asynchronously set the value of the future.\n\n2\n\nWait 1 second before setting the value of the future.\n\n3\n\nPause main until the future’s value is set.\n\n\n\n\nIs the future done? False\nIs the future done? True\n42",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "href": "Courses/3_Asynchronous.html#comparing-tasks-coroutines-futures-and-awaitables",
    "title": "4  Asynchronous Programming with Python",
    "section": "12.3 Comparing tasks, coroutines, futures, and awaitables",
    "text": "12.3 Comparing tasks, coroutines, futures, and awaitables\n\n\n\n\n\n\n\nAwaitables\n\nObjects that can be awaited in an async function, including coroutines, tasks, and futures.\n\nCoroutines\n\nSpecial functions that can be paused and resumed later, defined using async def, and can be awaited to allow other coroutines to run.\n\nFutures\n\nRepresent the result of an asynchronous operation, manage its state, and can be awaited to get the result.\n\nTasks\n\nSchedule and run coroutines concurrently, and can be used to cancel or check their status.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#with-a-decorator",
    "href": "Courses/3_Asynchronous.html#with-a-decorator",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.1 With a decorator",
    "text": "13.1 With a decorator\n\n\n\nimport functools\nimport time\nfrom typing import Callable, Any\n \ndef async_timed():\n    def wrapper(func: Callable) -&gt; Callable:\n        @functools.wraps(func)\n        async def wrapped(*args, **kwargs) -&gt; Any:\n            print(f'starting {func} with args {args} {kwargs}')\n            start = time.time()\n            try:\n                return await func(*args, **kwargs)\n            finally:\n                end = time.time()\n                total = end - start\n                print(f'finished {func} in {total:.4f} second(s)')\n \n        return wrapped\n \n    return wrapper\n\n\nOfficial Python documentation for decorators\n\nadd functionality to an existing function\nwithout modifying the function itself\nit intercepts the function call and runs “decorated” code before and after it",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#using-it",
    "href": "Courses/3_Asynchronous.html#using-it",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.2 Using it",
    "text": "13.2 Using it\n\nimport asyncio\n \n@async_timed()\nasync def delay(delay_seconds: int) -&gt; int:\n    print(f'sleeping for {delay_seconds} second(s)')\n    await asyncio.sleep(delay_seconds)\n    print(f'finished sleeping for {delay_seconds} second(s)')\n    return delay_seconds\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(delay(2))\n    task_two = asyncio.create_task(delay(3))\n \n    await task_one\n    await task_two\n\nawait main()\n\nstarting &lt;function main at 0x7f0b593465c0&gt; with args () {}\nstarting &lt;function delay at 0x7f0b59345e40&gt; with args (2,) {}\nsleeping for 2 second(s)\nstarting &lt;function delay at 0x7f0b59345e40&gt; with args (3,) {}\nsleeping for 3 second(s)\nfinished sleeping for 2 second(s)\nfinished &lt;function delay at 0x7f0b59345e40&gt; in 2.0021 second(s)\nfinished sleeping for 3 second(s)\nfinished &lt;function delay at 0x7f0b59345e40&gt; in 3.0011 second(s)\nfinished &lt;function main at 0x7f0b593465c0&gt; in 3.0013 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#asyncio.gather",
    "href": "Courses/3_Asynchronous.html#asyncio.gather",
    "title": "4  Asynchronous Programming with Python",
    "section": "13.3 asyncio.gather",
    "text": "13.3 asyncio.gather\nasyncio.gather() runs multiple asynchronous operations, wraps a coroutine as a task, and returns a list of results in the same order of awaitables.\n\nimport asyncio\n\n\nasync def call_api(message, result, delay=3):\n    print(message)\n    await asyncio.sleep(delay)\n    return result\n\n\nasync def main():\n    return await asyncio.gather(\n        call_api('Calling API 1 ...', 1),\n        call_api('Calling API 2 ...', 2)\n    )\n\nawait main()\n\nCalling API 1 ...\nCalling API 2 ...\n\n\n[1, 2]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nasyncio.gather takes a tuple of awaitables, not a list of awaitables, but returns a list of results in the same order of awaitables.\nIf you want to pass a list, use the * operator to unpack it as a tuple.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#running-cpu-bound-code",
    "href": "Courses/3_Asynchronous.html#running-cpu-bound-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.1 Running CPU-bound code",
    "text": "14.1 Running CPU-bound code\n\nimport asyncio\n\n@async_timed()\nasync def cpu_bound_work() -&gt; int:\n    counter = 0\n    for i in range(100000000):\n        counter = counter + 1\n    return counter\n \n \n@async_timed()\nasync def main():\n    task_one = asyncio.create_task(cpu_bound_work())\n    task_two = asyncio.create_task(cpu_bound_work())\n    await task_one\n    await task_two\n \nawait main()\n\nstarting &lt;function main at 0x7f0b59346ac0&gt; with args () {}\nstarting &lt;function cpu_bound_work at 0x7f0b593468e0&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x7f0b593468e0&gt; in 4.1800 second(s)\nstarting &lt;function cpu_bound_work at 0x7f0b593468e0&gt; with args () {}\nfinished &lt;function cpu_bound_work at 0x7f0b593468e0&gt; in 4.1962 second(s)\nfinished &lt;function main at 0x7f0b59346ac0&gt; in 8.3767 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#running-blocking-apis",
    "href": "Courses/3_Asynchronous.html#running-blocking-apis",
    "title": "4  Asynchronous Programming with Python",
    "section": "14.2 Running blocking APIs",
    "text": "14.2 Running blocking APIs\n\nimport asyncio\nimport requests\n \n@async_timed()\nasync def get_example_status() -&gt; int:\n    return requests.get('http://www.example.com').status_code\n \n \n@async_timed()\nasync def main():\n    task_1 = asyncio.create_task(get_example_status())\n    task_2 = asyncio.create_task(get_example_status())\n    task_3 = asyncio.create_task(get_example_status())\n    await task_1\n    await task_2\n    await task_3\n \nawait main()\n\nstarting &lt;function main at 0x7f0b586cafc0&gt; with args () {}\nstarting &lt;function get_example_status at 0x7f0b59323ec0&gt; with args () {}\nfinished &lt;function get_example_status at 0x7f0b59323ec0&gt; in 0.3124 second(s)\nstarting &lt;function get_example_status at 0x7f0b59323ec0&gt; with args () {}\nfinished &lt;function get_example_status at 0x7f0b59323ec0&gt; in 0.1888 second(s)\nstarting &lt;function get_example_status at 0x7f0b59323ec0&gt; with args () {}\nfinished &lt;function get_example_status at 0x7f0b59323ec0&gt; in 0.1788 second(s)\nfinished &lt;function main at 0x7f0b586cafc0&gt; in 0.6803 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#example-of-blocking-code",
    "href": "Courses/3_Asynchronous.html#example-of-blocking-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.1 Example of blocking code",
    "text": "15.1 Example of blocking code\n\nimport requests\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nurl = 'https://www.example.com'\nprint(get_status_code(url))\nprint(get_status_code(url))\n\n200\n200",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#thread-pool",
    "href": "Courses/3_Asynchronous.html#thread-pool",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.2 Thread Pool",
    "text": "15.2 Thread Pool\n\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\n \n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \nstart = time.time()\n \nwith ThreadPoolExecutor() as pool:\n    urls = ['https://www.example.com' for _ in range(10)]\n    results = pool.map(get_status_code, urls)\n    for result in results:\n        # print(result)\n        pass\n\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 0.3860 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#compare-with-sequential-code",
    "href": "Courses/3_Asynchronous.html#compare-with-sequential-code",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.3 Compare with sequential code",
    "text": "15.3 Compare with sequential code\n\nstart = time.time()\n \nurls = ['https://www.example.com' for _ in range(10)]\n \nfor url in urls:\n    result = get_status_code(url)\n    # print(result)\n \nend = time.time()\n \nprint(f'finished requests in {end - start:.4f} second(s)')\n\nfinished requests in 3.6004 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#thread-pool-with-asyncio",
    "href": "Courses/3_Asynchronous.html#thread-pool-with-asyncio",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.4 Thread pool with asyncio",
    "text": "15.4 Thread pool with asyncio\n\nimport functools\nimport requests\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n \ndef get_status_code(url: str) -&gt; int:\n    response = requests.get(url)\n    return response.status_code\n \n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        urls = ['https://www.example.com' for _ in range(10)]\n        tasks = [loop.run_in_executor(pool, functools.partial(get_status_code, url)) for url in urls]\n        results = await asyncio.gather(*tasks)\n        print(results)\n \nawait main()\n\nstarting &lt;function main at 0x7f0b5871bd80&gt; with args () {}\n[200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\nfinished &lt;function main at 0x7f0b5871bd80&gt; in 0.3865 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/3_Asynchronous.html#multithreading-with-numpy",
    "href": "Courses/3_Asynchronous.html#multithreading-with-numpy",
    "title": "4  Asynchronous Programming with Python",
    "section": "15.5 Multithreading with numpy",
    "text": "15.5 Multithreading with numpy\nLet’s define a big matrix on which we will compute the mean of each row.\nNow process the matrix sequentially.\n\ns = time.time()\n \nres_seq = np.mean(matrix, axis=1)\n \ne = time.time()\nprint(e - s)\n\n0.47089385986328125\n\n\nAnd then the same with multithreading (we check that the results are exactly the same).\n\nimport functools\nfrom concurrent.futures.thread import ThreadPoolExecutor\nimport asyncio\n \ndef mean_for_row(arr, row):\n    return np.mean(arr[row])\n \n@async_timed()\nasync def main():\n    loop = asyncio.get_running_loop()\n    with ThreadPoolExecutor() as pool:\n        tasks = []\n        for i in range(rows):\n            mean = functools.partial(mean_for_row, matrix, i)\n            tasks.append(loop.run_in_executor(pool, mean))\n \n        return await asyncio.gather(*tasks)\n\nres_threads = np.array(await main())\nnp.testing.assert_array_equal(res_seq, res_threads)\n\nstarting &lt;function main at 0x7f0b5871b9c0&gt; with args () {}\nfinished &lt;function main at 0x7f0b5871b9c0&gt; in 0.0771 second(s)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Asynchronous Programming with Python</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html",
    "href": "Courses/4_IPC_and_Locking.html",
    "title": "5  IPC and locking",
    "section": "",
    "text": "6 Inter-Process Communication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#remainder-on-process-level-parallelization",
    "href": "Courses/4_IPC_and_Locking.html#remainder-on-process-level-parallelization",
    "title": "5  IPC and locking",
    "section": "6.1 Remainder on Process-level parallelization",
    "text": "6.1 Remainder on Process-level parallelization\n\n\n\n\n\n\n\n\nMulti-Processing\n\n\n\n\n\n\n\nMulti-Threading",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#inter-process-is-easy",
    "href": "Courses/4_IPC_and_Locking.html#inter-process-is-easy",
    "title": "5  IPC and locking",
    "section": "6.2 Inter-process is easy…",
    "text": "6.2 Inter-process is easy…\n\n\nBut if my algorithm is not “embarrassingly parallel”, what if we want to share data between processes ?\nlet’s go for Shared Memory",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#shared-memory-model",
    "href": "Courses/4_IPC_and_Locking.html#shared-memory-model",
    "title": "5  IPC and locking",
    "section": "6.3 Shared Memory Model",
    "text": "6.3 Shared Memory Model\n┌─────────────────────────────┐    ┌─────────────────────────────┐\n│                             │    │                             │\n│ ┌──────────┐   ┌──────────┐ │    │ ┌──────────┐   ┌──────────┐ │\n│ │          │   │          │ │    │ │          │   │          │ │\n│ │  CORE 1  │   │  CORE 2  │ │    │ │  CORE 3  │   │  CORE 4  │ │\n│ │          │   │          │ │    │ │          │   │          │ │\n│ └─┬──┬─────┘   └────┬─────┘ │    │ └┬─────────┘   └──────┬───┘ │\n│   │  │              │       │    │  │                    │     │\n│   │  │              │       │    │  │                    │     │\n│   │  │  CPU 1       │       │    │  │      CPU 2         │     │\n│   │  │              │       │    │  │                    │     │\n└───┼──┼──────────────┼───────┘    └──┼────────────────────┼─────┘\n    │  │              │               │                    │\n    │  │              └────────────┐  │                    │\n    │  │                           │  │                    │\n    │  └─────────────────────────┐ │  │                    │\n    │                            │ │  │  ┌─────────────────┘\n    └──────────────────────────┐ │ │  │  │\n                               │ │ │  │  │\n┌──────────────────────────────┼─┼─┼──┼──┼──────────────────────┐\n│                              │ │ │  │  │                      │\n│ ┌─────┐  ┌─────┐  ┌─────┐  ┌─▼─▼─▼──▼──▼─┐                    │\n│ │     │  │     │  │     │  │Shared Memory│                    │\n│ └─────┘  └─────┘  └─────┘  └─────────────┘                    │\n│                                      Main Memory              │\n└───────────────────────────────────────────────────────────────┘",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#aside-memory-models",
    "href": "Courses/4_IPC_and_Locking.html#aside-memory-models",
    "title": "5  IPC and locking",
    "section": "6.4 Aside : memory models",
    "text": "6.4 Aside : memory models\n\n\n\n\n\n\n\n\nUMA\n\n\n\n\n\n \n\n\n\n\nNUMA\n\n\n\n\n\n\n\nThere are differents models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#shared-fifos-queues",
    "href": "Courses/4_IPC_and_Locking.html#shared-fifos-queues",
    "title": "5  IPC and locking",
    "section": "6.5 Shared FIFOs : Queues",
    "text": "6.5 Shared FIFOs : Queues\nAn ubiquitous tool in multiprocessing (and distributed computing) is shared memory FIFO list, aka Queues.\nA FIFO is a :\n\nLinked list\nwith FIFO (First In First Out) semantics, with enqueue(x) et dequeue() function (or push(x)/pop())\n\n\n\n\n\n\nIn the context of multi-processing (or multi-threading) :\nShared Memory + FIFO list = Queue\nQueues are the basis of the consumer/producer model, which is widely used in concurrent and distributed applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#when-to-use-queues",
    "href": "Courses/4_IPC_and_Locking.html#when-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.6 When to use queues?",
    "text": "6.6 When to use queues?\nAn algorithm with two computations A and B where :\n\nB depends on the result of A\nA is independent of B\n\n. . .\nA could be a producer for B, and B a consumer for A.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#how-to-use-queues",
    "href": "Courses/4_IPC_and_Locking.html#how-to-use-queues",
    "title": "5  IPC and locking",
    "section": "6.7 How to use queues?",
    "text": "6.7 How to use queues?\n┌───────────┐\n│           │\n│ Producer  │\n│           │ Process A\n│           │\n└─────┬─────┘\n      │\n ┌────┼───────────────────────────────────────────────────────────────────┐\n │    │                         Queue                                     │\n │    │        ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┐                │\n │    │        │     │     │     │     │     │     │     │                │\n │    └───────►│     │     │     │     │     │     │     ├──────────┐     │\n │             │     │     │     │     │     │     │     │          │     │\n │             └─────┴─────┴─────┴─────┴─────┴─────┴─────┘          │     │\n │                                                                  │     │\n │        Shared Memory                                             │     │\n └──────────────────────────────────────────────────────────────────┼─────┘\n                                                                    │\n                                                                    ▼\n                                                              ┌───────────┐\n                                                              │           │\n                                                   Process B  │ Consumer  │\n                                                              │           │\n                                                              │           │\n                                                              └───────────┘",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#producerconsumer-examples",
    "href": "Courses/4_IPC_and_Locking.html#producerconsumer-examples",
    "title": "5  IPC and locking",
    "section": "6.8 Producer/consumer, Examples",
    "text": "6.8 Producer/consumer, Examples\n\nA finds primes in a list of number, B formats and prints them every 10 numbers found.\nA fetches a bunch of images on the web, B downloads them and saves them to disk.\nA takes the orders in the restaurant, B cooks them.\n\n. . .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#the-main-gotcha",
    "href": "Courses/4_IPC_and_Locking.html#the-main-gotcha",
    "title": "5  IPC and locking",
    "section": "7.1 The main gotcha",
    "text": "7.1 The main gotcha\nwhat if several processes want to write/read the same shared memory portions at the same time?\n. . .\nEnter the realm of the dreaded race condition",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#simple-example",
    "href": "Courses/4_IPC_and_Locking.html#simple-example",
    "title": "5  IPC and locking",
    "section": "7.2 Simple example",
    "text": "7.2 Simple example\nPrinting from several processes a string with 10 times the same char.\n\n\nfrom multiprocessing.pool import Pool\nfrom itertools import repeat\n# print \"AAAAAAAAA\", \"BBBBBBBBBBB\" etc.\ndef repeat10Cap(c): \n    print(\"\".join(repeat(chr(c+65),10))) \nwith Pool(8) as pool:\n    pool.map(repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAACCCCCCCCCCBBBBBBBBBBDDDDDDDDDDEEEEEEEEEE\n\n\nFFFFFFFFFFGGGGGGGGGG\nIIIIIIIIII\n\nHHHHHHHHHH\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#critical-section-workflow",
    "href": "Courses/4_IPC_and_Locking.html#critical-section-workflow",
    "title": "5  IPC and locking",
    "section": "8.1 Critical section workflow",
    "text": "8.1 Critical section workflow\n\n\nThree processes with critical section",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#a-simple-implementation-in-python-lock",
    "href": "Courses/4_IPC_and_Locking.html#a-simple-implementation-in-python-lock",
    "title": "5  IPC and locking",
    "section": "8.2 A simple implementation in Python : Lock",
    "text": "8.2 A simple implementation in Python : Lock\n\n\nfrom multiprocessing.pool import Pool\nfrom multiprocessing import Lock\nfrom itertools import repeat\nlock = Lock()\ndef safe_repeat10Cap(c):\n    with lock: \n        # Beginning of critical section\n        print(\"\".join(repeat(chr(c+65),10)))\n        # End of critical section\nwith Pool(8) as pool:\n    pool.map(safe_repeat10Cap, range(10))\n\n\nOutput:\nAAAAAAAAAA\nBBBBBBBBBB\nCCCCCCCCCC\nDDDDDDDDDD\nEEEEEEEEEE\nFFFFFFFFFF\nGGGGGGGGGG\nHHHHHHHHHH\nIIIIIIIIII\nJJJJJJJJJJ",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#consistency-problems-with-fifo-example-i",
    "href": "Courses/4_IPC_and_Locking.html#consistency-problems-with-fifo-example-i",
    "title": "5  IPC and locking",
    "section": "9.1 Consistency problems with FIFO example I",
    "text": "9.1 Consistency problems with FIFO example I\nProcess A (resp. B) wants to push x (resp. y) on the list.\n\n\n\\Longrightarrow Consistency problem if they both create a new linked node to node 3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#consistency-problems-with-fifo-example-2",
    "href": "Courses/4_IPC_and_Locking.html#consistency-problems-with-fifo-example-2",
    "title": "5  IPC and locking",
    "section": "9.2 Consistency problems with FIFO example 2",
    "text": "9.2 Consistency problems with FIFO example 2\nProcess A and B both want to pop the list.\n\n\n\\Longrightarrow Consistency problem if they both pop the same node.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#no-consistency-problems-with-fifo-example-3",
    "href": "Courses/4_IPC_and_Locking.html#no-consistency-problems-with-fifo-example-3",
    "title": "5  IPC and locking",
    "section": "9.3 (No) Consistency problems with FIFO example 3",
    "text": "9.3 (No) Consistency problems with FIFO example 3\n\n\nNo problem there.\n\n\n\n. . .\n\n\n\n\n\n\nWarning\n\n\n\n⚠ ⚠ As long the list is not empty ⚠ ⚠",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#deadlock-example",
    "href": "Courses/4_IPC_and_Locking.html#deadlock-example",
    "title": "5  IPC and locking",
    "section": "10.1 Deadlock example",
    "text": "10.1 Deadlock example",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#deadlock-serious-example",
    "href": "Courses/4_IPC_and_Locking.html#deadlock-serious-example",
    "title": "5  IPC and locking",
    "section": "10.2 Deadlock (serious) example",
    "text": "10.2 Deadlock (serious) example\n\n\nDeadlock illustration\n\n\n\n\nProcess A acquires lock L1. Process B acquires lock L2. Process A tries to acquire lock L2, but it is already held by B. Process B tries to acquire lock L1, but it is already held by A. Both processes are blocked.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/4_IPC_and_Locking.html#avoiding-deadlocks",
    "href": "Courses/4_IPC_and_Locking.html#avoiding-deadlocks",
    "title": "5  IPC and locking",
    "section": "10.3 Avoiding Deadlocks",
    "text": "10.3 Avoiding Deadlocks\nThere is several ways to avoid deadlocks. One of them is the Dijkstra’s Resource Hiearchy Solution.\n. . .\nIn the previous example, processes should try the lowest numbered locks first. Instead of B acquiring L2 first, it should tries to acquire L1 instead and L2 after.\n. . .\nThis solution isn’t universal but is pretty usable in general case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>IPC and locking</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html",
    "href": "Courses/5_0_Distributed.html",
    "title": "6  Distributed Computing models",
    "section": "",
    "text": "7 Map-Reduce",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#the-real-beating-heart-of-big-data",
    "href": "Courses/5_0_Distributed.html#the-real-beating-heart-of-big-data",
    "title": "6  Distributed Computing models",
    "section": "7.1 The (real) beating Heart of Big Data",
    "text": "7.1 The (real) beating Heart of Big Data\nMap\\rightarrow{}Reduce patern is the most common pattern to process data in (real) Big Data.\n. . .\nIt is heavily used by Google, Facebook, and IBM.\n. . .\nHadoop from Apache is a popular Map-Reduce framework (also called MapReduce in the Hadoop framework, not to be confused with the more general Map\\rightarrow{}Reduce Pattern).\n. . .\nHadoop is backed by a HDFS (Hadoop Distributed File System) and a YARN (Yet Another Resource Manager)\n\nHDFS is a distributed file system (a file system that is distributed across a cluster of computers)\nYARN is a resource manager (a program that manages the resources of a cluster)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#split-apply-combine-pattern",
    "href": "Courses/5_0_Distributed.html#split-apply-combine-pattern",
    "title": "6  Distributed Computing models",
    "section": "7.2 Split-Apply-Combine pattern",
    "text": "7.2 Split-Apply-Combine pattern\n\n\n\n\nSplit:\n\nSplit the data into smaller pieces\n\nApply:\n\nProcess the data in the pieces\n\nCombine:\n\nMerge the results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#map",
    "href": "Courses/5_0_Distributed.html#map",
    "title": "6  Distributed Computing models",
    "section": "7.3 Map",
    "text": "7.3 Map\nMap takes one pair of data with a type in one data domain, and returns a list of pairs in a different domain:\nMap(k1,v1) → list(k2,v2)\n\\Longrightarrow heavily parallelized",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#reduce",
    "href": "Courses/5_0_Distributed.html#reduce",
    "title": "6  Distributed Computing models",
    "section": "7.4 Reduce",
    "text": "7.4 Reduce\nThe values associated from the same key are combined.\nThe Reduce function is then applied in parallel to each group, which in turn produces a collection of values in the same domain:\nReduce(k2, list (v2)) → list((k3, v3))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#schema",
    "href": "Courses/5_0_Distributed.html#schema",
    "title": "6  Distributed Computing models",
    "section": "7.5 Schema",
    "text": "7.5 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#canonical-example-word-count-i",
    "href": "Courses/5_0_Distributed.html#canonical-example-word-count-i",
    "title": "6  Distributed Computing models",
    "section": "7.6 Canonical example : Word Count, I",
    "text": "7.6 Canonical example : Word Count, I\nThe canonical MapReduce example counts the appearance of each word in a set of documents\ndef map(name, document):\n  // name: document name\n  // document: document contents (list of words)\n  for word in document:\n    emit (word, 1)\n\ndef reduce(word, partialCounts):\n  // word: a word\n  // partialCounts: a list of aggregated partial counts\n  sum = 0\n  for pc in partialCounts:\n    sum += pc\n  emit (word, sum)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#canonical-example-word-count-ii",
    "href": "Courses/5_0_Distributed.html#canonical-example-word-count-ii",
    "title": "6  Distributed Computing models",
    "section": "7.7 Canonical example : Word Count, II",
    "text": "7.7 Canonical example : Word Count, II",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#spark-spiritual-son-of-mapreduce",
    "href": "Courses/5_0_Distributed.html#spark-spiritual-son-of-mapreduce",
    "title": "6  Distributed Computing models",
    "section": "7.8 Spark, spiritual son of MapReduce",
    "text": "7.8 Spark, spiritual son of MapReduce\nSpark is widely used for machine learning on scalable data sets (faster than MapReduce by an order of magnitude).\n. . .\nSpark is largely inspired by the MapReduce pattern but extends it by using a distributed graph rather than a “linear” data flow like Map\\rightarrow{}Reduce.\n\\Longrightarrow Complex disbributed computing.\n. . .\nSpark emphasizes ease of use of the cluster ressources in a simple and functional way",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#spark-code-example-word-count",
    "href": "Courses/5_0_Distributed.html#spark-code-example-word-count",
    "title": "6  Distributed Computing models",
    "section": "7.9 Spark, code example : Word Count",
    "text": "7.9 Spark, code example : Word Count\ntext_file = sc.textFile(\"hdfs://...\")\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b)\ncounts.saveAsTextFile(\"hdfs://...\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#spark-another-example-machine-learning",
    "href": "Courses/5_0_Distributed.html#spark-another-example-machine-learning",
    "title": "6  Distributed Computing models",
    "section": "7.10 Spark, another example : machine learning",
    "text": "7.10 Spark, another example : machine learning\n# Every record of this DataFrame contains the label and\n# features represented by a vector.\ndf = sqlContext.createDataFrame(data, [\"label\", \"features\"])\n\n# Set parameters for the algorithm.\n# Here, we limit the number of iterations to 10.\nlr = LogisticRegression(maxIter=10)\n\n# Fit the model to the data.\nmodel = lr.fit(df)\n\n# Given a dataset, predict each point's label, \n# and show the results.\nmodel.transform(df).show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#schema-1",
    "href": "Courses/5_0_Distributed.html#schema-1",
    "title": "6  Distributed Computing models",
    "section": "8.1 Schema",
    "text": "8.1 Schema",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_0_Distributed.html#main-message-passing-functions",
    "href": "Courses/5_0_Distributed.html#main-message-passing-functions",
    "title": "6  Distributed Computing models",
    "section": "8.2 Main message-passing functions",
    "text": "8.2 Main message-passing functions\n\n\nScatter\n\npartition the data into smaller pieces and send them to the different processes\n\nGather\n\ncollect the data from the different processes and merge them.\n\nBroadcast\n\nSend the same data to all the processes.\n\nReduce\n\nMerge the data from all the processes and produce a single result.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributed Computing models</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html",
    "href": "Courses/5_1_Dask_delayed.html",
    "title": "7  Dask delayed",
    "section": "",
    "text": "8 Dask Delayed\nSometimes problems don't fit into one of the collections like dask.array or dask.dataframe. In these cases, users can parallelize custom algorithms using the simpler dask.delayed interface. This allows you to create graphs directly with a light annotation of normal python code:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#dask-delayed-1",
    "href": "Courses/5_1_Dask_delayed.html#dask-delayed-1",
    "title": "7  Dask delayed",
    "section": "Dask Delayed",
    "text": "Dask Delayed\n\n\nA Dask Delayed task graph with two \"inc\" functions combined using an \"add\" function resulting in an output node.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example",
    "href": "Courses/5_1_Dask_delayed.html#example",
    "title": "7  Dask delayed",
    "section": "8.1 Example",
    "text": "8.1 Example\nVisit https://examples.dask.org/delayed.html to see and run examples using Dask Delayed.\nSometimes we face problems that are parallelizable, but don't fit into high-level abstractions like Dask Array or Dask DataFrame. Consider the following example:\ndef inc(x):\n    return x + 1\n\ndef double(x):\n    return x * 2\n\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = sum(output)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example-1",
    "href": "Courses/5_1_Dask_delayed.html#example-1",
    "title": "7  Dask delayed",
    "section": "Example",
    "text": "Example\nThere is clearly parallelism in this problem (many of the inc, double, and add functions can be evaluated independently), but it's not clear how to convert this to an array or DataFrame computation. As written, this code runs sequentially in a single thread. However, we see that a lot of this could be executed in parallel.\nThe Dask delayed function decorates your functions so that they operate lazily. Rather than executing your function immediately, it will defer execution, placing the function and its arguments into a task graph.\n\ndask.delayed\n\n\ndelayed",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example-2",
    "href": "Courses/5_1_Dask_delayed.html#example-2",
    "title": "7  Dask delayed",
    "section": "Example",
    "text": "Example\nWe slightly modify our code by wrapping functions in delayed. This delays the execution of the function and generates a Dask graph instead:\nimport dask\n\noutput = []\nfor x in data:\n    a = dask.delayed(inc)(x)\n    b = dask.delayed(double)(x)\n    c = dask.delayed(add)(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example-3",
    "href": "Courses/5_1_Dask_delayed.html#example-3",
    "title": "7  Dask delayed",
    "section": "Example",
    "text": "Example\nWe used the dask.delayed function to wrap the function calls that we want to turn into tasks. None of the inc, double, add, or sum calls have happened yet. Instead, the object total is a Delayed result that contains a task graph of the entire computation. Looking at the graph we see clear opportunities for parallel execution. The Dask schedulers &lt;scheduling&gt; will exploit this parallelism, generally improving performance (although not in this example, because these functions are already very small and fast.)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example-4",
    "href": "Courses/5_1_Dask_delayed.html#example-4",
    "title": "7  Dask delayed",
    "section": "Example",
    "text": "Example\ntotal.visualize()  # see image to the right\n\n\nA task graph with many nodes for \"inc\" and \"double\" that combine with \"add\" nodes. The output of the \"add\" nodes finally aggregate with a \"sum\" node.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#example-5",
    "href": "Courses/5_1_Dask_delayed.html#example-5",
    "title": "7  Dask delayed",
    "section": "Example",
    "text": "Example\nWe can now compute this lazy result to execute the graph in parallel:\n&gt;&gt;&gt; total.compute()\n45",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#decorator",
    "href": "Courses/5_1_Dask_delayed.html#decorator",
    "title": "7  Dask delayed",
    "section": "8.2 Decorator",
    "text": "8.2 Decorator\nIt is also common to see the delayed function used as a decorator. Here is a reproduction of our original problem as a parallel code:\nimport dask\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef double(x):\n    return x * 2\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\nfor x in data:\n    a = inc(x)\n    b = double(x)\n    c = add(a, b)\n    output.append(c)\n\ntotal = dask.delayed(sum)(output)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#real-time",
    "href": "Courses/5_1_Dask_delayed.html#real-time",
    "title": "7  Dask delayed",
    "section": "8.3 Real time",
    "text": "8.3 Real time\nSometimes you want to create and destroy work during execution, launch tasks from other tasks, etc. For this, see the Futures &lt;futures&gt; interface.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#best-practices",
    "href": "Courses/5_1_Dask_delayed.html#best-practices",
    "title": "7  Dask delayed",
    "section": "8.4 Best Practices",
    "text": "8.4 Best Practices\nFor a list of common problems and recommendations see Delayed Best Practices &lt;delayed-best-practices&gt;.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#indirect-dependencies",
    "href": "Courses/5_1_Dask_delayed.html#indirect-dependencies",
    "title": "7  Dask delayed",
    "section": "8.5 Indirect Dependencies",
    "text": "8.5 Indirect Dependencies\nSometimes you might find yourself wanting to add a dependency to a task that does not take the result of that dependency as an input. For example when a task depends on the side-effect of another task. In these cases you can use dask.graph_manipulation.bind.\nimport dask\nfrom dask.graph_manipulation import bind\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef add_data(x):\n    DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n    return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = bind(sum_data, [b, d])(e)\nf.compute()\nsum_data will operate on DATA only after both the expected items have been appended to it. bind can also be used along with direct dependencies passed through the function arguments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#call-delayed-on-the-function-not-the-result",
    "href": "Courses/5_1_Dask_delayed.html#call-delayed-on-the-function-not-the-result",
    "title": "7  Dask delayed",
    "section": "9.1 Call delayed on the function, not the result",
    "text": "9.1 Call delayed on the function, not the result\nDask delayed operates on functions like dask.delayed(f)(x, y), not on their results like dask.delayed(f(x, y)). When you do the latter, Python first calculates f(x, y) before Dask has a chance to step in.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# This executes immediately\n\ndask.delayed(f(x, y))\n# This ma\nkes a delayed function, acting lazily\n\ndask.delayed(f)(x, y)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#compute-on-lots-of-computation-at-once",
    "href": "Courses/5_1_Dask_delayed.html#compute-on-lots-of-computation-at-once",
    "title": "7  Dask delayed",
    "section": "9.2 Compute on lots of computation at once",
    "text": "9.2 Compute on lots of computation at once\nTo improve parallelism, you want to include lots of computation in each compute call. Ideally, you want to make many dask.delayed calls to define your computation and then call dask.compute only at the end. It is ok to call dask.compute in the middle of your computation as well, but everything will stop there as Dask computes those results before moving forward with your code.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Avoid calling compute repeatedly\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y.compute())\n\nresults\n# Collec\nt many calls for one compute\n\nresults = []\nfor x in L:\n    y = dask.delayed(f)(x)\n    results.append(y)\n\nresu\nlts = dask.compute(*results)\n\n\n\nCalling y.compute() within the loop would await the result of the computation every time, and so inhibit parallelism.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#dont-mutate-inputs",
    "href": "Courses/5_1_Dask_delayed.html#dont-mutate-inputs",
    "title": "7  Dask delayed",
    "section": "9.3 Don't mutate inputs",
    "text": "9.3 Don't mutate inputs\nYour functions should not change the inputs directly.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Mutate inputs in functions\n\n@dask.delayed\ndef f(x):\n    x += 1\n    return x\n# Return new values or copies\n\n@dask.delayed\ndef f(x):\n    x = x + 1\n    return x\n\n\n\nIf you need to use a mutable operation, then make a copy within your function first:\n@dask.delayed\ndef f(x):\n    x = copy(x)\n    x += 1\n    return x",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#avoid-global-state",
    "href": "Courses/5_1_Dask_delayed.html#avoid-global-state",
    "title": "7  Dask delayed",
    "section": "9.4 Avoid global state",
    "text": "9.4 Avoid global state\nIdeally, your operations shouldn't rely on global state. Using global state might work if you only use threads, but when you move to multiprocessing or distributed computing then you will likely encounter confusing errors.\n\n\n\n\n\n\nDon't\n\n\nL = []\n\n# This references global variable L\n\n@dask.delayed\ndef f(x):\n    L.append(x)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#dont-rely-on-side-effects",
    "href": "Courses/5_1_Dask_delayed.html#dont-rely-on-side-effects",
    "title": "7  Dask delayed",
    "section": "9.5 Don't rely on side effects",
    "text": "9.5 Don't rely on side effects\nDelayed functions only do something if they are computed. You will always need to pass the output to something that eventually calls compute.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Forget to call compute\n\ndask.delayed(f)(1, 2, 3)\n\n...\n# Ensure delayed tasks are computed\n\nx = dask.delayed(f)(1, 2, 3)\n...\ndask.compute(x, ...)\n\n\n\nIn the first case here, nothing happens, because compute() is never called.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#break-up-computations-into-many-pieces",
    "href": "Courses/5_1_Dask_delayed.html#break-up-computations-into-many-pieces",
    "title": "7  Dask delayed",
    "section": "9.6 Break up computations into many pieces",
    "text": "9.6 Break up computations into many pieces\nEvery dask.delayed function call is a single operation from Dask's perspective. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with @dask.delayed and parallelize that code internally. To accomplish that, it needs your help to find good places to break up a computation.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# One giant task\n\n\ndef load(filename):\n    ...\n\n\ndef process(data):\n    ...\n\n\ndef save(data):\n    ...\n\n@dask.delayed\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n# Break up into many tasks\n\n@dask.delayed\ndef load(filename):\n    ...\n\n@dask.delayed\ndef process(data):\n    ...\n\n@dask.delayed\ndef save(data):\n    ...\n\n\ndef f(filenames):\n    results = []\n    for filename in filenames:\n        data = load(filename)\n        data = process(data)\n        result = save(data)\n        results.append(result)\n\n    return results\n\ndask.compute(f(filenames))\n\n\n\nThe first version only has one delayed task, and so cannot parallelize.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#avoid-too-many-tasks",
    "href": "Courses/5_1_Dask_delayed.html#avoid-too-many-tasks",
    "title": "7  Dask delayed",
    "section": "9.7 Avoid too many tasks",
    "text": "9.7 Avoid too many tasks\nEvery delayed task has an overhead of a few hundred microseconds. Usually this is ok, but it can become a problem if you apply dask.delayed too finely. In this case, it's often best to break up your many tasks into batches or use one of the Dask collections to help you.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Too many tasks\n\nresults = []\nfo\nr x in range(10000000):\n\n y = dask.delayed(f)(x)\n    results.append(y)\n# Use collections\n\nimport dask.bag as db\nb = db.from_s\nequence(range(10000000), npartitions=1000)\nb = b.map(f)\n...",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#avoid-too-many-tasks-1",
    "href": "Courses/5_1_Dask_delayed.html#avoid-too-many-tasks-1",
    "title": "7  Dask delayed",
    "section": "Avoid too many tasks",
    "text": "Avoid too many tasks\nHere we use dask.bag to automatically batch applying our function. We could also have constructed our own batching as follows\ndef batch(seq):\n    sub_results = []\n    for x in seq:\n        sub_results.append(f(x))\n    return sub_results\n\n batches = []\n for i in range(0, 10000000, 10000):\n     result_batch = dask.delayed(batch)(range(i, i + 10000))\n     batches.append(result_batch)\nHere we construct batches where each delayed function call computes for many data points from the original input.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#avoid-calling-delayed-within-delayed-functions",
    "href": "Courses/5_1_Dask_delayed.html#avoid-calling-delayed-within-delayed-functions",
    "title": "7  Dask delayed",
    "section": "9.8 Avoid calling delayed within delayed functions",
    "text": "9.8 Avoid calling delayed within delayed functions\nOften, if you are new to using Dask delayed, you place dask.delayed calls everywhere and hope for the best. While this may actually work, it's usually slow and results in hard-to-understand solutions.\nUsually you never call dask.delayed within dask.delayed functions.\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Delayed function calls delayed\n\n@dask.delayed\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n# Normal function calls delayed\n\n\ndef process_all(L):\n    result = []\n    for x in L:\n        y = dask.delayed(f)(x)\n        result.append(y)\n    return result\n\n\n\nBecause the normal function only does delayed work it is very fast and so there is no reason to delay it.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "href": "Courses/5_1_Dask_delayed.html#dont-call-dask.delayed-on-other-dask-collections",
    "title": "7  Dask delayed",
    "section": "9.9 Don't call dask.delayed on other Dask collections",
    "text": "9.9 Don't call dask.delayed on other Dask collections\nWhen you place a Dask array or Dask DataFrame into a delayed call, that function will receive the NumPy or Pandas equivalent. Beware that if your array is large, then this might crash your workers.\nInstead, it's more common to use methods like da.map_blocks\n\n\n\n\n\n\n\nDon't\nDo\n\n\n# Call del\nayed functions on Dask collections\n\nimport dask.dataframe as dd\ndf = dd.read_csv('/path/to/*.csv')\n\ndask.delayed(train)(df)\n# Us\ne mapping methods if applicable\n\nimport dask.dataframe as dd\ndf\n= dd.read_csv('/path/to/*.csv')\n\ndf.map_partitions(train)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "href": "Courses/5_1_Dask_delayed.html#dont-call-dask.delayed-on-other-dask-collections-1",
    "title": "7  Dask delayed",
    "section": "Don't call dask.delayed on other Dask collections",
    "text": "Don't call dask.delayed on other Dask collections\nAlternatively, if the procedure doesn't fit into a mapping, you can always turn your arrays or dataframes into many delayed objects, for example\npartitions = df.to_delayed()\ndelayed_values = [dask.delayed(train)(part)\n                  for part in partitions]\nHowever, if you don't mind turning your Dask array/DataFrame into a single chunk, then this is ok.\ndask.delayed(train)(..., y=df.sum())",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "href": "Courses/5_1_Dask_delayed.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls",
    "title": "7  Dask delayed",
    "section": "9.10 Avoid repeatedly putting large inputs into delayed calls",
    "text": "9.10 Avoid repeatedly putting large inputs into delayed calls\nEvery time you pass a concrete result (anything that isn't delayed) Dask will hash it by default to give it a name. This is fairly fast (around 500 MB/s) but can be slow if you do it over and over again. Instead, it is better to delay your data as well.\nThis is especially important when using a distributed cluster to avoid sending your data separately for each function call.\n\n\n\n\n\n\n\nDon't\nDo\n\n\nx = np.arr\nay(...)  # some large array\n\nresults =\n [dask.delayed(train)(x, i)\n\n      for i in range(1000)]\nx\n = np.array(...)    # some large array\nx =\ndask.delayed(x)  # delay the data once\nresults = [dask.delayed(train)(x, i)\n           for i in range(1000)]\n\n\n\nEvery call to dask.delayed(train)(x, ...) has to hash the NumPy array x, which slows things down.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#working-with-collections-1",
    "href": "Courses/5_1_Dask_delayed.html#working-with-collections-1",
    "title": "7  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nAs an example, consider the case where we store tabular data in a custom format not known by Dask DataFrame. This format is naturally broken apart into pieces and we have a function that reads one piece into a Pandas DataFrame. We use dask.delayed to lazily read these files into Pandas DataFrames, use dd.from_delayed to wrap these pieces up into a single Dask DataFrame, use the complex algorithms within the DataFrame (groupby, join, etc.), and then switch back to dask.delayed to save our results back to the custom format:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#working-with-collections-2",
    "href": "Courses/5_1_Dask_delayed.html#working-with-collections-2",
    "title": "7  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\nfrom my_custom_library import load, save\n\nfilenames = ...\ndfs = [delayed(load)(fn) for fn in filenames]\n\ndf = dd.from_delayed(dfs)\ndf = ... # do work with dask.dataframe\n\ndfs = df.to_delayed()\nwrites = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]\n\ndd.compute(*writes)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/5_1_Dask_delayed.html#working-with-collections-3",
    "href": "Courses/5_1_Dask_delayed.html#working-with-collections-3",
    "title": "7  Dask delayed",
    "section": "Working with Collections",
    "text": "Working with Collections\nData science is often complex, and dask.delayed provides a release valve for users to manage this complexity on their own, and solve the last mile problem for custom formats and complex situations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Dask `delayed`</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html",
    "href": "Courses/6_SIMD.html",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "",
    "text": "9 Generalities on SIMD",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#why-vectorization",
    "href": "Courses/6_SIMD.html#why-vectorization",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "9.1 Why vectorization ?",
    "text": "9.1 Why vectorization ?\n\n\nUbiquitousness\n\nAlmost every CPU made on the market since 2010 got a SIMD unit and it operates on multiple elements at the same time on a single instruction.\n\nNatural congruence with parallel progamming\n\nIn a good number of cases, parallelization of algorithm is often achieved by vectorization and the use of SIMD is then a free bonus.\n\nPerformance boost almost guaranteed\n\nWith a little effort it could boost your performance by a factor of two or more. It is even more energy efficient than raw CPU (non SIMD) computing(Inoue 2016).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#scalar-vs-vector-operation",
    "href": "Courses/6_SIMD.html#scalar-vs-vector-operation",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "9.2 Scalar vs Vector operation",
    "text": "9.2 Scalar vs Vector operation\n\n\n\n\n\nA scalar operation does a single double-precision addition in one cycle. It takes eight cycles to process a 64-byte cache line. In comparison, a vector operation on a 512-bit vector unit can process all eight double-precision values in one cycle.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#terminology",
    "href": "Courses/6_SIMD.html#terminology",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "9.3 Terminology",
    "text": "9.3 Terminology\n\nVector (SIMD) lane\n\nA pathway through a vector operation on vector registers for a single data element much like a lane on a multi-lane freeway.\n\nVector width\n\nThe width of the vector unit, usually expressed in bits.\n\nVector length\n\nThe number of data elements that can be processed by the vector in one operation.\n\nVector (SIMD) instruction sets\n\nThe set of instructions that extend the regular scalar processor instructions to utilize the vector processor.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#hardwaresoftware-requirements",
    "href": "Courses/6_SIMD.html#hardwaresoftware-requirements",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "9.4 Hardware/Software Requirements",
    "text": "9.4 Hardware/Software Requirements\n\nGeneration of instructions\n\nvector instructions are generated by the compiler OR manually by the programmer via the “intrisics” (explicit SIMD instructions)\n\nMatching of instructions\n\nmatching of the instructions and the hardware, because there is several units and instructions sets. (the compiler does the matching, most of the time).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#evolution-of-the-vector-length",
    "href": "Courses/6_SIMD.html#evolution-of-the-vector-length",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "10.1 Evolution of the vector length",
    "text": "10.1 Evolution of the vector length",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#versions-i",
    "href": "Courses/6_SIMD.html#versions-i",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "10.2 Versions, I",
    "text": "10.2 Versions, I\n\n\n\nRelease\nFunctionality\n\n\n\n\nMMX (trademark with no official meaning)\nTargeted towards the graphics market, but GPUs soon took over this function. Vector units shifted their focus to computation rather than graphics. AMD released its version under the name 3DNow! with single-precision support.\n\n\nSSE (Streaming SIMD Extensions)\nFirst Intel vector unit to offer floating-point operations with single-precision support\n\n\nSSE2\nDouble-precision support added",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#versions-ii",
    "href": "Courses/6_SIMD.html#versions-ii",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "10.3 Versions, II",
    "text": "10.3 Versions, II\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX (Advanced Vector Extensions)\nTwice the vector length. AMD added a fused multiply-add FMA vector instruction in its competing hardware, effectively doubling the performance for some loops.\n\n\nAVX2\nIntel added a fused multiply-add (FMA) to its vector processor.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#versions-iii",
    "href": "Courses/6_SIMD.html#versions-iii",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "10.4 Versions, III",
    "text": "10.4 Versions, III\n\n\n\nRelease\nFunctionality\n\n\n\n\nAVX512\nFirst offered on the Knights Landing processor; it came to the main-line multi-core processor hardware lineup in 2017.From the years 2018 and on, Intel and AMD (Advanced Micro Devices, Inc.) have created multiple variants of AVX512 as incremental improvements to vector hardware architectures.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#vectors-types-avx",
    "href": "Courses/6_SIMD.html#vectors-types-avx",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "11.1 Vectors types (AVX)",
    "text": "11.1 Vectors types (AVX)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#encoding-scheme",
    "href": "Courses/6_SIMD.html#encoding-scheme",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "11.2 Encoding scheme",
    "text": "11.2 Encoding scheme\n_mm256_{operation}{non-alignement}_{dataorganization}{datatype}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#add-example",
    "href": "Courses/6_SIMD.html#add-example",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "11.3 Add example",
    "text": "11.3 Add example",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#various-i",
    "href": "Courses/6_SIMD.html#various-i",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "11.4 Various, I",
    "text": "11.4 Various, I\n\ns (single): single precision float (32bits)\nd (double): double precision float (64bits)\ni… (integer): integer\np (packed): contiguous, operates on the whole vector\ns (scalar): operates on a single element",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/6_SIMD.html#various-ii",
    "href": "Courses/6_SIMD.html#various-ii",
    "title": "8  Hardware Vectorization with SIMD",
    "section": "11.5 Various, II",
    "text": "11.5 Various, II\n\nu (unaligned): data non aligned in memory\nl (low): least significant bits\nh (high): most significant bits\nr (reversed): reversed order",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Hardware Vectorization with SIMD</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html",
    "href": "Courses/7_GPU_Caching_etc.html",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "",
    "text": "10 GPU Computing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#gpu",
    "href": "Courses/7_GPU_Caching_etc.html#gpu",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.1 GPU ?",
    "text": "10.1 GPU ?\nGPU = graphical process unit\n\n“(…) a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.” (Wikipedia)\n\nOriginal purpose: image creation and manipulation for graphical rendering (video games, video edition, 3D conception, etc.)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#gpu-for-computing",
    "href": "Courses/7_GPU_Caching_etc.html#gpu-for-computing",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.2 GPU for computing ?",
    "text": "10.2 GPU for computing ?\n\n“Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel.” (Wikipedia)\n\nNowadays usage: more general massive computations based on matrix operations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#von-neumann-architecture",
    "href": "Courses/7_GPU_Caching_etc.html#von-neumann-architecture",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.3 Von Neumann architecture",
    "text": "10.3 Von Neumann architecture\nBasic principle (present in most modern computing units) with three interconnected parts:\n\ncontrol unit: does most of the work, such as deciding what operation to do next, fetching the next instruction from memory, etc.\narithmetic/logic unit (ALU): implements operations such as addition, subtraction, multiplication, etc.\nmemory unit: stores the data processed by the chip, such as its inputs, outputs, and any intermediate data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#von-neumann-architecture-schema",
    "href": "Courses/7_GPU_Caching_etc.html#von-neumann-architecture-schema",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.4 Von Neumann architecture, schema",
    "text": "10.4 Von Neumann architecture, schema\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#gpu-architecture",
    "href": "Courses/7_GPU_Caching_etc.html#gpu-architecture",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.5 GPU architecture",
    "text": "10.5 GPU architecture\n\n\nYgor Serpa (TowardsDataScience)\n\nPrinciple: control units are large and expensive, while arithmetic units are more straightforward and cheaper\nDesign: a sub-processor = one control unity commanding several ALUs (= GPU cores) to operate on larger chunks of data\nLimitation: all ALUs (connected to the same control unity) must obey the same commands (i.e. do the same thing at the same moment)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#gpu-trades-control-for-arithmetic-power",
    "href": "Courses/7_GPU_Caching_etc.html#gpu-trades-control-for-arithmetic-power",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.6 GPU trades “control” for “arithmetic power”",
    "text": "10.6 GPU trades “control” for “arithmetic power”\n\nALU blocks (= gpu sub-processors) not as versatile as CPU cores but can operate over large amounts of data in batches\nCentral control unit syncs all the sub-processors (each one can do a different task) but the same set of ALUs cannot do different things in parallel (e.g. if statements costly for GPUs)\n\nExample: each block of 16 ALUs is limited to processing the same instruction over 16 pairs of operands",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput",
    "href": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.7 Latency vs Throughput",
    "text": "10.7 Latency vs Throughput\n\nLatency: how long does it take to finish a given task\nThroughput: how many times can you complete the task within a period\n\nCPU cores optimized for latency: to finish a task as fast as possible.\n\\rightarrow scales with more complex operations but not with larger data\nGPU cores optimized for throughput: individually slower, but they operate on bulks of data at once.\n\\rightarrow scales with larger data but not with more complex operations",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput-an-analogy-i",
    "href": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput-an-analogy-i",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.8 Latency vs Throughput: an analogy (I)",
    "text": "10.8 Latency vs Throughput: an analogy (I)\nTravelling by road:\n\nlatency: use a sport car (faster but can only take 2-5 persons)\nthroughput: use a bus (slower but can take up to 80 persons)\n\nTotal time to transport 5 persons over 100km (latency):\n\n1 trip with a 80-seat bus at 50km/h: 2hours\n1 trip with a 5-seat car at 200km/h: 0.5hours\n\nTotal time to transport 160 persons over 100km (throughput):\n\n2 round trips with a 80-seat bus at 50km/h: 8hours\n32 round trips with a 5-seat car at 200km/h: 32hours",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput-an-analogy-ii",
    "href": "Courses/7_GPU_Caching_etc.html#latency-vs-throughput-an-analogy-ii",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.9 Latency vs Throughput: an analogy (II)",
    "text": "10.9 Latency vs Throughput: an analogy (II)\nFastest way to learn?\n\n\nLatency: private teacher or small classroom\nThroughput: big classroom or online course\n\n\n\n\n\nYgor Serpa (TowardsDataScience)\n\n\\rightarrow none is arguably better than the others, all work and fill a specific need",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-hardware-side",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-hardware-side",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.10 CPU vs GPU: hardware side",
    "text": "10.10 CPU vs GPU: hardware side\n\nUnits: B = Byte (memory), W = Watts (power), Hz = Hertz (frequency)\n\n\nType\nNb of cores\nMemory (cache1)\nMemory (device)\nPower per core\nClock rate2\n\n\n\n\nCPU\n10\\times\n10\\times - 100\\times KB\n10\\times - 100\\times GB3\n10\\times W\n3 - 5 GHz\n\n\nGPU\n1000\\times\n100\\times B\n10\\times GB4\n0.01\\times - 0.1\\times W\n0.5 - 1 GHz",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-concept-side",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-concept-side",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.11 CPU vs GPU: concept side",
    "text": "10.11 CPU vs GPU: concept side\n\n\n\nCPU\nGPU\n\n\n\n\nTask parallelism\nData parallelism\n\n\nA few “heavyweight” cores\nMany “ligthweight” cores\n\n\nHigh memory size\nHigh memory throughput\n\n\nMany diverse instruction sets\nA few highly optimized instruction sets\n\n\nSoftware thread management\nHardware thread management\n\n\n\n\nMantas Levinas (CherryServers)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#gpu-dedicated-chip",
    "href": "Courses/7_GPU_Caching_etc.html#gpu-dedicated-chip",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.12 GPU = dedicated chip",
    "text": "10.12 GPU = dedicated chip\n\n\nCPU directly mounted on the motherboard\n&lt;5\\times 5 cm dimension5\n \n\nGPU is a HUGE chip (\\sim 30\\times 10 cm)\nmounted through PCI-express connection\n\n\n\n\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.13 CPU vs GPU comparison: an example",
    "text": "10.13 CPU vs GPU comparison: an example\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example-cpu",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example-cpu",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.14 CPU vs GPU comparison: an example, CPU",
    "text": "10.14 CPU vs GPU comparison: an example, CPU\nAMD Ryzen Threadripper 2990X processor: 32cores (each capable of running two independent threads), 3 GHz up to 4.2 GHz (boost) clock rate, 2 instructions per clock cycle, each thread processes 8 floating-point values at the same time",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example-gpu",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-vs-gpu-comparison-an-example-gpu",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.15 CPU vs GPU comparison: an example, GPU",
    "text": "10.15 CPU vs GPU comparison: an example, GPU\nNvidia RTX 2080 TI GPU: 4352 ALUs, 1.35 GHz up to 1.545 GHz (boost) clock rate, 2 operations per clock cycle, one floating point operation per ALU",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#performance-illustration",
    "href": "Courses/7_GPU_Caching_etc.html#performance-illustration",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.16 Performance illustration",
    "text": "10.16 Performance illustration\nThroughput:\nMaximum theoretical number of floating-point operations they can handle per second (FLOPS)\n\nCPU (Ryzen): (4.2 \\times 10^9) \\times 2 \\times (8 \\times 64) = 4309.8 \\times 10^9 FLOPS \\approx 4.3 TFLOPS\nGPU (2080 TI): (1.545 \\times 10^9) \\times 2 \\times 4352 = 13447.68 \\times 10^9 FLOPS \\approx 13.4 TFLOPS\n\nLatency: the CPU clocks at 4.2 GHz, while the GPU clocks at \\sim 1.5 GHz, nearly three times slower\n\\rightarrow one is not better than the other, they serve different purposes",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#from-3d-image-processing-for-games-to-general-purpose-computing",
    "href": "Courses/7_GPU_Caching_etc.html#from-3d-image-processing-for-games-to-general-purpose-computing",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.17 From 3D image processing (for games) to general-purpose computing",
    "text": "10.17 From 3D image processing (for games) to general-purpose computing\nimage processing = large matrix operations\n\\rightarrow why not using GPU for general matrix operations?\nExample: deep-learning = combination of linear transformation (=matrix product) and simple non-linear operations\nGeneral-Purpose Graphics Processing Units (GPGPU): computations on GPU not dedicated to image processing",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#hardware",
    "href": "Courses/7_GPU_Caching_etc.html#hardware",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.18 Hardware",
    "text": "10.18 Hardware\n\nNvidia (with CUDA drivers) \nAMD (with AMD drivers or openCL)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#software-python",
    "href": "Courses/7_GPU_Caching_etc.html#software-python",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.19 Software (Python)",
    "text": "10.19 Software (Python)\n\ndeep learning oriented: tensorflow, PyTorch, keras \nmore general purpose computing: see numba, cupy, RAPIDS (cuDF, cuML, etc.), keops, and more",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#software-r",
    "href": "Courses/7_GPU_Caching_etc.html#software-r",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.20 Software (R)",
    "text": "10.20 Software (R)\nSee this page6\nExample with gpuR:\nORDER = 1024\nA = matrix(rnorm(ORDER^2), nrow=ORDER)\nB = matrix(rnorm(ORDER^2), nrow=ORDER)\ngpuA = gpuMatrix(A, type=\"double\")\ngpuB = gpuMatrix(B, type=\"double\")\nC = A %*% B\ngpuC = gpuA %*% gpuB\nall(C == gpuC[])\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#in-practice",
    "href": "Courses/7_GPU_Caching_etc.html#in-practice",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "10.21 In practice",
    "text": "10.21 In practice\nSee the dedicated notebook “GPU computing with Numba (introduction)” in the Applications list.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-cache",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-cache",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.1 CPU cache ?",
    "text": "11.1 CPU cache ?\nCache = on-board memory unit for CPU cores\n\n\nYgor Serpa (TowardsDataScience)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#example-of-architecture",
    "href": "Courses/7_GPU_Caching_etc.html#example-of-architecture",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.2 Example of architecture",
    "text": "11.2 Example of architecture\n\n\nCredit: Wikipedia (AMD Bulldozer server)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cache-memory-size",
    "href": "Courses/7_GPU_Caching_etc.html#cache-memory-size",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.3 Cache memory size",
    "text": "11.3 Cache memory size\n\n\nVery small\n\nL1 Cache = 10\\times KB (per core) \nL2 Cache = 100\\times KB (per core) \nL3 Cache = 1000\\times KB (shared per CPU)\n\n\nExample:\n\n\n\n\nWikipedia (K8 core in the AMD Athlon 64 CPU)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cache-access-time-latency",
    "href": "Courses/7_GPU_Caching_etc.html#cache-access-time-latency",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.4 Cache access time (latency)",
    "text": "11.4 Cache access time (latency)\n\n\nStackoverflow",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cpu-cache-miss",
    "href": "Courses/7_GPU_Caching_etc.html#cpu-cache-miss",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.5 CPU cache miss",
    "text": "11.5 CPU cache miss\n\ndata transferred from memory to cache by blocks of contiguous data\nto be efficient: necessary to use contiguous data in computations\n\n\n\nData analysis workflows with R and Python course",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#in-practice-1",
    "href": "Courses/7_GPU_Caching_etc.html#in-practice-1",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "11.6 In practice",
    "text": "11.6 In practice\nSee the dedicated notebook “CPU Cache and its impact on computations” in the Applications list.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#cluster",
    "href": "Courses/7_GPU_Caching_etc.html#cluster",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "12.1 Cluster",
    "text": "12.1 Cluster\n\na front-end server: user interface to submit computations, accessible from the internet (e.g. by ssh connection or through a web interface)\nmany computing servers (also called computing nodes or workers) that run the computations\none or more storage servers: to store the data and results\n\n\n\nWikipedia",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#usage",
    "href": "Courses/7_GPU_Caching_etc.html#usage",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "12.2 Usage",
    "text": "12.2 Usage\n\nsingle-node multi-core computations (multi-threading or multi-processing) \nmulti-node computations (distributed computations) \nGPU computing \ncombinations of two or more",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#illustration",
    "href": "Courses/7_GPU_Caching_etc.html#illustration",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "12.3 Illustration",
    "text": "12.3 Illustration\nMUSE cluster at MESO@LR computing center in Montpellier\n\n\nmeso@LR",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#resource-management",
    "href": "Courses/7_GPU_Caching_etc.html#resource-management",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "12.4 Resource management",
    "text": "12.4 Resource management\nHow to share the computing resources (cores, memory, full nodes) between user?\n\\rightarrow a resource management system (also called a scheduler) to assign resources to users depending on their request\nFunctions:\n\nallocating exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work \nproviding a framework for starting, executing, and monitoring work, typically a parallel job \narbitrating contention for resources by managing a queue of pending jobs",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#an-example",
    "href": "Courses/7_GPU_Caching_etc.html#an-example",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "12.5 An example",
    "text": "12.5 An example\nSLURM = a job scheduler for clusters and supercomputers\nJob submission:\n\nIn-session command submission (e.g. requesting 16 cores with 32GB of RAM memory and a RTX 2080 TI GPU, on a single node, during 3 days):\n\nsrun -N 1 -c 16 --mem=32G --gpus=rtx2080:1 -t 3-0 my_command\n\nScript submission to be run off-session (e.g. requesting 3 full nodes during 96hours):\n\nsbatch -N 3 -t 96:0:0 my_script.sh",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "Courses/7_GPU_Caching_etc.html#footnotes",
    "href": "Courses/7_GPU_Caching_etc.html#footnotes",
    "title": "9  GPU computing, CPU cache, Computing cluster",
    "section": "",
    "text": "per core↩︎\ncycles per second (one operation = 1\\times - 10\\times cycles↩︎\nRAM = host memory↩︎\nGPU on-board memory↩︎\nsize fixed for decades↩︎\ncorresponding to CRAN task view on high performance computing↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>GPU computing, CPU cache, Computing cluster</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Fowler, M. 2022. Python Concurrency with Asyncio. Manning. https://www.manning.com/books/python-concurrency-with-asyncio.\n\n\nInoue, Hiroshi. 2016. “How SIMD Width Affects Energy Efficiency: A\nCase Study on Sorting.” In 2016 IEEE Symposium in Low-Power\nand High-Speed Chips (COOL CHIPS XIX), 1–3. IEEE.\n\n\nRobey, R., and Y. Zamora. 2021. Parallel and High Performance\nComputing. Manning. https://www.manning.com/books/parallel-and-high-performance-computing.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Appendix A — Applications",
    "section": "",
    "text": "Original (In Percent Format)\nOnline Html (Corrected)\nNotebook (Corrected)\n\n\n\n\nNumpy Workout\nSolution\nNotebook\n\n\nMultiprocessing in Python 3\nSolution\nNotebook\n\n\nMultiProcessing, Strong Scaling\nSolution\nNotebook\n\n\nScaling App with multiprocessing\nSolution\nNotebook\n\n\nAn asyncio application\nSolution\nNotebook\n\n\nDecorators Tutorial\nSolution\nNotebook\n\n\nIPC and Locking\n\n\n\n\nLocking with multiprocessing.Value\n\n\n\n\nA tutorial on Python generators\nSolution\nNotebook\n\n\nDistributed models with dask\n\n\n\n\nDask delayed App\n\n\n\n\nNumba Introduction\nSolution\nNotebook\n\n\nSIMD Autovectorization in Numba\nSolution\nNotebook\n\n\nGPU computing with Numba (introduction)\nSolution\nNotebook\n\n\nCPU Cache and its impact on computations\nSolution\nNotebook\n\n\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Appendix B — Slides in reveal.js",
    "section": "",
    "text": "Numpy Workout\nIntroduction to parallel computing\nAdvanced concepts in parallel programming\nAsynchronous Programming with Python\nIPC and locking\nDistributed Computing models\nDask delayed\nHardware Vectorization with SIMD\nGPU computing, CPU cache, Computing cluster\n\n\nNo matching items",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Slides in reveal.js</span>"
    ]
  }
]